# Foundational Principles

## Tokens

## Co-occurrence

## Attention

The transformer architecture [@vaswani2017attention] revolutionized NLP.
As of October 7, this paper had **_197,775_** citations on [Google Scholar](https://scholar.google.com/scholar?cluster=2960712678066186980&hl=en&as_sdt=0,33){target="_blank"}.

## Embeddings
