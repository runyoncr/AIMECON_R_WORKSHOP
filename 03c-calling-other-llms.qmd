# Call other (non-Gen AI) LLMs

Although not the focus of our workshop, many use cases when a different LLM model might be useful, such as [DeBERTa](https://huggingface.co/docs/transformers/en/model_doc/deberta), a strong non-generative LLM that has improvements over the BERT and RoBERTa models (original paper by He, Liu, Gao, and Chen (2020) that introduces the model is [here](https://arxiv.org/abs/2006.03654); see this ['towards data science' article for a higher-level overview](https://towardsdatascience.com/large-language-models-deberta-decoding-enhanced-bert-with-disentangled-attention-90016668db4b/)) . [Here's the full list of models available though Hugging Face.](https://huggingface.co/models)

Many of these models are open-source and can be downloaded and run locally. 
If you're like me and don't have the technical expertise required to implement such a workflow, the good news is that you can call many of these models through an API at [Hugging Face (huggingface.co)](https://huggingface.co/) using similiar syntax as to what we'll be using today.

You'll have to sign up for an account, and the free account provides you with a fair amount of capabilities: 100GB private storage limit, 1000 API calls (per 5-minute window), 5,000 resolvers(per 5-minute window; delayed API calls), and 200 pages.