[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Integrating Generative AI into R Workflows: From APIs to Shiny Apps",
    "section": "",
    "text": "Welcome\nThis online book was created to be a user-friendly way to present the materials in support of the “Integrating Generative AI into R Workflows: From APIs to Shiny Apps” workshop, first given at AIMECON in October, 2025. Although created primarily for this purpose, I intend to contiually update these materials as I learn.\nChanges to the book last made on: 2025-10-12 21:06:27\n\n\nLearning Objectives\n\nBy the end of this workshop, participants will be able to:\n\nExplain key LLM architectural features that inform effective integration practices\nApply prompt engineering principles to achieve consistent, reliable outputs in R workflows\nImplement both single-use and conversational API interactions with LLMs from R\nDesign and deploy simple multi-agentic systems for complex tasks\nBuild interactive Shiny applications that leverage LLM capabilities for end-user functionality\n\n\n\n⚠️ Privacy Reminder\n\nPlease don’t enter any personal, confidential, or sensitive information into generative AI tools. This includes private details about yourself or others, as well as proprietary data. Assume that anything you input could be used to train future models or be visible to others. It is important for you to consult both your IT departments and legal counsel before using a generative AI model in any operational setting.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#workshop-goals",
    "href": "index.html#workshop-goals",
    "title": "AIMECON R Workshop",
    "section": "1.1 Workshop Goals",
    "text": "1.1 Workshop Goals\n\nLearn R fundamentals\nPractice data analysis\nBuild reproducible reports",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AIMECON R Workshop</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction to R",
    "section": "",
    "text": "3 Getting Started with R\nThis chapter covers the basics of R programming.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro.html#installing-r",
    "href": "intro.html#installing-r",
    "title": "2  Introduction to R",
    "section": "3.1 Installing R",
    "text": "3.1 Installing R\nInstructions for installing R…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Workshop Summary",
    "section": "",
    "text": "4 Summary\nKey takeaways from the workshop…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Workshop Summary</span>"
    ]
  },
  {
    "objectID": "06-reference-materials.html",
    "href": "06-reference-materials.html",
    "title": "Reference Materials",
    "section": "",
    "text": "Background Information on GPTs\nThese materials / links were last checked on October 2, 2025. All apologies for links that no longer work. Please email me at CRunyon@nbme.org if you notice something no longer works so I can change / remove the link.\nThe 3Blue1Brown YouTube Channel provides several good videos on the some of the technical aspects of large language models.\nAnthropic’s paper On the Biology of a Large Language Model is particularly interesting.\nThis post on lesswrong provides a nice high-level summary for understanding LLMs.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#background-information-on-gpts",
    "href": "06-reference-materials.html#background-information-on-gpts",
    "title": "Reference Materials",
    "section": "",
    "text": "The Neural Networks section is particularly informative.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#reference-guides-and-prompt-engineering",
    "href": "06-reference-materials.html#reference-guides-and-prompt-engineering",
    "title": "Reference Materials",
    "section": "Reference Guides (and Prompt Engineering)",
    "text": "Reference Guides (and Prompt Engineering)\nOpenAI Cookbook",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#shiny-related-resources",
    "href": "06-reference-materials.html#shiny-related-resources",
    "title": "Reference Materials",
    "section": "Shiny-related Resources",
    "text": "Shiny-related Resources\nPosit has many useful things.\n\nMaterials for the R language start here.\nMaterials for the Python language start here.\n\nnanxstats has nicely organized many Shiny extension packages.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#legal-considerations",
    "href": "06-reference-materials.html#legal-considerations",
    "title": "Reference Materials",
    "section": "Legal Considerations",
    "text": "Legal Considerations\nThe information provided here is for general informational purposes only and does not constitute legal advice. You should not act upon any information presented without first seeking qualified legal counsel regarding your specific situation. The authors disclaim any liability for actions taken based on the content provided here.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "00a-R-setup.html",
    "href": "00a-R-setup.html",
    "title": "2  R Setup",
    "section": "",
    "text": "2.1 Installing R\nThis workshop is based in the R programming language. As such, it is necessary to have a recent version installed on your computer. I would suggest R Version \\(\\geq\\) 4.4.\nThese workshop materials were built and tested using R version 4.5.1 (Great Square Root, released 2025-06-13), with all packages updated on or after October 5, 2025.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "00a-R-setup.html#installing-r-studio-desktop",
    "href": "00a-R-setup.html#installing-r-studio-desktop",
    "title": "2  R Setup",
    "section": "\n2.2 Installing R Studio Desktop",
    "text": "2.2 Installing R Studio Desktop\nPosit makes a wonderful integrated development environment (IDE) called R Studio Desktop. This is my preferred IDE for R, although you may choose others if you like. I will be using R Studio throughout the workshop.\nI am using R Studio Version 2025.09.1+401, which was installed on October 5, 2025.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "00a-R-setup.html#installing-r-tools",
    "href": "00a-R-setup.html#installing-r-tools",
    "title": "2  R Setup",
    "section": "\n2.3 Installing R Tools",
    "text": "2.3 Installing R Tools\nRTools is necessary to install or build R packages that require compilation of C, C++, or Fortran code, as it provides the necessary compiler toolchain and build tools. The packages that are available on CRAN do not require having RTools installed because the packages hosted there are precompiled binary packages.\nI’m unsure of which packages I’ve downloaded from Github or other sources that require compilation, so I recommend also installing at RTools 4.4. (And it’s just good to have in case you need it later.)",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "00b-R-packages.html",
    "href": "00b-R-packages.html",
    "title": "3  R Packages",
    "section": "",
    "text": "3.1 Workshop package\nR’s strength lies not just in its statistical capabilities, but in its open-source nature. R’s code is freely available for anyone to inspect, modify, and improve. This openness has cultivated a vibrant global community of statisticians, data scientists, and developers who actively contribute to the language’s evolution.\nThe R package ecosystem exemplifies this collaborative spirit. Community members can develop packages that address specific needs that span many academic domains (see Task Views). These packages allow you to utilize and build on classic and new analytical methods, advancing science and, thereby, public good.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Packages</span>"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html",
    "href": "01a-gen-ai-basics.html",
    "title": "5  Foundational Principles",
    "section": "",
    "text": "5.1 Tokens",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-learning.html",
    "href": "01b-gen-ai-learning.html",
    "title": "4  Learning More about Gen AI",
    "section": "",
    "text": "4.1 YouTube / Video Resources\nBrownBlue",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Learning More about Gen AI</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-learning.html#articles-and-blogs",
    "href": "01b-gen-ai-learning.html#articles-and-blogs",
    "title": "4  Learning More about Gen AI",
    "section": "4.2 Articles and Blogs",
    "text": "4.2 Articles and Blogs\nCool recent jawn",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Learning More about Gen AI</span>"
    ]
  },
  {
    "objectID": "02a-general-prompt-information.html",
    "href": "02a-general-prompt-information.html",
    "title": "6  General Prompt Engineering",
    "section": "",
    "text": "6.1 Including Context\nIntegrate AI Fundamentals information in this section.\nLink to promptingguide.ai here?",
    "crumbs": [
      "Prompt Engineering",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>General Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "02b-prompt-formulas.html",
    "href": "02b-prompt-formulas.html",
    "title": "7  Example Prompt Formulas",
    "section": "",
    "text": "#Prompt Formulas”\nLink to examples here",
    "crumbs": [
      "Prompt Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Prompt Formulas</span>"
    ]
  },
  {
    "objectID": "02c-chained-workflows.html",
    "href": "02c-chained-workflows.html",
    "title": "8  Chained Workflows",
    "section": "",
    "text": "8.1 Prompt Chaining\nHistory in chain-of-thought models; evolved to workflows and reasoning models.",
    "crumbs": [
      "Prompt Engineering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chained Workflows</span>"
    ]
  },
  {
    "objectID": "03a-api-keys.html",
    "href": "03a-api-keys.html",
    "title": "4  API Keys",
    "section": "",
    "text": "4.1 Workshop API Key\nAn API key is a unique identifier that allows software programs to communicate securely with external services, such as a large language model hosted by a provider. The key acts like a digital credential: it tells the system who you are, verifies that you have permission to use the service, and tracks how much you use it for billing and security purposes. Unfortunately, if you have a subscription to an LLM provider (e.g., Anthropic, OpenAI, etc.), this does not automatically come with an API key. You will have to set up the API separately.\nFor users integrating LLMs into R or other analytic workflows, the API key serves as the bridge between your local environment and the remote model. When your script makes an API call, the key authenticates that request and returns the model’s response. Because API keys grant direct access to paid and potentially sensitive resources, they should be stored securely—never shared publicly, committed to version control, or embedded directly in reproducible code examples.\nIn essence, the API key is what allows researchers to treat a model as a callable function, enabling controlled, programmatic access to a complex system running on remote infrastructure. Understanding how to manage this key responsibly is a foundational step in incorporating LLMs into reproducible research and educational applications.\nIn practice, an API key is a long string of letters and numbers that you include in your code whenever you send a request to the model. I asked OpenAI’s ChatGPT5 to generate a fake API key for demonstrative purposes. They generally look like this: sk-1234567890abcdefGHIJKLMNOPQRSTUVWXYZ1234. The actual prefixes and length of random alphanumeric characters will usually vary by provider.\nLinks to documentation for API keys:\nI created an API key that you will be able to use for the purposes of the workshop. I have chosen to use Anthropic’s Claude Sonnet 4.5 model](https://www.anthropic.com/news/claude-sonnet-4-5){target=“_blank”} for this workshop. This API key will only be active during the workshop hours. If you attempt to use the API key outside of these hours, you will see that it has been disabled and your calls to the model will not be completed.\nBecause API keys are cost-per-use, I ask that you please only do the workshop activities and other experimentation. Use costs are relatively low for this type of use and I’m happy to cover the cost and provide an API key for educational purposes. Outside of the workshop you’ll need to create an account with a model provider and register for an API key to track your model use.",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API Keys</span>"
    ]
  },
  {
    "objectID": "03b-prompting-models.html",
    "href": "03b-prompting-models.html",
    "title": "10  Prompting LLMs via API",
    "section": "",
    "text": "10.1 Single interactions with Models\nWhen are these appropriate?",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Prompting LLMs via API</span>"
    ]
  },
  {
    "objectID": "04a-conversations-via-api.html",
    "href": "04a-conversations-via-api.html",
    "title": "10  Conversational Models",
    "section": "",
    "text": "10.1 Continued interactions with models\nWhen are these appropriate?",
    "crumbs": [
      "The ellmer() package",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conversational Models</span>"
    ]
  },
  {
    "objectID": "04b-rag-models.html",
    "href": "04b-rag-models.html",
    "title": "11  Retrieval Augmented Generation",
    "section": "",
    "text": "11.1 What are RAG models?\nWhen are these appropriate?",
    "crumbs": [
      "The ellmer() package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "05a-shiny.html",
    "href": "05a-shiny.html",
    "title": "12  Shiny",
    "section": "",
    "text": "12.1 Online Web Application\nCan be hosted locally\nWritten in either R or Python",
    "crumbs": [
      "Shiny Integration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Shiny</span>"
    ]
  },
  {
    "objectID": "05b-vibe-coding.html",
    "href": "05b-vibe-coding.html",
    "title": "15  “Vibe Coding”",
    "section": "",
    "text": "15.1 Feel the Vibe!\nVibe Coding is the recent term of the art for using an LLM to help you write syntax, which includes online applications. The term was coined by Andrej Karpathy in a tweet on February 2, 2025 (ars technica article)\nReduces tedious syntax writing Focus on functionality",
    "crumbs": [
      "Shiny Integration",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>\"Vibe Coding\"</span>"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html",
    "href": "01-gen-ai-fundamentals.html",
    "title": "Generative AI Fundamentals",
    "section": "",
    "text": "What is Generative AI?\nIt is useful to learn some fundamental aspects about GenAI models prior to their use. I have often found that there are many guides online that tell you how to use these LLMs, but don’t tell you why you should use these LLMs. Although the guidance given by these guides is often useful, when your prompt isn’t resulting in the output you want it’s unclear how to make improvements because you don’t have the foundational knowledge of how the model might be processing your prompt.\nEven though these generative AI models continue to improve without extensive prompt engineering, this foundational knowledge will serve you well in building your own generative AI workflows.\nSo what is generative artificial intelligence? We often hear the words GPT, generative AI, AI, LLM, and machine learning models used somewhat interchangeably and that isn’t always necessarily the case. Sometimes the speaker may not understand that they’re referring to something they don’t intend to refer to, or sometimes the technical aspects of what exactly they’re referring to doesn’t quite matter.\nIn this diagram, I’ve attempted to show you the relationship between Artificial Intelligence, Natural Language Processing, and related fields, with Generative Artificial Intelligence in the middle. The scale of the circles in this image is not accurate – I’ve only attempted to show the relationships between some of these larger fields.\nWorking from the left-hand side inward, our outermost circle is Artificial Intelligence, or AI. Artificial intelligence is the broad field of computer science focused on creating systems that can perform tasks typically requiring human intelligence, such as problem-solving, reasoning, and understanding language. I’ve heard some people argue that statistical models like regressions can be included in this large circle because you’re learning about relationships in data that are not directly observable, but I’m not sure that I’m convinced of this argument, but it does give you an idea of how broad the field of AI can be construed.\nAs we start to go inward a little more, we have Machine Learning. Machine learning is a subset of AI that involves algorithms that learn patterns from data and improve their performance on tasks over time without being explicitly programmed for every possible scenario. This is different from a regression where you identify all the key predictor variables of interest based on your knowledge of the domain. With machine learning you just need to identify enough of the potentially relevant variables to maximize your predictions. Here is where we start to encounter so-called “black box” models because sometimes interpreting the decision-making processes of the algorithm becomes difficult due to the complex nature of the models.\nGoing further inward, we have Deep Learning, which is a subset of Machine Learning models. These models eliminate the need for identifying variables due to the large amount and variety of data that are supplied to the models. Deep Learning models use very complex algorithms to identify complex patterns in the data, and their results are often more uninterpretable. This is good for things like speech and natural language, where relationships between variables are not easily identified.\nI’ll now jump to the right-hand side and the large field of Natural Language Processing. Broadly speaking, natural language processing is focused on enabling computers to understand and interpret human language, and you can see it’s a very broad field of study in and of itself. There are areas of NLP that don’t necessarily overlap with machine learning or deep learning, such as sentiment analysis or topic modeling which may not use Machine Learning or Deep Learning Models.\nIf we look at the overlap between deep learning models and natural language processing models, we see that Large Language Models – LLMS – live here. LLMs represent a deep learning approach within NLP that leverages vast amounts of text data to generate and understand human-like language. These models go beyond traditional NLP techniques by using statistical learning to predict and produce coherent, context-aware text at scale. Not all LLMs are generative AI models, such as language translation app.\nNow, finally, in the middle we have Generative AI Models (Gen AI). These models that are a specific type of LLM that are designed to generate output that model resemble their training data. The most common model is a generative text model, although models are now being developed that have the capability to generate images, audio, and video.",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#what-is-a-gpt",
    "href": "01-gen-ai-fundamentals.html#what-is-a-gpt",
    "title": "Generative AI Fundamentals",
    "section": "What is a GPT?",
    "text": "What is a GPT?",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#co-occurrence",
    "href": "01-gen-ai-fundamentals.html#co-occurrence",
    "title": "Generative AI Fundamentals",
    "section": "Co-occurrence",
    "text": "Co-occurrence",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#attention",
    "href": "01-gen-ai-fundamentals.html#attention",
    "title": "Generative AI Fundamentals",
    "section": "Attention",
    "text": "Attention",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#model-parameters",
    "href": "01-gen-ai-fundamentals.html#model-parameters",
    "title": "Generative AI Fundamentals",
    "section": "Model Parameters",
    "text": "Model Parameters\nOrganize into classes of parameters?\n\nTemperature\n\nGeneration based?\nChecking if 4 hashtags work\n\n\n\ntop_p\n\n\ntop_k\n\n\nmax tokens",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html",
    "href": "02-prompt-engineering.html",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Prompt Engineering Importance\nWe’ll only cover a sampling of these methods in the workshop today because we’ll be focusing more on their application than extensively testing different prompting strategies, but I encourage you to further explore these techniques based on your specific use case.\nTie in to chapter 1",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#madlibs",
    "href": "02-prompt-engineering.html#madlibs",
    "title": "Prompt Engineering",
    "section": "Madlibs",
    "text": "Madlibs",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#prompt-formulas",
    "href": "02-prompt-engineering.html#prompt-formulas",
    "title": "Prompt Engineering",
    "section": "Prompt Formulas",
    "text": "Prompt Formulas",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#prompt-chaining",
    "href": "02-prompt-engineering.html#prompt-chaining",
    "title": "Prompt Engineering",
    "section": "Prompt Chaining",
    "text": "Prompt Chaining\n\nChain-of-thought Prompting\n\nInvisible Instructions\n\n\n\nManual Prompt Chaining",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "03-api-implementation.html",
    "href": "03-api-implementation.html",
    "title": "API Implementation",
    "section": "",
    "text": "Choosing a Generative AI Model\nArguably the most popular method for interacting with a generative AI model is a simple chatbot-based interface. These interfaces support a conversational interactions with LLM much like a text conversation or Teams chat (or AOL Instant Messaging). There is a text box, you enter a prompt, the model processes that prompt, and provides a response. And continue.\nThis method of interacting with a generative AI model has many advantages, the most salient of which is ease of interaction. It’s easy have a back-and-forth conversation, and many of the model providers have made it so you can continue old conversations or search through previous conversations.\nHowever, for certain tasks (e.g., repetitive tasks to be completed at scale), the chatbot interface can be inefficient. It can be time consuming to copy-cut-paste-submit-copy-cut-paste - and repeat - for the n number of times you need to complete a task. In these instances it may be better to interact with a model via an application programming interface (API).\nAn API is a structured way for one piece of software to communicate with another. When you use a generative AI model through its API, you’re programmatically sending requests with specific instructions and receiving structured responses—allowing you to integrate AI capabilities directly into your R scripts, automate repetitive tasks, and process data at scale. Unlike a chatbot interface where you manually type and read each exchange, the API allows your code to handle hundreds or thousands of interactions automatically, making it the foundation for building LLM-powered tools and workflows.",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03-api-implementation.html#r-packages-for-api-implementation",
    "href": "03-api-implementation.html#r-packages-for-api-implementation",
    "title": "API Implementation",
    "section": "R Packages for API Implementation",
    "text": "R Packages for API Implementation\nMany available. Covering a handful - saving special time for ellmer, which was created by Hadley Wickham and other amazing developers at Posit.",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03-api-implementation.html#system-prompt",
    "href": "03-api-implementation.html#system-prompt",
    "title": "API Implementation",
    "section": "System Prompt",
    "text": "System Prompt",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03-api-implementation.html#user-prompt",
    "href": "03-api-implementation.html#user-prompt",
    "title": "API Implementation",
    "section": "User Prompt",
    "text": "User Prompt\nWhat’s the difference?",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "04-ellmer.html",
    "href": "04-ellmer.html",
    "title": "The ellmer() package",
    "section": "",
    "text": "ellmer() is the GOAT\nImplements conversational AI easily, much like a chatbot.\nWhen good?\nWhen bad?",
    "crumbs": [
      "The ellmer() package"
    ]
  },
  {
    "objectID": "04-ellmer.html#first-thing",
    "href": "04-ellmer.html#first-thing",
    "title": "The ellmer() package",
    "section": "First Thing",
    "text": "First Thing",
    "crumbs": [
      "The ellmer() package"
    ]
  },
  {
    "objectID": "04-ellmer.html#second-thing",
    "href": "04-ellmer.html#second-thing",
    "title": "The ellmer() package",
    "section": "Second Thing",
    "text": "Second Thing",
    "crumbs": [
      "The ellmer() package"
    ]
  },
  {
    "objectID": "05-shiny-integration.html",
    "href": "05-shiny-integration.html",
    "title": "Shiny Integration",
    "section": "",
    "text": "What is Shiny?\nThis chapter covers the basics of R programming.",
    "crumbs": [
      "Shiny Integration"
    ]
  },
  {
    "objectID": "05-shiny-integration.html#vibe-coding",
    "href": "05-shiny-integration.html#vibe-coding",
    "title": "Shiny Integration",
    "section": "Vibe Coding",
    "text": "Vibe Coding\nV I B E C O D I N G\n\n\n\nThis is the image’s alt text.",
    "crumbs": [
      "Shiny Integration"
    ]
  },
  {
    "objectID": "00-preparation.html",
    "href": "00-preparation.html",
    "title": "Set Up",
    "section": "",
    "text": "This section will help you ensure that you are appropriately set up to complete the rest of the workshop.",
    "crumbs": [
      "Set Up"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html",
    "href": "00c-LLM-R-packages.html",
    "title": "3  LLM-specific R Packages",
    "section": "",
    "text": "3.1 ellmer\nA number of packages have been developed to more easily facilitate interacting with LLMs via R. Many of these packages are useful (we’ll cover some of those in the workshop), whereas other packages include some developer design decisions that don’t work particularly well for my usual workflows.\nBelow is a non-exhaustive list of packages that I’ve found to interact with LLMs. This is not meant to be exhaustive or a curated list; it’s only to provide you with information about the packages you’ll be using in the workshop (and others) in the case you find them helpful for your workflow. All package summaries were initially generated with AI. Some summaries have been edited, some have not.\nellmer Overview CRAN Documentation\nellmer is an R package that provides a unified interface for interacting with large language models from over 17 providers including OpenAI, Anthropic, Google Gemini, and AWS Bedrock. It supports advanced features like streaming outputs, tool/function calling, structured data extraction, and multimodal inputs. Chat objects are stateful and maintain conversation context, enabling both interactive console-based conversations and programmatic use in R scripts and applications.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#tidyprompt",
    "href": "00c-LLM-R-packages.html#tidyprompt",
    "title": "3  LLM-specific R Packages",
    "section": "3.2 tidyprompt",
    "text": "3.2 tidyprompt\ntidyprompt Overview CRAN Documentation\ntidyprompt is an R package that provides a compositional framework (“prompt wraps”) for building prompts enriched with logic, validation, and extraction functions when interacting with LLMs. It supports structured output, retry/feedback loops, reasoning strategies (e.g. ReAct or chain-of-thought), and even autonomous R code or function calling as part of an LLM dialogue. The package is provider-agnostic, meaning its features can layer on top of any chat completion API (e.g. via ellmer) to produce more robust, predictable interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#tidyllm",
    "href": "00c-LLM-R-packages.html#tidyllm",
    "title": "3  LLM-specific R Packages",
    "section": "3.3 tidyllm",
    "text": "3.3 tidyllm\ntidyllm Overview CRAN Documentation\ntidyllm provides a tidy, pipeline-friendly interface for interacting with multiple LLM APIs (e.g. Claude, OpenAI, Gemini, Mistral) and local models via Ollama. It supports multimodal inputs (text, images, PDFs), maintains conversational history, handles batching and rate limits, and allows structured schema-based extraction of responses. The design emphasizes composability and integration into typical R data workflows.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#chattr",
    "href": "00c-LLM-R-packages.html#chattr",
    "title": "3  LLM-specific R Packages",
    "section": "3.4 chattr",
    "text": "3.4 chattr\nchattr Overview CRAN Documentation\nchattr is an R package that enables interactive communication with large language models directly within RStudio using a Shiny gadget or from the console. It enriches prompts with contextual information (e.g. loaded data frames) and integrates with various back-ends (e.g. OpenAI, Copilot, local LlamaGPT) via the ellmer interface. The package is geared toward exploratory workflows and rapid prototyping of LLM-assisted analysis.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#llmagentr",
    "href": "00c-LLM-R-packages.html#llmagentr",
    "title": "3  LLM-specific R Packages",
    "section": "3.5 LLMAgentR",
    "text": "3.5 LLMAgentR\nLLMAgentR Overview CRAN Documentation\nLLMAgentR is an R package for constructing language model “agents” using a modular, graph-based execution framework inspired by LangChain/LangGraph architectures. It offers a suite of agent types (e.g. code generation, data wrangling, SQL agents, document summarization) that iteratively reason, generate R code, execute, debug, and explain results. The package aims to support reproducible AI workflows for analysis, research, and automation by integrating LLM reasoning and domain logic.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#packetllm",
    "href": "00c-LLM-R-packages.html#packetllm",
    "title": "3  LLM-specific R Packages",
    "section": "3.6 PacketLLM",
    "text": "3.6 PacketLLM\nPacketLLM Overview CRAN Documentation\nPacketLLM offers an interactive RStudio gadget interface for chatting with OpenAI LLMs (e.g. GPT-5 and variants) directly within the R environment. It supports multiple simultaneous conversation tabs, file upload (e.g. .R, PDF, DOCX) as contextual input, and per-conversation system message configuration. API calls are handled asynchronously (via promises + future) to avoid blocking the R console during model interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#what-is-a-generative-pre-trained-transformer",
    "href": "01-gen-ai-fundamentals.html#what-is-a-generative-pre-trained-transformer",
    "title": "Generative AI Fundamentals",
    "section": "What is a Generative Pre-trained Transformer?",
    "text": "What is a Generative Pre-trained Transformer?\nGenerative Pre-trained Transformers (GPTs) are a type of generative AI model. While not all generative text models have “GPT” in their name, understanding their underlying structure can be helpful for learning how these models work. Let’s look at each letter in the acronym:\n\nGenerative\nThe G in GPT stands for generative, which refers to the model’s ability to generate new content based on its training data. :Make analogy to regression:\n\n\nPre-trained\nGenerative AI models undergo a vast amount of model pre-training (the P). In a generative text model, this includes a vast corpus of text data. To give you an idea of how large this training data is, when I was first learning about these models, I often heard presenters say some of these models were trained on all the publicly-available text on the internet. Having such a vast amount of data equips the model with a broad “understanding” of language and its nuances. This understanding is a recognition of the patterns of word usage across many contexts.\nIt is important to note that the models are made to generate content and do not have the ability to reflect on the factual accuracy of the information that is provided. If the training data contains extensive information that is related to the input, there’s a higher probability that it will be correct, but there is no guarantee. When using text models, these models are merely predicting the most likely next token in the output. There is no self-reflective step where the model evaluates whether the information being provided is factually accurate. When the model produces information that is not correct in some aspect, this is what is known as a “hallucination”.\n\n\nTransformer\nAnd, finally, T – transformer. This is a technical aspect of the model architecture. : More details and links for this audience. :\nThis blog post on Financial Times is a great visual explanation of transformers (and related concepts) transformers without being too technical.",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html#attention",
    "href": "01a-gen-ai-basics.html#attention",
    "title": "5  Foundational Principles",
    "section": "5.3 Attention",
    "text": "5.3 Attention\nThe transformer architecture1 revolutionized NLP. As of October 7, this paper had 197,775 citations on Google Scholar.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html#embeddings",
    "href": "01a-gen-ai-basics.html#embeddings",
    "title": "5  Foundational Principles",
    "section": "5.4 Embeddings",
    "text": "5.4 Embeddings\n\n\n\n\n\n\n1. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Advances in neural information processing systems. 2017;30.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html",
    "href": "01b-gen-ai-parameters.html",
    "title": "8  Generation Parameters",
    "section": "",
    "text": "8.1 Sampling Controls\nWhen large language models produce text, they do so through a probabilistic process—choosing one word (or token) at a time based on learned likelihoods from vast amounts of training data. Generation parameters govern how that probabilistic process unfolds. Rather than altering what the model “knows,” these parameters control how it expresses that knowledge: how much variability is allowed, how long a response can be, and how the model manages uncertainty while generating language.\nFrom an educational measurement perspective, generation parameters serve a role analogous to setting conditions for test administration or scoring protocols. They define the boundaries within which the model operates, affecting reliability, reproducibility, and interpretability. Understanding these controls allows researchers and educators to align model behavior with the goals of a particular task—whether that task emphasizes consistency and fairness in scoring or diversity and creativity in content generation.\nThese parameters affect the randomness and diversity of the model output.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#temperature",
    "href": "01b-gen-ai-parameters.html#temperature",
    "title": "5  Model Parameters",
    "section": "",
    "text": "5.1.1 Generation based?\nChecking if 3 hashtags work",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#top_p",
    "href": "01b-gen-ai-parameters.html#top_p",
    "title": "5  Model Parameters",
    "section": "5.2 top_p",
    "text": "5.2 top_p",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#top_k",
    "href": "01b-gen-ai-parameters.html#top_k",
    "title": "5  Model Parameters",
    "section": "5.3 top_k",
    "text": "5.3 top_k",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#max-tokens",
    "href": "01b-gen-ai-parameters.html#max-tokens",
    "title": "5  Model Parameters",
    "section": "5.4 max tokens",
    "text": "5.4 max tokens",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#frequency-penalty",
    "href": "01b-gen-ai-parameters.html#frequency-penalty",
    "title": "5  Model Parameters",
    "section": "5.5 Frequency Penalty",
    "text": "5.5 Frequency Penalty\nWhat about other model arguments, such as: ## System Prompt",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#user-prompt",
    "href": "01b-gen-ai-parameters.html#user-prompt",
    "title": "5  Model Parameters",
    "section": "5.6 User Prompt",
    "text": "5.6 User Prompt\nWhat’s the difference?",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "03-api-implementation.html#choosing-a-generative-ai-model",
    "href": "03-api-implementation.html#choosing-a-generative-ai-model",
    "title": "API Implementation",
    "section": "",
    "text": "Anthropic\n\n\nOpenAI\n\n\nOther Model Options",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03a-api-keys.html#workshop-api-key",
    "href": "03a-api-keys.html#workshop-api-key",
    "title": "9  API Keys",
    "section": "9.2 Workshop API Key",
    "text": "9.2 Workshop API Key\nI created an API key that you will be able to use for the purposes of the workshop. This API key will only be active during the workshop hours. If you attempt to use the API key outside of these hours, you will see that it has been disabled and your calls to the model will not be completed.\nBecause API keys are cost-per-use, I ask that you please only do the workshop activities and other experimentation. Use costs are relatively low for this type of use and I’m happy to cover the cost and provide an API key for educational purposes.\nI used Anthropic’s Claude model for the majority of",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>API Keys</span>"
    ]
  },
  {
    "objectID": "03b-prompting-models.html#batch-processing",
    "href": "03b-prompting-models.html#batch-processing",
    "title": "10  Prompting LLMs via API",
    "section": "10.2 Batch Processing",
    "text": "10.2 Batch Processing",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Prompting LLMs via API</span>"
    ]
  },
  {
    "objectID": "04-integration.html",
    "href": "04-integration.html",
    "title": "Integrating LLMs into R Workflows",
    "section": "",
    "text": "First Thing",
    "crumbs": [
      "Integrating LLMs into R Workflows"
    ]
  },
  {
    "objectID": "04-integration.html#second-thing",
    "href": "04-integration.html#second-thing",
    "title": "Integrating LLMs into R Workflows",
    "section": "Second Thing",
    "text": "Second Thing",
    "crumbs": [
      "Integrating LLMs into R Workflows"
    ]
  },
  {
    "objectID": "04a-single-interactions-via-api.html",
    "href": "04a-single-interactions-via-api.html",
    "title": "11  Single Interactions via API",
    "section": "",
    "text": "11.1 Continued interactions with models\nWhen are these appropriate?",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Single Interactions via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html",
    "href": "04b-conversations-via-api.html",
    "title": "\n15  Chat Conversations via API\n",
    "section": "",
    "text": "15.1 Quick Start\nSo far I’ve focused on just interacting with generative models in one-off interactions, where the chat history isn’t preserved. This functionality significantly differs from the normal chatbot interface user experience. This may not be helpful, depending on what you want out of your interaction with the generative model.\nThe ellmer package, developed by Posit, offers an easy way to have a conversational interaction with the different generative AI models. In this section we’ll go over some of the basics for using this functionality. We’ll continue to use our Anthropic API key, although ellmer supports interactions with a variety of model providers: OpenAI, Google Gemini, Mistal, Hugging Face, perplexity.ai, etc. What follows can mostly be generalized to working with other models, with some slight differences (which I’ll point out below).\nThe easiest way to start a conversation is just to use the default settings for a model. You’ll see (as of October 10) that the model is using Claude Sonnet 4 by default. This may change in the future. You can see what Anthropic models are available with the following:\nCodelibrary(ellmer)\n\nmodels_anthropic()\n\n                           id              name created_at cached_input input\nNA claude-sonnet-4-5-20250929 Claude Sonnet 4.5 2025-09-29           NA    NA\n14   claude-opus-4-1-20250805   Claude Opus 4.1 2025-08-05         1.50 15.00\n15     claude-opus-4-20250514     Claude Opus 4 2025-05-22         1.50 15.00\n16   claude-sonnet-4-20250514   Claude Sonnet 4 2025-05-22         0.30  3.00\n6  claude-3-7-sonnet-20250219 Claude Sonnet 3.7 2025-02-24         0.30  3.00\n1   claude-3-5-haiku-20241022  Claude Haiku 3.5 2024-10-22         0.08  0.80\n8     claude-3-haiku-20240307    Claude Haiku 3 2024-03-07         0.03  0.25\n   output\nNA     NA\n14  75.00\n15  75.00\n16  15.00\n6   15.00\n1    4.00\n8    1.25\nThe “input” column is the cost per million tokens of model input (the prompts). The “output” column is the cost per million tokens of the model response. For some context, Shakespeare’s Romeo and Juliet is about 25,000 words, which roughly translates to 40,000 tokens (depends on the tokenization method of the model).\nFor the purposes of demonstration in our workshop, there’s no need to change it, although I’ll show you how you can do this below.\nCode# Gets the API from the .Renviron file\napi_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n\n# You'll see a Claude Sonnet 4 is being used by default.\nchat &lt;- chat_anthropic()\n\nUsing model = \"claude-sonnet-4-20250514\".\nNow let’s look at the conversational functionality. Below I’ve prompted the model via chat$chat(\"prompt\"), and then immediately used the same syntax again (with a different prompt). I’ve hidden the output because it’s so long; you’ll need to click to see the prompt and model response.\nAs you can see, the second response from the model takes into context the first prompt - it’s still talking about the moon! This conversational functionality is useful when you’re doing iterative development or planning, and the previous calls to the model are important for providing content and building upon previous prompts and model responses.",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04c-rag-models.html",
    "href": "04c-rag-models.html",
    "title": "16  Retrieval Augmented Generation",
    "section": "",
    "text": "16.1 ragnar\nRetrieval-Augmented Generation (RAG) is an approach that enhances large language models by connecting them to external knowledge sources. Instead of relying solely on the information encoded in the model during training, RAG systems first search through a database or document collection to find relevant information, then use that retrieved content to generate more accurate and grounded responses. Think of it like an open-book exam versus a closed-book exam: the model can “look up” information from trusted sources rather than depending entirely on memorized knowledge.\nWhen you submit a query to a RAG system, it first converts your question into a mathematical representation called an embedding (see ?sec-embeddings). The documents in the RAG database have been pre-processed the same way - each document or chunk of text has been converted into its own embedding. The system then performs a similarity search to find which document embeddings are most similar to your query embedding, retrieving the most relevant passages.\nThose retrieved passages are then inserted directly into the prompt that gets sent to the LLM. So the model receives something like: “Here are some relevant documents: [retrieved passage 1], [retrieved passage 2], [retrieved passage 3]. Now answer this question: [your original query].” The LLM reads both the retrieved context and your question together, then generates a response based on that combined information. The database itself doesn’t generate anything—it just stores and retrieves text. The LLM does all the language understanding and generation, but it’s working with an enriched prompt that includes relevant background information it didn’t have in its training data. This is why RAG is sometimes described as giving the model a “working memory” or “external knowledge base”—you’re dynamically providing it with relevant information to reference while generating its response.\nFor educational measurement professionals, RAG has promising applications in areas like automated item generation, where the system could retrieve examples from existing item banks before generating new assessment items, or in providing feedback to students by pulling from curriculum materials and scoring rubrics. The key advantage is that RAG systems can work with your organization’s specific content—test specifications, standards documents, or assessment frameworks—without requiring expensive retraining of the underlying model. This makes the technology more practical and trustworthy for high-stakes applications, since you can update the knowledge base as standards evolve and trace the model’s responses back to specific source documents.\nUnsuprisingly (again), Posit has created an app called ragnar that is part of their tidyverse suite of packages.",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "07-acknowledgements.html",
    "href": "07-acknowledgements.html",
    "title": "16  Acknowledgements",
    "section": "",
    "text": "Many thanks to those that helped me in creating this guide (whether they knew it or not 😅).\n\nVictoria Yaneva, who really helped me upskill in all things related to NLP and AI.\nHadley Wickham. Aside from being generally inspirational in the quality of their work and open-science attitude, I largely modeled this Quarto book off his immensely helpful “R for Data Science” book. Being able to review the code used to make that book saved me a significant amount of time in learning Quarto and making this book.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Acknowledgements</span>"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "\n1  Test\n",
    "section": "",
    "text": "Inline test: 4\n\nCode2 + 2\n\n[1] 4",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Test</span>"
    ]
  },
  {
    "objectID": "08-ref-start.html",
    "href": "08-ref-start.html",
    "title": "References",
    "section": "",
    "text": "1. Vaswani A, Shazeer N, Parmar N, et al.\nAttention is all you need. Advances in neural information processing\nsystems. 2017;30.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "00b-R-packages.html#package-details",
    "href": "00b-R-packages.html#package-details",
    "title": "3  R Packages",
    "section": "\n3.2 Package details",
    "text": "3.2 Package details\n\nCodeinstall.packages(c(\"tidyverse\", \"curl\", \"ragnar\", \"duckdb\", \"shiny\", \"shinyjs\"))",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Packages</span>"
    ]
  },
  {
    "objectID": "08-acknowledgements.html",
    "href": "08-acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Many thanks to those that helped me in creating this guide (whether they knew it or not 😅).\n\nVictoria Yaneva, who really helped me upskill in all things related to NLP and AI.\nThe many kind, patient members of the NBME AI and Data Science team, who were always kind enough to help me learn, correct my mistakes, and point me in the direction of new things happening in AI.\n\nYiyun Zhou\nSaed Rezayi\nTazin Afrin\nAndrew Emerson\nKeelan Evanini\n\nHadley Wickham. Aside from being generally inspirational in the quality of their work and open-science attitude, I largely modeled this Quarto book off his immensely helpful “R for Data Science” book. Being able to review the code used to make that book saved me a significant amount of time in learning Quarto and making this book.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#sampling-controls",
    "href": "01b-gen-ai-parameters.html#sampling-controls",
    "title": "8  Generation Parameters",
    "section": "",
    "text": "8.1.1 Temperature\nTemperature controls how much randomness is introduced during text generation. A value near 0 produces deterministic, highly focused responses; higher values (e.g., 0.8–1.0) make the output more varied and creative. Statistically, it scales the logits before sampling, flattening or sharpening the probability distribution over possible next tokens. For reproducible outputs or grading tasks, low temperature is preferred; for brainstorming or ideation, higher values work better.\n\n\n8.1.2 top_p (Nucleus Sampling)\ntop_p defines how much of the total probability mass is considered when sampling the next token. The model first sorts possible next tokens by probability and keeps only the smallest set whose cumulative probability exceeds p.  For example, top_p = 0.9 means sampling only from the top 90% of the probability mass. This is another way to control diversity — lower values produce more predictable text.\n\n\n8.1.3 top_k\ntop_k restricts the number of candidate tokens the model can choose from at each step. If k = 50, only the 50 most likely next tokens are considered. This parameter is conceptually similar to top_p but framed in terms of count rather than probability. Many APIs use either top_p or top_k, but not both — using one usually provides enough control over randomness.\n\n\n8.1.4 Seed\nThe seed parameter fixes the random number generator used during sampling, ensuring that the same prompt and parameters produce identical outputs every time. It’s especially valuable for research or assessment contexts where reproducibility matters. Setting a seed makes model behavior more deterministic, which supports fair comparisons across conditions or versions of a prompt.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#length-and-structure-controls",
    "href": "01b-gen-ai-parameters.html#length-and-structure-controls",
    "title": "8  Generation Parameters",
    "section": "8.2 Length and Structure Controls",
    "text": "8.2 Length and Structure Controls\nThese parameters constrain how much or what kind of text the model can produce.\n\n8.2.1 Max Tokens\nmax_tokens sets the upper limit for how long the model’s output can be, measured in tokens (roughly pieces of words). If the model reaches this limit, it stops generating even if the thought or sentence isn’t complete. This parameter is useful for keeping outputs concise or fitting within budget constraints, since longer outputs consume more tokens (and thus cost more).\n\n\n8.2.2 Stop Sequences\nStop sequences define one or more strings that tell the model when to stop generating text. When the model outputs any of these sequences, generation ends immediately. This helps control response boundaries—useful for cutting off unwanted explanations or ensuring that responses end cleanly at a specific marker, such as “END SCORE” or “###”.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#bias-and-repetition-controls",
    "href": "01b-gen-ai-parameters.html#bias-and-repetition-controls",
    "title": "8  Generation Parameters",
    "section": "8.3 Bias and Repetition Controls",
    "text": "8.3 Bias and Repetition Controls\nThese parameters discourage certain token patterns.\n\n8.3.1 Frequency Penalty\nfrequency_penalty discourages the model from repeating the same words or phrases. It adjusts token probabilities based on how often they’ve already appeared in the current response. Higher values push the model to use more varied vocabulary, while lower or zero values allow freer repetition. It’s especially useful for generating longer outputs that shouldn’t sound redundant.\n\n\n8.3.2 Presence Penalty\nThe presence_penalty discourages the model from reusing tokens that have already appeared in the text. Unlike the frequency_penalty, which scales with repetition, the presence penalty applies whenever a token has occurred before, even once. Increasing this value nudges the model to introduce new concepts or vocabulary, which can make generated text more diverse and exploratory.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#prompt-components",
    "href": "01b-gen-ai-parameters.html#prompt-components",
    "title": "8  Generation Parameters",
    "section": "8.4 Prompt Components",
    "text": "8.4 Prompt Components\n\n8.4.1 System Prompt\nThe system prompt sets the model’s overall role, tone, or behavior—essentially, the “meta” instruction that defines how the model should interpret everything that follows. For example, it might specify “You are an R assistant who explains concepts clearly and uses examples.” This prompt influences style and scope across the entire conversation. Most often the default system prompt is set to “User”.\n\n\n8.4.2 User Prompt\nThe user prompt is the immediate question or task you’re asking the model to perform. It represents the actual input or query, such as “Write an R function that calculates bootstrapped confidence intervals.” Together, the system and user prompts define both who the model should be and what it should do—analogous to a function’s global defaults and its current arguments.\n\n\n8.4.3 Response Schema\nA response schema specifies the structure or format the model should follow when producing its output. For example, you might require responses in JSON with fields like “score” and “rationale”. Defining a schema encourages consistency across runs, simplifies parsing in R workflows, and reduces the need for post-processing or cleanup.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#deterministic-replicable-and-non-reproducible-sampling",
    "href": "01b-gen-ai-parameters.html#deterministic-replicable-and-non-reproducible-sampling",
    "title": "\n6  Generation Parameters\n",
    "section": "\n6.5 Deterministic, Replicable, and Non-Reproducible Sampling",
    "text": "6.5 Deterministic, Replicable, and Non-Reproducible Sampling\n\nCode# --- Helper Functions ------------------------------------------------------\n\nsoftmax &lt;- function(x) exp(x - max(x)) / sum(exp(x - max(x)))\n\nsample_next_token &lt;- function(logits, temperature = 1, top_p = 1) {\n  # Deterministic path\n  if (isTRUE(all.equal(temperature, 0))) {\n    return(which.max(logits))\n  }\n\n  # Apply temperature and convert to probabilities\n  p &lt;- softmax(logits / temperature)\n\n  # Nucleus (top_p) filtering: keep smallest prefix whose cumulative prob &gt;= top_p\n  ord &lt;- order(p, decreasing = TRUE)\n  p_sorted &lt;- p[ord]\n  cutoff_idx &lt;- which(cumsum(p_sorted) &gt;= top_p)[1]\n  if (is.na(cutoff_idx)) cutoff_idx &lt;- length(p_sorted)\n  keep &lt;- ord[seq_len(cutoff_idx)]\n  p_keep &lt;- p[keep] / sum(p[keep])\n\n  # Sample from truncated distribution\n  sample(keep, size = 1, prob = p_keep)\n}\n\ndraw_tokens &lt;- function(logits, n = 12, temperature = 0.7, top_p = 0.9) {\n  ids &lt;- integer(n)\n  for (i in seq_len(n)) {\n    ids[i] &lt;- sample_next_token(logits, temperature = temperature, top_p = top_p)\n  }\n  ids\n}\n\nid2token &lt;- function(id, vocab) vocab[id]\n\n# --- A larger \"vocabulary\" and fixed base logits (no RNG here) -------------\nvocab &lt;- c(\n  \"the\",\"patient\",\"reports\",\"fever\",\"no\",\"cough\",\"today\",\"denies\",\"nausea\",\"vomiting\",\n  \"chills\",\"sore\",\"throat\",\"pain\",\"abdominal\",\"headache\",\"shortness\",\"breath\",\"since\",\"yesterday\"\n)\n\n# Hand-tuned logits (arbitrary but stable). Higher = more likely.\nbase_logits &lt;- c(\n  1.20, 0.85, 1.05, 0.15, -0.20, 0.40, -0.10, 0.70, 0.55, 0.25,\n  0.35, -0.05, 0.30, 0.95, 0.10, 0.65, 0.50, 0.45, -0.15, 0.05\n)\n\n# A tiny \"model update\" nudging logits (deterministic noise)\nset.seed(1234)\nupdated_logits &lt;- base_logits + rnorm(length(base_logits), mean = 0, sd = 0.03)\n\n# --- 1) Deterministic (temperature = 0) ------------------------------------\ndet_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0, top_p = 1)\n\n# --- 2) Replicable (same seed + same params) --------------------------------\nset.seed(42)\nrepA_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\nset.seed(42)\nrepB_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\nset.seed(7)\nrepC_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\n# --- 3) Non-Reproducible (same seed/params, different logits) ---------------\nset.seed(42)\nupd_ids &lt;- draw_tokens(updated_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\n# --- Pretty printing --------------------------------------------------------\ncat(\"**Deterministic (temp = 0):**\\n\", paste(id2token(det_ids, vocab), collapse = \" \"), \"\\n\\n\")\n\n**Deterministic (temp = 0):**\n the the the the the the the the the the the the \n\nCodecat(\"**Replicable A (seed = 42):**\\n\", paste(id2token(repA_ids, vocab), collapse = \" \"), \"\\n\")\n\n**Replicable A (seed = 42):**\n fever fever pain chills nausea denies breath the shortness breath denies breath \n\nCodecat(\"**Replicable B (seed = 42):**\\n\", paste(id2token(repB_ids, vocab), collapse = \" \"), \"\\n\")\n\n**Replicable B (seed = 42):**\n fever fever pain chills nausea denies breath the shortness breath denies breath \n\nCodecat(\"**Different seed (seed = 7):**\\n\", paste(id2token(repC_ids, vocab), collapse = \" \"), \"\\n\\n\")\n\n**Different seed (seed = 7):**\n yesterday patient the the reports chills pain abdominal reports denies reports reports \n\nCodecat(\"**Updated model (seed = 42, new logits):**\\n\", paste(id2token(upd_ids, vocab), collapse = \" \"), \"\\n\\n\")\n\n**Updated model (seed = 42, new logits):**\n abdominal abdominal pain chills nausea denies breath the shortness breath denies breath \n\nCode# --- Compact comparison table ----------------------------------------------\ndf &lt;- data.frame(\n  pos = rep(1:12, 5),\n  run = rep(c(\"Deterministic\",\"Replicable_A_s42\",\"Replicable_B_s42\",\"DiffSeed_s7\",\"UpdatedModel_s42\"), each = 12),\n  token = c(id2token(det_ids, vocab),\n            id2token(repA_ids, vocab),\n            id2token(repB_ids, vocab),\n            id2token(repC_ids, vocab),\n            id2token(upd_ids, vocab))\n)\n\n# Quick checks: A vs B identical; A vs Updated often differs at multiple positions.\nidentical_A_B &lt;- identical(repA_ids, repB_ids)\nmatches_A_upd &lt;- sum(repA_ids == upd_ids)\n\ncat(sprintf(\"Check — A vs B identical: %s\\n\", ifelse(identical_A_B, \"YES\", \"NO\")))\n\nCheck — A vs B identical: YES\n\nCodecat(sprintf(\"Positions matching A vs Updated: %d of %d\\n\\n\", matches_A_upd, length(repA_ids)))\n\nPositions matching A vs Updated: 10 of 12\n\nCodeknitr::kable(df, caption = \"Deterministic, Replicable, and Non-Reproducible sequences (12 tokens)\")\n\n\nDeterministic, Replicable, and Non-Reproducible sequences (12 tokens)\n\npos\nrun\ntoken\n\n\n\n1\nDeterministic\nthe\n\n\n2\nDeterministic\nthe\n\n\n3\nDeterministic\nthe\n\n\n4\nDeterministic\nthe\n\n\n5\nDeterministic\nthe\n\n\n6\nDeterministic\nthe\n\n\n7\nDeterministic\nthe\n\n\n8\nDeterministic\nthe\n\n\n9\nDeterministic\nthe\n\n\n10\nDeterministic\nthe\n\n\n11\nDeterministic\nthe\n\n\n12\nDeterministic\nthe\n\n\n1\nReplicable_A_s42\nfever\n\n\n2\nReplicable_A_s42\nfever\n\n\n3\nReplicable_A_s42\npain\n\n\n4\nReplicable_A_s42\nchills\n\n\n5\nReplicable_A_s42\nnausea\n\n\n6\nReplicable_A_s42\ndenies\n\n\n7\nReplicable_A_s42\nbreath\n\n\n8\nReplicable_A_s42\nthe\n\n\n9\nReplicable_A_s42\nshortness\n\n\n10\nReplicable_A_s42\nbreath\n\n\n11\nReplicable_A_s42\ndenies\n\n\n12\nReplicable_A_s42\nbreath\n\n\n1\nReplicable_B_s42\nfever\n\n\n2\nReplicable_B_s42\nfever\n\n\n3\nReplicable_B_s42\npain\n\n\n4\nReplicable_B_s42\nchills\n\n\n5\nReplicable_B_s42\nnausea\n\n\n6\nReplicable_B_s42\ndenies\n\n\n7\nReplicable_B_s42\nbreath\n\n\n8\nReplicable_B_s42\nthe\n\n\n9\nReplicable_B_s42\nshortness\n\n\n10\nReplicable_B_s42\nbreath\n\n\n11\nReplicable_B_s42\ndenies\n\n\n12\nReplicable_B_s42\nbreath\n\n\n1\nDiffSeed_s7\nyesterday\n\n\n2\nDiffSeed_s7\npatient\n\n\n3\nDiffSeed_s7\nthe\n\n\n4\nDiffSeed_s7\nthe\n\n\n5\nDiffSeed_s7\nreports\n\n\n6\nDiffSeed_s7\nchills\n\n\n7\nDiffSeed_s7\npain\n\n\n8\nDiffSeed_s7\nabdominal\n\n\n9\nDiffSeed_s7\nreports\n\n\n10\nDiffSeed_s7\ndenies\n\n\n11\nDiffSeed_s7\nreports\n\n\n12\nDiffSeed_s7\nreports\n\n\n1\nUpdatedModel_s42\nabdominal\n\n\n2\nUpdatedModel_s42\nabdominal\n\n\n3\nUpdatedModel_s42\npain\n\n\n4\nUpdatedModel_s42\nchills\n\n\n5\nUpdatedModel_s42\nnausea\n\n\n6\nUpdatedModel_s42\ndenies\n\n\n7\nUpdatedModel_s42\nbreath\n\n\n8\nUpdatedModel_s42\nthe\n\n\n9\nUpdatedModel_s42\nshortness\n\n\n10\nUpdatedModel_s42\nbreath\n\n\n11\nUpdatedModel_s42\ndenies\n\n\n12\nUpdatedModel_s42\nbreath",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "00c-test-connect.html",
    "href": "00c-test-connect.html",
    "title": "3  Testing API Connection",
    "section": "",
    "text": "Many of the interactive portions of this workshop depend on successfully being able to connect to a generative AI model via API (we will discuss API keys in a little more depth later). For the purposes of this workshop I’ve set up an API key through Anthropic for you all to use (see Section 10.2).\nThe following is a very basic function for you to test that you can connect to Anthropic’s Claude Sonnet 4.5 model. As we progress through the workshop we’ll use a slightly different function that gives you more control over some of the model generation parameters.\nTo use this function you must have a .Renviron file in your working directory, as the function pulls the API key from that environment file. I will securely distribute this file to workshop participants (contact me if you have not yet received this file). If you’ve just added the .Renviron file to your directory, you’ll need to refresh your R session in order for it to be registered in the environment. You can do this via the tabs at the top of the R Studio interface: Session –&gt; Restart R.\nYou can download the following function by clicking the copy button (looks like a clipboard) in the code chunk displayed below. Alternatively, you can click the icon below:\n 📥 Download call_claude \n\nCode# Required packages\nlibrary(httr)\nlibrary(jsonlite)\n\ncall_claude &lt;- function(prompt,\n                        model = \"claude-sonnet-4-5-20250929\") {\n  \n  # Get API key from environment\n  # You will need to change this to your own API key after workshop\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n\n  # Convert text prompt to required message format\n  messages &lt;- list(list(role = \"user\", content = prompt))\n  \n  # Build request body\n  request_body &lt;- list(\n    model = model,\n    messages = messages,\n    max_tokens = 1024 # Required; will be an argument in other functions\n  )\n  \n  # Set up headers\n  headers &lt;- add_headers(\n    \"x-api-key\" = api_key,\n    \"anthropic-version\" = \"2023-06-01\",\n    \"content-type\" = \"application/json\"\n  )\n  \n  # Make the API request\n  response &lt;- POST(\n    url = \"https://api.anthropic.com/v1/messages\",\n    headers,\n    body = toJSON(request_body, auto_unbox = TRUE)\n  )\n  \n  # Check if request was successful\n  if (http_status(response)$category != \"Success\") {\n    stop(paste(\"API request failed:\", http_status(response)$message, \n               \"\\nDetails:\", content(response, \"text\", encoding = \"UTF-8\")))\n  }\n  \n  # Parse response and extract text content\n  result &lt;- fromJSON(content(response, \"text\", encoding = \"UTF-8\"))\n  return(as.character(result$content)[2])\n}\n\n\nThe only argument that needs to be supplied to the function is the prompt that you want to send to the model. This should be a text string that is enclosed by parentheses: “Tell me a joke about educational measurement.”\n\nCodetest_joke &lt;- call_claude(\"Tell me a joke about educational measurement.\")\n\ntest_joke\n\n[1] \"Why did the test item go to therapy?\\n\\nIt had too many validity issues and couldn't tell if it was really measuring what it was supposed to measure! \\n\\n(And its reliability was so low, it gave a different answer every time someone asked.)\"\n\n\nA few things to note:\n\nGenerally speaking, the process used by a generative AI model involves predicting the next token to be selected and sampling from a distribution of possible options. (We will discuss ways to control this later.) A (sometimes, mostly) beautiful implication of this is that responses from generative AI models will often be different, even if the exact same prompt is used. This is especially the case in the simple function above, as we haven’t made an effort to tune the model generation parameters to achieve a deterministic or replicable result. Thus, when you call Anthropic and have it generate a joke for you using the same prompt, you will likely get different responses each time. In fact, each time that I compile this book on GitHub the result changes!\nYou may notice that escape sequences (\\n, \\t, etc.) may be present because the information is returned in the JSON format. One way to handle this is to wrap your response in cat(). We’ll revisit this again later in other sections.\n\n\nCodecat(test_joke)\n\nWhy did the test item go to therapy?\n\nIt had too many validity issues and couldn't tell if it was really measuring what it was supposed to measure! \n\n(And its reliability was so low, it gave a different answer every time someone asked.)",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing API Connection</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html",
    "href": "00d-LLM-R-packages.html",
    "title": "4  LLM-specific R Packages",
    "section": "",
    "text": "4.1 ellmer\nA number of packages have been developed to more easily facilitate interacting with LLMs via R. Many of these packages are useful (we’ll cover some of those in the workshop), whereas other packages include some developer design decisions that don’t work particularly well for my usual workflows.\nBelow is a non-exhaustive list of packages that I’ve found to interact with LLMs. This is not meant to be exhaustive or a curated list; it’s only to provide you with information about the packages you’ll be using in the workshop (and others) in the case you find them helpful for your workflow. All package summaries were initially generated with AI. Some summaries have been edited, some have not.\nellmer Overview CRAN Documentation\nellmer is an R package that provides a unified interface for interacting with large language models from over 17 providers including OpenAI, Anthropic, Google Gemini, and AWS Bedrock. It supports advanced features like streaming outputs, tool/function calling, structured data extraction, and multimodal inputs. Chat objects are stateful and maintain conversation context, enabling both interactive console-based conversations and programmatic use in R scripts and applications.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#tidyprompt",
    "href": "00d-LLM-R-packages.html#tidyprompt",
    "title": "4  LLM-specific R Packages",
    "section": "4.2 tidyprompt",
    "text": "4.2 tidyprompt\ntidyprompt Overview CRAN Documentation\ntidyprompt is an R package that provides a compositional framework (“prompt wraps”) for building prompts enriched with logic, validation, and extraction functions when interacting with LLMs. It supports structured output, retry/feedback loops, reasoning strategies (e.g. ReAct or chain-of-thought), and even autonomous R code or function calling as part of an LLM dialogue. The package is provider-agnostic, meaning its features can layer on top of any chat completion API (e.g. via ellmer) to produce more robust, predictable interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#tidyllm",
    "href": "00d-LLM-R-packages.html#tidyllm",
    "title": "4  LLM-specific R Packages",
    "section": "4.3 tidyllm",
    "text": "4.3 tidyllm\ntidyllm Overview CRAN Documentation\ntidyllm provides a tidy, pipeline-friendly interface for interacting with multiple LLM APIs (e.g. Claude, OpenAI, Gemini, Mistral) and local models via Ollama. It supports multimodal inputs (text, images, PDFs), maintains conversational history, handles batching and rate limits, and allows structured schema-based extraction of responses. The design emphasizes composability and integration into typical R data workflows.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#chattr",
    "href": "00d-LLM-R-packages.html#chattr",
    "title": "4  LLM-specific R Packages",
    "section": "4.4 chattr",
    "text": "4.4 chattr\nchattr Overview CRAN Documentation\nchattr is an R package that enables interactive communication with large language models directly within RStudio using a Shiny gadget or from the console. It enriches prompts with contextual information (e.g. loaded data frames) and integrates with various back-ends (e.g. OpenAI, Copilot, local LlamaGPT) via the ellmer interface. The package is geared toward exploratory workflows and rapid prototyping of LLM-assisted analysis.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#llmagentr",
    "href": "00d-LLM-R-packages.html#llmagentr",
    "title": "4  LLM-specific R Packages",
    "section": "4.5 LLMAgentR",
    "text": "4.5 LLMAgentR\nLLMAgentR Overview CRAN Documentation\nLLMAgentR is an R package for constructing language model “agents” using a modular, graph-based execution framework inspired by LangChain/LangGraph architectures. It offers a suite of agent types (e.g. code generation, data wrangling, SQL agents, document summarization) that iteratively reason, generate R code, execute, debug, and explain results. The package aims to support reproducible AI workflows for analysis, research, and automation by integrating LLM reasoning and domain logic.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#packetllm",
    "href": "00d-LLM-R-packages.html#packetllm",
    "title": "4  LLM-specific R Packages",
    "section": "4.6 PacketLLM",
    "text": "4.6 PacketLLM\nPacketLLM Overview CRAN Documentation\nPacketLLM offers an interactive RStudio gadget interface for chatting with OpenAI LLMs (e.g. GPT-5 and variants) directly within the R environment. It supports multiple simultaneous conversation tabs, file upload (e.g. .R, PDF, DOCX) as contextual input, and per-conversation system message configuration. API calls are handled asynchronously (via promises + future) to avoid blocking the R console during model interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#what-is-generative-ai",
    "href": "01-gen-ai-fundamentals.html#what-is-generative-ai",
    "title": "Generative AI Fundamentals",
    "section": "",
    "text": "Comparison of AI and NLP",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html#co-occurrence",
    "href": "01a-gen-ai-basics.html#co-occurrence",
    "title": "5  Foundational Principles",
    "section": "5.2 Co-occurrence",
    "text": "5.2 Co-occurrence",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "03a-api-keys.html#sec-workshop-key",
    "href": "03a-api-keys.html#sec-workshop-key",
    "title": "4  API Keys",
    "section": "",
    "text": "4.1.1 Calling the Model via API\nA very simple function to call the Anthropic Claude Sonnet 4.5 model is here: ?sec-call-claude. Below I’ve made a stripped-down version of the model function, indicating what information needs to be changed if you want to use another model.\n\nCode# Required packages\nlibrary(httr)\nlibrary(jsonlite)\n\n# MODEL_NAME needs to be specified - can be found on the provider's API documentation page\n\ncall_claude &lt;- function(prompt,\n                        model = \"MODEL_NAME\") {\n  \n  # Get API key from environment\n  # You will need to change this to your own API key after workshop\n  api_key &lt;- Sys.getenv(\"\") # Need have the API key in your .Renviron file\n\n  # Convert text prompt to required message format\n  messages &lt;- list(list(role = \"user\", content = prompt))\n  \n  # Build request body\n  request_body &lt;- list(\n    model = model,\n    messages = messages,\n    max_tokens = 1024 # Required; will be an argument in other functions\n  )\n  \n  # Set up headers\n  headers &lt;- add_headers(\n    \"x-api-key\" = api_key,\n    # \"anthropic-version\" = \"2023-06-01\", # This line will need to be changed\n    \"content-type\" = \"application/json\"\n  )\n  \n  # Make the API request\n  response &lt;- POST(\n    # url = \"https://api.anthropic.com/v1/messages\", # This line will need to be changed\n    headers,\n    body = toJSON(request_body, auto_unbox = TRUE)\n  )\n  \n  # Check if request was successful\n  if (http_status(response)$category != \"Success\") {\n    stop(paste(\"API request failed:\", http_status(response)$message, \n               \"\\nDetails:\", content(response, \"text\", encoding = \"UTF-8\")))\n  }\n  \n  # Parse response and extract text content\n  # This section may also need to be modified depending on how the provider returns model output.\n  result &lt;- fromJSON(content(response, \"text\", encoding = \"UTF-8\"))\n  return(as.character(result$content)[2])\n}\n\n\n\n4.1.2 OpenAI ChatGPT5 example\nHere is a version of a function you can use to call OpenAI’s ChatGPT5. Note that calling this model has two generation parameters: verbosity and reasoning_effort, which you can read more about here (cookbook.openai.com) and here (latest model page the OpenAI Platform; will change when new model is released.)\n\nCodelibrary(httr)\nlibrary(jsonlite)\n\ncall_gpt5 &lt;- function(prompt,\n                      model = \"gpt-5\",\n                      verbosity = NULL,           # \"low\", \"medium\", \"high\"\n                      reasoning_effort = NULL) {  # \"minimal\", \"low\", \"medium\", \"high\"\n\n  api_key &lt;- Sys.getenv(\"OPENAI_API_KEY\")\n  if (api_key == \"\") {\n    stop(\"OpenAI API key is not set. Please set OPENAI_API_KEY in your environment.\")\n  }\n  \n  messages &lt;- list(list(role = \"user\", content = prompt))\n  \n  body_list &lt;- list(\n    model = model,\n    messages = messages,\n    max_tokens = 1024,\n  )\n  \n  # Add GPT-5 specific parameters only if non-NULL\n  params &lt;- list()\n\n  headers &lt;- add_headers(\n    Authorization = paste(\"Bearer\", api_key),\n    `Content-Type` = \"application/json\"\n  )\n  \n  resp &lt;- POST(\n    url = \"https://api.openai.com/v1/chat/completions\",\n    headers,\n    body = toJSON(body_list, auto_unbox = TRUE)\n  )\n  \n  if (http_status(resp)$category != \"Success\") {\n    stop(paste0(\"API request failed (\", http_status(resp)$message, \"): \",\n                content(resp, \"text\", encoding = \"UTF-8\")))\n  }\n  \n  res &lt;- fromJSON(content(resp, \"text\", encoding = \"UTF-8\"), simplifyVector = TRUE)\n  \n  # Extract the assistant content\n  return(res$choices[[1]]$message$content)\n}\n\n# Example usages:\n# 1. Default call (default verbosity & reasoning, both = \"medium\")\n# result &lt;- call_gpt5(\"Explain item response theory in simple terms.\")\n\n# 2. With lowest verbosity and reasoning settings:\n# result2 &lt;- call_gpt5(\"Explain item response theory in simple terms.\",\n#                      verbosity = \"low\",\n#                      reasoning_effort = \"minimal\")\n# cat(result2)\n\n# 3. With highest verbosity and reasoning settings:\n# result2 &lt;- call_gpt5(\"Explain item response theory in simple terms.\",\n#                      verbosity = \"high\",\n#                      reasoning_effort = \"high\")\n# cat(result2)",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>API Keys</span>"
    ]
  },
  {
    "objectID": "06-references.html",
    "href": "06-references.html",
    "title": "References",
    "section": "",
    "text": "1. Vaswani A, Shazeer N, Parmar N, et al.\nAttention is all you need. Advances in neural information processing\nsystems. 2017;30.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "07-reference-materials.html",
    "href": "07-reference-materials.html",
    "title": "Reference Materials",
    "section": "",
    "text": "Background Information on GPTs\nThese materials / links were last checked on October 2, 2025. All apologies for links that no longer work. Please email me at CRunyon@nbme.org if you notice something no longer works so I can change / remove the link.\nThe 3Blue1Brown YouTube Channel provides several good videos on the some of the technical aspects of large language models.\nAnthropic’s paper On the Biology of a Large Language Model is particularly interesting.\nThis post on lesswrong provides a nice high-level summary for understanding LLMs.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#background-information-on-gpts",
    "href": "07-reference-materials.html#background-information-on-gpts",
    "title": "Reference Materials",
    "section": "",
    "text": "The Neural Networks section is particularly informative.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#reference-guides-and-prompt-engineering",
    "href": "07-reference-materials.html#reference-guides-and-prompt-engineering",
    "title": "Reference Materials",
    "section": "Reference Guides (and Prompt Engineering)",
    "text": "Reference Guides (and Prompt Engineering)\nOpenAI Cookbook",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#shiny-related-resources",
    "href": "07-reference-materials.html#shiny-related-resources",
    "title": "Reference Materials",
    "section": "Shiny-related Resources",
    "text": "Shiny-related Resources\nPosit has many useful things.\n\nMaterials for the R language start here.\nMaterials for the Python language start here.\n\nnanxstats has nicely organized many Shiny extension packages.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#resources-for-continuing-development",
    "href": "07-reference-materials.html#resources-for-continuing-development",
    "title": "Reference Materials",
    "section": "Resources for Continuing Development",
    "text": "Resources for Continuing Development\nThe following list of books / blogs / newsletters / training is a compilation of resources that have either I found useful or have been recommended by colleagues.\n\nNewsletters\n\n\nBooks\n\n\nBlogs\n\n\nTraining",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#legal-considerations",
    "href": "07-reference-materials.html#legal-considerations",
    "title": "Reference Materials",
    "section": "Legal Considerations",
    "text": "Legal Considerations\nThe information provided here is for general informational purposes only and does not constitute legal advice. You should not act upon any information presented without first seeking qualified legal counsel regarding your specific situation. The authors disclaim any liability for actions taken based on the content provided here.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#resources",
    "href": "02-prompt-engineering.html#resources",
    "title": "Prompt Engineering",
    "section": "Resources",
    "text": "Resources\nAnthropic’s Prompt Engineering Guide\n\nOpenAI’s Prompt Engineering Guide\n\nGoogle Gemini’s Prompt Engineering Guide\n\nIncludes directions on uploading a file via API as part of a prompt.\n\n\npromptingguide.ai is a great resources for learning more about prompt engineering techniques.",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html",
    "href": "activity-parameter-testing.html",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "",
    "text": "9.1 claude_plus function\nThis activity is designed to allow you to get hands-on experience playing around with model generation parameters. To do so, we need to modify the initial call_claude function to incorporate specifying the generation parameters.\nThe parameters that we will be testing are temperature,top_p, and top_k. (max_tokens is also included, but we won’t be using that in our testing.) Note that for our particular model (Claude Sonnet 4.5), you cannot specify temperature and top_p at the same time.\nYou can download the following function by clicking the copy button (looks like a clipboard) in the code chunk displayed below. Alternatively, you can click the icon below:\n📥 Download claude_plus\nCodelibrary(httr)\nlibrary(jsonlite)\n\nclaude_plus &lt;- function(prompt,\n                        model = \"claude-sonnet-4-5-20250929\",\n                        temperature = NULL,\n                        top_p = NULL,\n                        max_tokens = 1024) {\n  \n  # Check if both temperature and top_p are supplied\n  if (!is.null(temperature) && !is.null(top_p)) {\n    warning(\"Both temperature and top_p arguments are supplied. The Anthropic API does not support using both simultaneously. Only temperature will be used.\")\n  }\n  \n  # Get API key from environment\n  # You will need to change this to your own API key after workshop\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n  # Convert text prompt to required message format\n  messages &lt;- list(list(role = \"user\", content = prompt))\n  \n  # Build request body\n  request_body &lt;- list(\n    model = model,\n    messages = messages,\n    max_tokens = max_tokens # Required; will be an argument in other functions\n  )\n  \n  # Add temperature if provided (takes precedence over top_p)\n  if (!is.null(temperature)) {\n    request_body$temperature &lt;- temperature\n  } else if (!is.null(top_p)) {\n    # Only add top_p if temperature is not provided\n    request_body$top_p &lt;- top_p\n  }\n  \n  # Set up headers\n  headers &lt;- add_headers(\n    \"x-api-key\" = api_key,\n    \"anthropic-version\" = \"2023-06-01\",\n    \"content-type\" = \"application/json\"\n  )\n  \n  # Make the API request\n  response &lt;- POST(\n    url = \"https://api.anthropic.com/v1/messages\",\n    headers,\n    body = toJSON(request_body, auto_unbox = TRUE)\n  )\n  \n  # Check if request was successful\n  if (http_status(response)$category != \"Success\") {\n    stop(paste(\"API request failed:\", http_status(response)$message, \n               \"\\nDetails:\", content(response, \"text\", encoding = \"UTF-8\")))\n  }\n  \n  # Parse response and extract text content\n  result &lt;- fromJSON(content(response, \"text\", encoding = \"UTF-8\"))\n  return(as.character(result$content)[2])\n}\nUnderstanding how these generation parameters change model output is more obvious when you can see the different outputs from the model using the same parameters. I’ve made the claude_param_test function to help with this - it includes a n_reps argument (default = 5, max of 20) that replicates the call several times.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#sec-claude-param-test",
    "href": "activity-parameter-testing.html#sec-claude-param-test",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.2 claude_param_test function",
    "text": "9.2 claude_param_test function\nYou can download the following function by clicking the copy button (looks like a clipboard) in the code chunk displayed below. Alternatively, you can click the icon below:\n 📥 Download claude_param_test \n\nCodeclaude_param_test &lt;- function(prompt,\n                              temperature = NULL,\n                              top_p = NULL,\n                              top_k = NULL,\n                              max_tokens = 1024,\n                              n_reps = 5) {\n  \n  # Check if n_reps is within allowed range\n  if (n_reps &gt; 20) {\n    warning(\"n_reps exceeds maximum allowed value of 20. Setting n_reps to 20.\")\n    n_reps &lt;- 20\n  }\n  \n  if (n_reps &lt; 5) {\n    warning(\"n_reps is below minimum value of 5. Setting n_reps to 5.\")\n    n_reps &lt;- 5\n  }\n  \n  # Initialize results dataframe\n  results &lt;- data.frame(\n    rep_n = integer(),\n    temp = numeric(),\n    top_p = numeric(),\n    top_k = integer(),\n    output = character(),\n    stringsAsFactors = FALSE\n  )\n  \n  # Loop through n_reps\n  for (i in 1:n_reps) {\n    # Call claude_plus\n    output &lt;- claude_plus(\n      prompt = prompt,\n      temperature = temperature,\n      top_p = top_p,\n      top_k = top_k,\n      max_tokens = max_tokens\n    )\n    \n    # Add to results\n    results &lt;- rbind(results, data.frame(\n      rep_n = i,\n      temp = ifelse(is.null(temperature), NA, temperature),\n      top_p = ifelse(is.null(top_p), NA, top_p),\n      top_k = ifelse(is.null(top_k), NA, top_k),\n      output = output,\n      stringsAsFactors = FALSE\n    ))\n    \n    # Sleep between calls (except after last call)\n    if (i &lt; n_reps) {\n      Sys.sleep(0.2)\n    }\n  }\n  \n  return(results)\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#quick-reference-generation-parameter-table",
    "href": "activity-parameter-testing.html#quick-reference-generation-parameter-table",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.3 Quick Reference Generation Parameter Table",
    "text": "9.3 Quick Reference Generation Parameter Table\n\n\n\n\n\n\n\nGeneration Parameter\nDescription\nRange\n\n\n\ntemperature\nAffects creativity by changing the probability distribution when choosing the next token\n\\(0 \\leq \\texttt{temperature} \\leq 1\\)\n\n\ntop_p\nUses nucleus sampling to limit token choices by selecting options that make up the top_p proportion of options\n\\(0 &lt; \\texttt{top\\_p} \\leq 1\\)\n\n\ntop_k\nSamples from only the top k most probable tokens\n\\(\\texttt{top\\_k} \\geq 1\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-1-high-temperature",
    "href": "activity-parameter-testing.html#task-1-high-temperature",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.4 Task 1: High temperature\n",
    "text": "9.4 Task 1: High temperature\n\nFor each of the following tasks, you’re welcome to use my prompt or update the syntax with your own.\n\nCodehigh_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                temperature = 1,\n                                n_reps = 10)\nsave(high_pizza, file = \"./data/high_pizza.R\")\n\n\n\nCodelibrary(knitr)\nload(\"data/high_pizza.R\")\n  kable(high_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\n1\nNA\nNA\nPerfect combo of savory cheese, tangy sauce, crispy crust, and endless delicious toppings.\n\n\n2\n1\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory, salty, satisfying flavors.\n\n\n3\n1\nNA\nNA\nPerfect blend of cheese, sauce, and toppings on crispy-chewy crust hits all taste centers.\n\n\n4\n1\nNA\nNA\nPerfect combo of melted cheese, savory sauce, crispy crust, and endless topping possibilities.\n\n\n5\n1\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible flavors and textures.\n\n\n6\n1\nNA\nNA\nPerfect balance of carbs, cheese, and toppings plus crispy-chewy texture creates ultimate comfort food.\n\n\n7\n1\nNA\nNA\nPerfect blend of savory cheese, tangy sauce, and crispy crust hits all taste receptors.\n\n\n8\n1\nNA\nNA\nPerfect combination of cheese, sauce, bread, and toppings creates satisfying flavors and textures.\n\n\n9\n1\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates an irresistible savory flavor explosion.\n\n\n10\n1\nNA\nNA\nPerfect combo of crispy crust, tangy sauce, melted cheese, and endless topping possibilities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-2-low-temperature",
    "href": "activity-parameter-testing.html#task-2-low-temperature",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.5 Task 2: Low temperature\n",
    "text": "9.5 Task 2: Low temperature\n\n\nCodelow_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                temperature = 0,\n                                n_reps = 10)\nsave(low_pizza, file = \"./data/low_pizza.R\")\n\n\n\nCodeload(\"data/low_pizza.R\")\nkable(low_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n2\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n3\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n4\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n5\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n6\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n7\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n8\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n9\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n10\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n\n\n\nNote although the results from this experiment show that the output was identical across the 10 calls, this was by chance; setting the temperature to 0 does not guarantee the exact same outputs will be obtained across different calls.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-3-low-temperature-longer-output",
    "href": "activity-parameter-testing.html#task-3-low-temperature-longer-output",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.6 Task 3: Low temperature, longer output",
    "text": "9.6 Task 3: Low temperature, longer output\nI repeated the above task, but increased max_token = 2048 and use the prompt of “Write a short essay about the importance of educational measurement.” I was shocked to see that all the essays were still identical!\n\nCodeedmeasure_low &lt;- claude_param_test(\"Write a short essay about the importance of educational measurement.\",\n                                   temperature = 0,\n                                   n_reps = 20,\n                                   max_tokens = 2048)\nsave(edmeasure_low, file = \"./data/edmeasure_low.R\")\n\n\n\nCodeload(\"data/edmeasure_low.R\")\n\n\n\n\nEssay 1\nEssay 2\nEssay 3\nEssay 4\nEssay 5\n\n\n\n\n10 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n11 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n12 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n13 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n14 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-4-high-top_p",
    "href": "activity-parameter-testing.html#task-4-high-top_p",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n14.1 Task 4: High top_p\n",
    "text": "14.1 Task 4: High top_p\n\n\nCodehighp_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                 top_p = .90,\n                                 n_reps = 10)\nsave(highp_pizza, file = \"./data/highp_pizza.R\")\n\n\n\nCodeload(\"data/highp_pizza.R\")\nkable(highp_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\nNA\n0.9\nNA\nPerfect blend of cheese, sauce, bread, and toppings creates irresistible savory comfort food.\n\n\n2\nNA\n0.9\nNA\nPerfect combo of crispy crust, tangy sauce, melted cheese, and endless topping possibilities.\n\n\n3\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n4\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n5\nNA\n0.9\nNA\nPerfect blend of carbs, cheese, and toppings creates an irresistible savory, satisfying comfort food.\n\n\n6\nNA\n0.9\nNA\nPerfect combo of melty cheese, tangy sauce, crispy crust, and endless topping possibilities.\n\n\n7\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and satisfying textures.\n\n\n8\nNA\n0.9\nNA\nPerfect combo of crispy crust, tangy sauce, melted cheese, and endless topping possibilities.\n\n\n9\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory, satisfying flavors.\n\n\n10\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-5-low-top_p",
    "href": "activity-parameter-testing.html#task-5-low-top_p",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n14.2 Task 5: Low top_p\n",
    "text": "14.2 Task 5: Low top_p\n\n\nCodelowp_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                top_p = .10,\n                                n_reps = 10)\nsave(lowp_pizza, file = \"./data/lowp_pizza.R\")\n\n\n\nCodeload(\"data/lowp_pizza.R\")\nkable(lowp_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n2\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n3\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n4\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n5\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n6\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n7\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n8\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n9\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n10\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#quick-start",
    "href": "04b-conversations-via-api.html#quick-start",
    "title": "\n15  Chat Conversations via API\n",
    "section": "",
    "text": "Click to see first prompt and response\n\n\n\n\n\n\nCodechat$chat(\"Tell me about the history of the exploration of the moon\")\n\n# The History of Lunar Exploration\n\nThe exploration of the Moon represents one of humanity's greatest achievements,\nspanning from ancient observations to modern robotic missions.\n\n## Ancient and Early Modern Observations\n- **Ancient civilizations** tracked lunar phases and eclipses, creating some of\nthe first astronomical records\n- **1609**: Galileo Galilei used his telescope to make the first detailed \nobservations of the Moon's surface, discovering mountains and craters\n- **17th-19th centuries**: Astronomers mapped the Moon and named its features\n\n## Early Space Age (1950s-1960s)\n### Soviet Achievements\n- **Luna 1 (1959)**: First spacecraft to reach the vicinity of the Moon\n- **Luna 2 (1959)**: First human-made object to reach the Moon's surface\n- **Luna 3 (1959)**: First images of the Moon's far side\n- **Luna 9 (1966)**: First successful soft landing\n\n### U.S. Programs\n- **Ranger program**: Crash-landed probes that sent back images during descent\n- **Surveyor program**: Soft-landing missions to test lunar surface conditions\n- **Lunar Orbiter program**: Mapped the Moon to select Apollo landing sites\n\n## The Apollo Era (1961-1972)\n- **Apollo 8 (1968)**: First crewed mission to orbit the Moon\n- **Apollo 11 (July 20, 1969)**: Neil Armstrong and Buzz Aldrin became the \nfirst humans to walk on the Moon\n- **Six successful landings**: Apollo 11, 12, 14, 15, 16, and 17\n- **Scientific achievements**: Collected 842 pounds of lunar samples, conducted\nexperiments, and deployed scientific instruments\n\n## Post-Apollo Period (1970s-1990s)\n- **Soviet Luna program continued**: Luna 16, 20, and 24 returned samples \nrobotically\n- **Lunokhod rovers**: First successful robotic rovers on another celestial \nbody\n- **Long hiatus**: Limited lunar activity during the 1980s and early 1990s\n\n## Modern Lunar Renaissance (1990s-Present)\n### Major Missions\n- **Clementine (1994)**: First evidence of water ice at lunar poles\n- **Lunar Prospector (1998)**: Confirmed water ice evidence\n- **SMART-1 (2003)**: European Space Agency's first lunar mission\n- **Chang'e program**: China's successful lunar exploration series, including \nsample returns\n- **Chandrayaan missions**: India's lunar exploration program\n- **Artemis program**: NASA's current plan to return humans to the Moon\n\n### Recent Achievements\n- **2019**: China's Chang'e 4 achieved the first soft landing on the far side\n- **2020**: China's Chang'e 5 returned fresh lunar samples\n- **Ongoing**: Multiple nations and private companies planning lunar missions\n\n## Scientific Impact\nLunar exploration has revealed that the Moon likely formed from a giant impact,\nhelped us understand planetary formation, and provided insights into early \nEarth history. The Moon continues to be a target for scientific research and a \nstepping stone for deeper space exploration.\n\nThe story of lunar exploration reflects humanity's drive to explore and \nunderstand our cosmic neighborhood, involving international collaboration and \ncompetition that has advanced our technological capabilities immensely.\n\n\n\n\n\n\n\n\n\n\n\nClick to see follow-up prompt and response\n\n\n\n\n\n\nCodechat$chat(\"What are the most important non-USA exploration missions?\")\n\n# Most Important Non-USA Lunar Exploration Missions\n\n## Soviet Union/Russia - The Pioneers\n\n### Luna Program Breakthroughs\n- **Luna 2 (1959)**: First human-made object to reach the Moon\n- **Luna 3 (1959)**: Provided first images of the Moon's far side, \nrevolutionizing our understanding\n- **Luna 9 (1966)**: First successful soft landing, proving the surface could \nsupport spacecraft\n- **Luna 16 (1970)**: First robotic sample return mission, bringing back 101 \ngrams of lunar soil\n- **Luna 17/Lunokhod 1 (1970)**: First successful planetary rover, operated for\n11 months\n- **Lunokhod 2 (1973)**: More advanced rover that traveled 39 km and returned \nover 80,000 images\n\n**Impact**: The Soviet missions achieved most of the major \"firsts\" in lunar \nexploration and proved that robotic missions could accomplish complex \nscientific tasks.\n\n## China - The Modern Leader\n\n### Chang'e Program\n- **Chang'e 1 & 2**: Successful orbital missions that mapped the Moon\n- **Chang'e 3/Yutu (2013)**: First soft landing since 1976, deployed the Yutu \nrover\n- **Chang'e 4/Yutu-2 (2019)**: **Historic first soft landing on the far side** \nof the Moon, using a relay satellite for communication\n- **Chang'e 5 (2020)**: First sample return mission in 44 years, brought back \n1.7 kg of fresh samples from a previously unexplored region\n\n**Impact**: Revitalized global lunar exploration and demonstrated China's \nadvanced space capabilities.\n\n## European Space Agency\n\n### SMART-1 (2003-2006)\n- First European lunar mission\n- Used revolutionary ion propulsion\n- Discovered new details about lunar composition and the theory of Moon \nformation\n- **Impact**: Proved Europe's deep space capabilities and advanced propulsion \ntechnology\n\n## India\n\n### Chandrayaan Program\n- **Chandrayaan-1 (2008)**: Made crucial discovery of water molecules across \nthe lunar surface using NASA's Moon Mineralogy Mapper\n- **Chandrayaan-2 (2019)**: Though the lander crashed, the orbiter continues \nsuccessful operations\n- **Chandrayaan-3 (2023)**: Successfully soft-landed near the lunar south pole\n\n**Impact**: Major scientific discoveries about lunar water and demonstrated \nIndia's cost-effective space exploration approach.\n\n## Japan\n\n### SELENE/Kaguya (2007-2009)\n- Most comprehensive lunar observation mission since Apollo\n- Created detailed gravitational and topographical maps\n- Provided stunning high-definition video of Earth-rise from lunar orbit\n- **Impact**: Advanced our understanding of lunar internal structure and \nevolution\n\n## Israel (Private/International)\n\n### Beresheet (2019)\n- First privately funded lunar mission\n- Though it crashed during landing, it demonstrated that smaller nations and \nprivate entities could attempt lunar exploration\n- **Impact**: Opened new models for lunar exploration beyond major space \nagencies\n\n## Most Scientifically Significant Contributions\n\n1. **Luna 3's far side images** - Revealed the Moon's asymmetry\n2. **Chang'e 4's far side landing** - Enabled study of the Moon's most ancient \nterrain\n3. **Chandrayaan-1's water discovery** - Fundamentally changed our \nunderstanding of the Moon\n4. **Chang'e 5's sample return** - Provided new insights into lunar geological \ntimeline\n5. **Soviet robotic sample returns** - Proved automated exploration \ncapabilities\n\n## Current Global Lunar Landscape\n\nToday, lunar exploration is truly international, with active programs from:\n- **China**: Most active current program\n- **India**: Cost-effective missions with major discoveries\n- **Russia**: Planning Luna-25 and future missions\n- **Japan, South Korea, UAE**: All have current lunar programs\n- **Private companies**: SpaceX, Intuitive Machines, and others\n\nThese non-USA missions have been crucial in maintaining momentum in lunar \nscience, making fundamental discoveries, and ensuring that lunar exploration \nremains a global endeavor rather than the achievement of a single nation.",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#chat_anthropic-details",
    "href": "04b-conversations-via-api.html#chat_anthropic-details",
    "title": "\n15  Chat Conversations via API\n",
    "section": "\n15.2 chat_anthropic Details",
    "text": "15.2 chat_anthropic Details\nLet’s look at the details of chat_anthropic (from this page of the ellmer package reference.)\n\nCodechat_anthropic(\n  system_prompt = NULL,\n  params = NULL,\n  max_tokens = deprecated(),\n  model = NULL,\n  api_args = list(),\n  base_url = \"https://api.anthropic.com/v1\",\n  beta_headers = character(),\n  api_key = anthropic_key(),\n  api_headers = character(),\n  echo = NULL\n)\n\n\nIt’s important to note the params argument, which allows you to set a variety of model parameters when chatting with the model. This is a general argument in the ellmer package. You’ll need to ensure that your model input allows a specific generation parameter before including it in your call.\nThis is one place where having a conversation with a model via API instead of via a chatbot interface is different - it’s not always easy (and sometimes impossible) to change these parameters in the normal chat interface.\n\nCodeparams(\n  temperature = NULL,\n  top_p = NULL,\n  top_k = NULL,\n  frequency_penalty = NULL,\n  presence_penalty = NULL,\n  seed = NULL,\n  max_tokens = NULL,\n  log_probs = NULL,\n  stop_sequences = NULL,\n  ...\n)",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#options-for-clearing-the-chat",
    "href": "04b-conversations-via-api.html#options-for-clearing-the-chat",
    "title": "\n15  Chat Conversations via API\n",
    "section": "\n15.3 Options for Clearing the Chat",
    "text": "15.3 Options for Clearing the Chat\nThere are two methods to reset the chat history. This is useful when you want to start a conversation about another topic.\n\n15.3.1 Clearing While Maintaining Chat Configuration\nThe following syntax simply clears the turns but maintains the other aspects of the chat configuration (which I’ll discuss momentarily in ?sec-chat-config). In the background the ellmer package is saving a history of your prompts and model responses, and it sending this history as part of the prompt when you send a new prompt. This is also happens when having a conversation with a chatbot, but it’s even less obvious.\n\nCodechat$set_turns(list())\n\n\n\n15.3.2 Clearing All Chat Settings\nThis starts an entirely new chat with the Anthropic model, and removes any settings you’ve made (system prompt, parameters). You can also include this in your argument - the important part is that using chat_anthropic() again resets any previously-specified chat configuration.\n\nCodechat &lt;- chat_anthoropic",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#system-prompt",
    "href": "04b-conversations-via-api.html#system-prompt",
    "title": "\n15  Chat Conversations via API\n",
    "section": "\n15.4 System Prompt",
    "text": "15.4 System Prompt\nWe briefly discussed system prompts in the section about generative parameters (see ?sec-system-prompt). The system prompt is an instruction to the model that is maintained throughout all of your interactions with the model. I don’t generally use system prompts when calling models via API, but I probably should. 😅 Nonetheless, let’s see how changing the system prompt can change the model output.\nFirst, with no setting of the system prompt:\n\nCodechat &lt;- chat_anthropic()\n\nUsing model = \"claude-sonnet-4-20250514\".\n\nCodechat$chat(\"Briefly tell me the point of using a Rasch model.\")\n\nThe main point of using a Rasch model is to create **linear, interval-level \nmeasurements** from ordinal data (like test scores or survey responses).\n\nKey benefits:\n- **Person-item separation**: Measures both person ability and item difficulty \non the same scale\n- **Invariant measurement**: Person ability estimates don't depend on which \nspecific items were used\n- **Missing data handling**: Can estimate scores even with incomplete responses\n- **Quality control**: Identifies misfitting items or response patterns that \ndon't follow the expected pattern\n\nIt's particularly valuable in educational testing, psychological assessments, \nand surveys where you want to convert raw scores into meaningful, comparable \nmeasurements that behave like true interval scales (equal differences between \nscale points).\n\n\nNow using a playful system prompt:\n\nCodechat &lt;- chat_anthropic(\n  system_prompt = \"You are an assistant that likes to respond in rhymes.\"\n)\n\nUsing model = \"claude-sonnet-4-20250514\".\n\nCodechat$chat(\"Briefly tell me the point of using a Rasch model.\")\n\nThe Rasch model's aim is quite neat,\nTo make measurement fair and complete!\nIt separates trait from task difficulty,\nEnsuring responses show true ability.\n\nWith invariant measures it does provide,\nPerson and item parameters reside\nOn the same scale with equal intervals true,\nMaking comparisons meaningful too!\n\nIn short, it's designed to be:\nA tool for measurement consistency!\n\n\nSome more helpful examples of good system prompts are:\n\n\nspecifying output structure\n\n“Always respond in JSON format with keys: ‘answer’, ‘confidence’, ‘sources’. Never include any text outside the JSON object.”\n\n\n\nensuring constraints\n\n“You are a medical information assistant. Always:\n\n\nEmphasize you’re not a doctor\nRecommend consulting healthcare professionals\nCite medical sources when possible\nNever diagnose conditions”",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html",
    "href": "activity-content-development-revision.html",
    "title": "Activity: Content Development and Revision",
    "section": "",
    "text": "Task 1 : Pros and Cons\nChoose a topic and ask the model to generate a list of pros and cons for each side. You can use one of the suggestions below or pick your own:\nSuggested prompt: “What are the pros and cons of {topic}? For each point, include a brief explanation.”\nReview the suggested pros and cons. Which points seem most accurate or well-supported? Are any based on weaker reasoning or questionable information?\nCodelibrary(ellmer)\n\nchat &lt;- chat_anthropic()\n\nchat$chat(\"What are the pros and cons of Bayesian vs Frequentist statistics? For each point, include a brief explanation.\")",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#pros-and-cons",
    "href": "activity-content-development-revision.html#pros-and-cons",
    "title": "Activity: Content Development and Revision",
    "section": "Bayesian Statistics",
    "text": "Bayesian vs Frequentist Statistics\nClassical Test Theory vs Item Response Theory\nUnidimensional / Rasch Modeling vs Multidimensional IRT Models\nArgument-Based Validity (Kane) vs Traditional Validity Frameworks (Messick)\nPerformance Assessments vs Selected-Response Items\nStatistical Significance vs Practical Significance in DIF Detection\n\n\n\n\n\n\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\nBayesian Statistics\nPros:\n1. Incorporates Prior Knowledge - Can integrate existing information, expert opinion, or historical data into analysis - Particularly valuable when data is limited or expensive to collect\n2. Intuitive Probability Interpretation - Treats parameters as random variables with probability distributions - Results directly answer questions like “What’s the probability this hypothesis is true?”\n3. Natural Uncertainty Quantification - Credible intervals represent actual probability ranges for parameters - Provides full posterior distributions, not just point estimates\n4. Flexible Model Building - Hierarchical models handle complex data structures naturally - Can easily incorporate missing data and measurement error\n5. Sequential Learning - Today’s posterior becomes tomorrow’s prior - Naturally updates beliefs as new data arrives\nCons:\n1. Subjective Prior Selection - Choice of prior can influence results, especially with small datasets - Different analysts may choose different priors, leading to different conclusions\n2. Computational Complexity - Often requires sophisticated algorithms (MCMC, variational inference) - Can be computationally intensive for complex models\n3. Interpretation Challenges - Results depend on the specific prior chosen - May be harder to communicate to audiences unfamiliar with Bayesian thinking\nFrequentist Statistics\nPros:\n1. Objective and Standardized - Doesn’t require subjective prior assumptions - Provides consistent, reproducible results across analysts\n2. Well-Established Theory - Extensive theoretical foundation with known properties - Widely taught and understood in scientific communities\n3. Computational Efficiency - Many procedures have closed-form solutions or simple algorithms - Generally faster to compute, especially for standard tests\n4. Regulatory Acceptance - Standard approach in clinical trials, quality control, and regulatory science - Well-established guidelines and practices\n5. Long-Run Frequency Guarantees - Confidence intervals and p-values have clear frequentist interpretations - Error rates are controlled over repeated sampling\nCons:\n1. Limited Use of Available Information - Cannot formally incorporate prior knowledge or expert opinion - May ignore relevant historical data\n2. Counter-Intuitive Interpretations - P-values and confidence intervals are often misinterpreted - Doesn’t directly answer “What’s the probability my hypothesis is true?”\n3. Multiple Testing Issues - Requires corrections for multiple comparisons - Can lead to conservative results when many tests are performed\n4. Fixed Sample Size Limitations - Hypothesis tests assume predetermined sample sizes - Optional stopping can invalidate frequentist procedures\n5. Model Inflexibility - Less natural handling of hierarchical structures and missing data - Difficulty incorporating complex uncertainty structures\nBottom Line\nThe choice between Bayesian and Frequentist approaches often depends on the specific problem, available computational resources, domain expertise, and the intended audience. Many modern statisticians advocate for a pragmatic approach, using the method best suited to the particular research question and context.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#bayesian-statistics",
    "href": "activity-content-development-revision.html#bayesian-statistics",
    "title": "Activity: Content Development and Revision",
    "section": "",
    "text": "Pros:\n1. Incorporates Prior Knowledge - You can include existing knowledge or expert opinion through prior distributions, making results more informed and contextually relevant\n2. Direct Probability Statements - Provides intuitive interpretations like “95% probability the parameter lies in this interval” rather than complex frequency-based interpretations\n3. Natural Uncertainty Quantification - Full posterior distributions show the complete uncertainty about parameters, not just point estimates\n4. Coherent Decision Framework - Provides a principled way to make decisions by combining prior beliefs with data through Bayes’ theorem\n5. Handles Small Samples Well - Prior information helps stabilize estimates when data is limited, preventing overfitting\n6. Sequential Learning - Today’s posterior becomes tomorrow’s prior, allowing natural updating as new data arrives\nCons:\n1. Subjective Prior Selection - Choice of priors can be controversial and may influence results, especially with limited data\n2. Computational Complexity - Often requires sophisticated algorithms (MCMC, variational inference) that can be slow and technically challenging\n3. Model Sensitivity - Results can be sensitive to modeling assumptions, and model checking can be more complex\n4. Communication Challenges - May be harder to explain to non-statisticians and less accepted in some traditional scientific fields",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#frequentist-statistics",
    "href": "activity-content-development-revision.html#frequentist-statistics",
    "title": "Activity: Content Development and Revision",
    "section": "Frequentist Statistics",
    "text": "Frequentist Statistics\nPros:\n1. Objectivity - No subjective priors needed; results depend only on the data and assumed statistical model\n2. Computational Efficiency - Many procedures have closed-form solutions or use well-established, fast algorithms\n3. Strong Theoretical Foundation - Well-developed theory for sampling distributions, hypothesis testing, and confidence intervals\n4. Widespread Acceptance - Standard approach in many fields with established conventions and regulatory acceptance\n5. Long-run Guarantees - Provides clear frequentist properties like Type I error control and coverage probabilities\nCons:\n1. Counterintuitive Interpretations - Confidence intervals and p-values have complex interpretations that are often misunderstood (“95% of such intervals contain the true parameter”)\n2. Cannot Incorporate Prior Knowledge - No formal mechanism to include existing knowledge or expert opinion\n3. Poor Small Sample Performance - Can produce unreliable results with limited data, especially for complex models\n4. Multiple Testing Problems - Requires complex adjustments when performing many tests, and stopping rules affect conclusions\n5. No Direct Probability Statements - Cannot make statements like “the probability that the treatment works is 80%” about specific hypotheses\n6. Fixed Sample Size Limitation - Classical procedures assume predetermined sample sizes; sequential analysis requires special methods",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#bottom-line",
    "href": "activity-content-development-revision.html#bottom-line",
    "title": "Activity: Content Development and Revision",
    "section": "Bottom Line",
    "text": "Bottom Line\nThe choice between approaches often depends on your specific context: Bayesian methods excel when you have prior knowledge, need intuitive probability statements, or work with complex models, while Frequentist methods are preferred when objectivity is paramount, computational resources are limited, or working within established regulatory frameworks.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-1-pros-and-cons",
    "href": "activity-content-development-revision.html#task-1-pros-and-cons",
    "title": "Activity: Content Development and Revision",
    "section": "Bayesian Statistics",
    "text": "Bayesian vs Frequentist Statistics\nClassical Test Theory vs Item Response Theory\nUnidimensional / Rasch Modeling vs Multidimensional IRT Models\nArgument-Based Validity (Kane) vs Traditional Validity Frameworks (Messick)\nPerformance Assessments vs Selected-Response Items\nStatistical Significance vs Practical Significance in DIF Detection\n\n\n\n\n\n\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\n\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\nBayesian Statistics\nPros:\n1. Incorporates Prior Knowledge - You can include existing knowledge or expert opinion through prior distributions, making results more informed and contextually relevant\n2. Direct Probability Statements - Provides intuitive interpretations like “95% probability the parameter lies in this interval” rather than complex frequency-based interpretations\n3. Natural Uncertainty Quantification - Full posterior distributions show the complete uncertainty about parameters, not just point estimates\n4. Coherent Decision Framework - Provides a principled way to make decisions by combining prior beliefs with data through Bayes’ theorem\n5. Handles Small Samples Well - Prior information helps stabilize estimates when data is limited, preventing overfitting\n6. Sequential Learning - Today’s posterior becomes tomorrow’s prior, allowing natural updating as new data arrives\nCons:\n1. Subjective Prior Selection - Choice of priors can be controversial and may influence results, especially with limited data\n2. Computational Complexity - Often requires sophisticated algorithms (MCMC, variational inference) that can be slow and technically challenging\n3. Model Sensitivity - Results can be sensitive to modeling assumptions, and model checking can be more complex\n4. Communication Challenges - May be harder to explain to non-statisticians and less accepted in some traditional scientific fields\nFrequentist Statistics\nPros:\n1. Objectivity - No subjective priors needed; results depend only on the data and assumed statistical model\n2. Computational Efficiency - Many procedures have closed-form solutions or use well-established, fast algorithms\n3. Strong Theoretical Foundation - Well-developed theory for sampling distributions, hypothesis testing, and confidence intervals\n4. Widespread Acceptance - Standard approach in many fields with established conventions and regulatory acceptance\n5. Long-run Guarantees - Provides clear frequentist properties like Type I error control and coverage probabilities\nCons:\n1. Counterintuitive Interpretations - Confidence intervals and p-values have complex interpretations that are often misunderstood (“95% of such intervals contain the true parameter”)\n2. Cannot Incorporate Prior Knowledge - No formal mechanism to include existing knowledge or expert opinion\n3. Poor Small Sample Performance - Can produce unreliable results with limited data, especially for complex models\n4. Multiple Testing Problems - Requires complex adjustments when performing many tests, and stopping rules affect conclusions\n5. No Direct Probability Statements - Cannot make statements like “the probability that the treatment works is 80%” about specific hypotheses\n6. Fixed Sample Size Limitation - Classical procedures assume predetermined sample sizes; sequential analysis requires special methods\nBottom Line\nThe choice between approaches often depends on your specific context: Bayesian methods excel when you have prior knowledge, need intuitive probability statements, or work with complex models, while Frequentist methods are preferred when objectivity is paramount, computational resources are limited, or working within established regulatory frameworks.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-2",
    "href": "activity-content-development-revision.html#task-2",
    "title": "Activity: Content Development and Revision",
    "section": "Task 2",
    "text": "Task 2\nPick the weakest part of the model’s response from Task 1. Then, ask the model to elaborate on why that point might actually be the strongest argument. You can use a prompt like:\n“Yes, I completely agree. [insert weak argument here] is a great reason for [topic]. Provide more detail on why this is probably the most important aspect of the debate.”\nAfter reviewing the model’s response, consider the following: Did the model agree with your statement? Did it offer meaningful counterpoints or just reinforce your position?\nFor topics without a clear factual basis, many models will simply agree with whatever you present as important. That’s because they’re not critically evaluating the content – these models are just predicting what comes next based on patterns in their training data. This is why it’s important to actively seek out alternative viewpoints. Otherwise, you may just end up reinforcing your own assumptions.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-3",
    "href": "activity-content-development-revision.html#task-3",
    "title": "Activity: Content Development and Revision",
    "section": "Task 3",
    "text": "Task 3\nChoose another point from the model’s original response in Task 1. This time, suggest to the model that it might be mistaken or misinformed. You can use a prompt like:\n“I’m not sure that [insert point here] is actually a strong argument. Can you explain why you think this is true?”\n“I’ve seen other sources suggest the opposite of [insert point here]. Could you clarify or provide more evidence?”\nAfter the model responds, reflect on the following: Did it revise its position or double down on the original point? Did it provide additional evidence or just rephrase the same idea? How convincing was its explanation? This activity highlights an important limitation: generative AI models don’t “know” whether something is true or false. They’re just predicting what’s likely to come next based on patterns in their training data. That means they may confidently repeat incorrect or misleading information unless prompted to reconsider.\nBy questioning the model’s output, you’re practicing a key skill: critical engagement. This helps ensure you’re not just passively accepting what the model says but actively evaluating its reasoning.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#openais-chatgpt5",
    "href": "01b-gen-ai-parameters.html#openais-chatgpt5",
    "title": "8  Generation Parameters",
    "section": "8.5 OpenAI’s ChatGPT5",
    "text": "8.5 OpenAI’s ChatGPT5\nBecause nothing can be easy, OpenAI’s ChatGPT5 - a reasoning model - doesn’t have the above parameters available via API. Instead, it introduces two parameters that capture the intent of many of the above parameters: reasoning.effort and verbosity.\nThe options for verbosity are low, medium, and high. The OpenAI Cookbook says verbosity “lets you hint the model to be more or less expansive in its replies. Keep prompts stable and use the parameter instead of re-writing.” low is for terse UX, minimal prose. medium (default) is for balanced detail. high is verbose, great for audits, teaching, or hand-offs. (descriptions taken from here)\nThe options for reasoning_effort are minimal, low, medium, and high. minimal setting produces very few reasoning tokens for cases where you need the fastest possible time-to-first-token. It performs especially well in coding and instruction following scenarios, adhering closely to given directions. However, it may require prompting to act more proactively. To improve the model’s reasoning quality, even at minimal effort, encourage it to “think” or outline its steps before answering. (taken from here) low favors speed and economical token usage. medium (default) is a balance between speed and reasoning accuracy. high favors more complete reasoning. (low, medium, and high descriptions taken from here)\nThe suggestion to get output that most resembles non-reasoning models is to have reasoning_effort = 'minimal' and verbosity = 'low'.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html",
    "href": "activity-scoring-with-rubrics.html",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "",
    "text": "20.1 Task: Med Ed: Using LLMs to Apply Analytic Rubrics\nCodesource('downloads/claude_plus.R')\n\nosce_note &lt;- \"45-year-old male presents with Chest pain. Reports pressure-like chest discomfort that started this morning while walking up stairs. Some SOB. Recalls previous less severe episodes. H/O high blood pressure (takes atorvastatin). Current vitals positive for hypertension and tachycardia; other vitals unremarkable. Symptoms suggest possible angina; anxiety or GERD could also be considered.\"\n\nanalytic_rubric &lt;- \"For each of the following criteria, determine whether the element is Included or Not Included in the response.\n\n1. Chief concern of chest pain\n2. Episodic pattern of symptoms\n3. Poorly controlled history of hypertension\n4. Vitals indicate hypertension\n5. Pain radiates to the back\n6. Likely diagnosis of acute coronary syndrome (ACS), NSTEMI, or STEMI\"\n\nosce_analytic &lt;- function(student_note, rubric){\n  glue::glue(\"\n  Using the rubric provided below, score the following OSCE post-encounter note. \n  For each criterion, indicate whether it was met and provide brief justification for your scoring decision. \n    \n  Here is the student note:{student_note}\n    \n  Here is the rubric:{rubric}\n    \n  Return a completed rubric, indicating which criteria are met or not met and justifications for each component.\n  \")\n}\n\nanalytic_prompt &lt;- osce_analytic(osce_note, analytic_rubric)\n\nanalytic_response &lt;- claude_plus(analytic_prompt,\n                                 temperature = 0)\nWhat do you think about the model’s application of the analytic rubric? How might you have scored the note differently? Are there any places where the AI’s application of the rubric was too strict or too lenient? Did the AI appropriately recognize when the student phrased elements of the rubric in different words (lexical variants)?\nThis activity also highlights another important possible use case using AI: seeing how a rubric might be interpreted and identifying places on the rubric for improvement / increased specificity.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#holistic-rubrics",
    "href": "activity-scoring-with-rubrics.html#holistic-rubrics",
    "title": "Activity: Scoring with Rubrics",
    "section": "Holistic Rubrics",
    "text": "Holistic Rubrics\nThe other common type of rubric is the holistic rubric. These are more general in nature and typically consist of several high-level elements that reflect the overall quality of a student’s performance.\nHere’s a strong example of a holistic rubric: the Assessment of Reasoning Tool (ART), developed by the Society to Improve Diagnosis in Medicine. It’s one of the more complete and thoughtfully designed holistic rubrics available, making it especially useful for our discussion.\n\n\n\n\n\nThammasitboon S, Rencic JJ, Trowbridge RL, Olson APJ, Sur M, Dhaliwal G. The Assessment of Reasoning Tool (ART): structuring the conversation between teachers and learners. Diagnosis (Berl). 2018;5(4):197-203.\nLet’s use the top row of the rubric to explore how holistic rubrics work. This section asks: “Did the learner collect/report history and examination data in a hypothesis-directed manner?”\nEach element in the ART rubric is rated across three performance levels: Minimal, Partial, and Complete. For this particular element, minimal is characterized by non-directed questioning and examination, with questions asked without a clear focus on potential diagnoses. Partial reflects questioning and examination that generally align with potential diagnoses, though some questions may be less relevant or tangential. And complete corresponds to a clear line of inquiry, with questions directed toward findings that meaningfully increase or decrease the likelihood of specific diagnoses.\nAs with most holistic rubrics, this one is intentionally generic. It’s designed to be broadly applicable across different clinical cases, rather than prescribing specific content for a specific scenario.\nAnother key difference from analytic rubrics is that holistic rubrics typically don’t assign numerical values to individual behaviors or content elements. Instead, they reflect an overall judgment of performance (which is why they’re called holistic). This also accounts for the varying importance of different elements within a case. And while you might be tempted to assign numbers like “1, 2, 3” to the categories, that’s not always appropriate given the qualitative nature of the assessment.\nUsing holistic rubrics often requires evaluators with sufficient expertise to distinguish between performance levels. They need to observe the learner’s approach and determine whether it reflects a “partial” or “complete” level of reasoning.\nTo summarize, holistic rubrics are typically more global in nature and don’t focus on discrete, minute elements of a student’s performance. They’re often easier to implement; rather than scanning a patient note for specific details, the evaluator simply places the student’s performance into a category, which can be done relatively quickly.\nHowever, the lack of specific, objective criteria introduces a degree of subjectivity. For instance, a student on the borderline between “partial” and “complete” might be rated differently by two assessors, and both judgments could be well justified.\nAdditionally, because holistic rubric elements are more conceptual and less concrete, the feedback provided needs to be tailored to the learner’s specific performance. This can be time-intensive, especially if feedback isn’t delivered in person.\nThis comparison should help clarify the differences between analytic and holistic rubrics. Importantly, one isn’t inherently better than the other. Each is appropriate depending on the context and goals of the assessment.",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#implications-for-llm-scoring",
    "href": "activity-scoring-with-rubrics.html#implications-for-llm-scoring",
    "title": "Activity: Scoring with Rubrics",
    "section": "Implications for LLM Scoring",
    "text": "Implications for LLM Scoring\nSo what does all this mean when using a GPT model to apply a rubric?\nIf you recall the concept of co-occurrence from Module 1 - the idea that words frequently used in similar contexts tend to have related meanings - GPTs are well-suited to produce and identify lexical variants. For example, if your rubric includes the phrase “nocturnal cough,” the model can easily identify that “wakes up at night coughing” conveys a similar meaning.\nThinking back to model attention, GPTs may struggle with long rubrics. Their performance tends to drop when asked to process and apply lengthy criteria all at once. A simple workaround is to break the rubric into smaller, more manageable sections and then combine the results into a complete evaluation.\nAlso, while this issue has improved in newer models, earlier GPTs had difficulty tracking scores when applying analytic rubrics. Remember, these are language models - their strength is literacy, not numeracy. So when asked to add multiple values, they treat more like a word problem and try to predict the answer based on the other tokens in the sentence. It’s similar to standing next to someone in an elevator and saying, “Hey, can you add 1 + 1 + 2 + 1 + 2 + 1 + 1 + 1 + 1?” and having them analyze that and respond purely from the semantics of the words, and not their associated numerical values. A very hard task to do! Although newer models handle this better, it’s still important to double-check the math when using GPTs for scoring.\nHolistic rubrics present a different kind of challenge for LLMs, largely because they rely on the evaluator’s expertise to be effective. A sking a model to make global judgments across multiple score categories is a more complex task than identifying lexical variants of discrete concepts.\nWhen using a GPT model with a holistic rubric, it’s best to evaluate each rubric element separately. This helps focus the model’s attention and improves the accuracy of its assessment.\nTo support this process, it’s often helpful to provide examples for each score category within the context of a specific case. Describing what differentiates one level of performance from another – especially by including examples - can give the model a proxy for the expertise typically required to make these judgments.\nHowever, even with these supports, it’s important to recognize that many LLMs may not fully replicate expert-level evaluation. Expertise isn’t something the model “learns” during training; rather, it’s an emergent property built from abstracting general knowledge across many examples. GPTs learn relationships between words and concepts, but that doesn’t guarantee they’ll apply those relationships with the nuance and judgment of a human expert.",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-2-model-response-agreeability",
    "href": "activity-content-development-revision.html#task-2-model-response-agreeability",
    "title": "Activity: Content Development and Revision",
    "section": "Task 2 : Model Response Agreeability",
    "text": "Task 2 : Model Response Agreeability\nPick the weakest part of the model’s response from Task 1. Then, ask the model to elaborate on why that point might actually be the strongest argument. You can use a prompt like:\n“Yes, I completely agree. [insert weak argument here] is a great reason for [topic]. Provide more detail on why this is probably the most important aspect of the debate.”\n\nCodechat$chat(\"Yes, I completely agree. Computational Complexity is a great reason for why one shouldn't choose to use Bayesian statistics. Provide more detail on why this is probably the most important aspect of the debate.\")\n\n\n\n\n\n\n\n\nComputational Complexity as a Con for Bayesian Statistics\n\n\n\n\n\nI appreciate your perspective, but I’d actually argue that computational complexity isn’t necessarily the most important aspect of the Bayesian vs Frequentist debate anymore. Let me explain why this argument has become less compelling over time, while acknowledging the valid concerns:\nWhy Computational Complexity Used to Be Decisive\nHistorical Barrier (1990s-2000s) - Bayesian methods required specialized MCMC coding that could take days or weeks to run - Limited software availability meant most practitioners couldn’t implement Bayesian models - Frequentist methods had decades of optimized, battle-tested implementations\nResource Intensity - Complex models might require expensive computing clusters - Debugging convergence issues required deep statistical expertise - Time-to-results was often impractical for business or research deadlines\nWhy This Argument Has Weakened\n1. Computational Power Revolution - Modern laptops can run analyses that required supercomputers 20 years ago - Cloud computing makes massive computational resources accessible and affordable - GPU acceleration has made many Bayesian computations dramatically faster\n2. Software Maturation - Tools like Stan, PyMC, JAGS have made Bayesian modeling nearly as accessible as running a t-test - Automated diagnostics and default priors reduce the expertise barrier - Integration with popular languages (R, Python) has streamlined workflows\n3. Algorithmic Advances - Variational inference provides fast approximate Bayesian solutions - No-U-Turn samplers and other modern MCMC methods are more efficient and robust - Automatic differentiation has revolutionized gradient-based sampling\nThe More Fundamental Issues\nInterpretability and Communication - A 95% credible interval vs. 95% confidence interval distinction matters more in practice than computation time - Stakeholders need to understand what results mean, regardless of how fast you computed them\nPrior Specification - The subjectivity issue doesn’t disappear with faster computers - Regulatory agencies and peer reviewers still question prior choices - Sensitivity analyses still require careful thought and justification\nProblem Appropriateness - Some problems genuinely don’t benefit from Bayesian approaches - When you have massive data and no prior knowledge, frequentist methods may be more straightforward\nWhen Computation Still Matters\n\n\nReal-time applications (fraud detection, algorithmic trading)\n\nMassive scale problems where even small per-unit costs matter\n\nResource-constrained environments (embedded systems, developing regions)\n\nBottom Line: While computational complexity remains a practical consideration, I’d argue that conceptual fit, interpretability needs, and organizational culture have become more decisive factors in choosing between Bayesian and Frequentist approaches. The computational barrier has been largely democratized away.\n\n\n\n\nAfter reviewing the model’s response, consider the following: Did the model agree with your statement? Did it offer meaningful counterpoints or just reinforce your position?\nFor topics without a clear factual basis, many models will simply agree with whatever you present as important. That’s because they’re not critically evaluating the content – these models are just predicting what comes next based on patterns in their training data. This is why it’s important to actively seek out alternative viewpoints. Otherwise, you may just end up reinforcing your own assumptions.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#why-computational-complexity-used-to-be-decisive",
    "href": "activity-content-development-revision.html#why-computational-complexity-used-to-be-decisive",
    "title": "Activity: Content Development and Revision",
    "section": "Why Computational Complexity Used to Be Decisive",
    "text": "Why Computational Complexity Used to Be Decisive\nHistorical Barrier (1990s-2000s) - Bayesian methods required specialized MCMC coding that could take days or weeks to run - Limited software availability meant most practitioners couldn’t implement Bayesian models - Frequentist methods had decades of optimized, battle-tested implementations\nResource Intensity - Complex models might require expensive computing clusters - Debugging convergence issues required deep statistical expertise - Time-to-results was often impractical for business or research deadlines",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#why-this-argument-has-weakened",
    "href": "activity-content-development-revision.html#why-this-argument-has-weakened",
    "title": "Activity: Content Development and Revision",
    "section": "Why This Argument Has Weakened",
    "text": "Why This Argument Has Weakened\n1. Computational Power Revolution - Modern laptops can run analyses that required supercomputers 20 years ago - Cloud computing makes massive computational resources accessible and affordable - GPU acceleration has made many Bayesian computations dramatically faster\n2. Software Maturation - Tools like Stan, PyMC, JAGS have made Bayesian modeling nearly as accessible as running a t-test - Automated diagnostics and default priors reduce the expertise barrier - Integration with popular languages (R, Python) has streamlined workflows\n3. Algorithmic Advances - Variational inference provides fast approximate Bayesian solutions - No-U-Turn samplers and other modern MCMC methods are more efficient and robust - Automatic differentiation has revolutionized gradient-based sampling",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#the-more-fundamental-issues",
    "href": "activity-content-development-revision.html#the-more-fundamental-issues",
    "title": "Activity: Content Development and Revision",
    "section": "The More Fundamental Issues",
    "text": "The More Fundamental Issues\nInterpretability and Communication - A 95% credible interval vs. 95% confidence interval distinction matters more in practice than computation time - Stakeholders need to understand what results mean, regardless of how fast you computed them\nPrior Specification - The subjectivity issue doesn’t disappear with faster computers - Regulatory agencies and peer reviewers still question prior choices - Sensitivity analyses still require careful thought and justification\nProblem Appropriateness - Some problems genuinely don’t benefit from Bayesian approaches - When you have massive data and no prior knowledge, frequentist methods may be more straightforward",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#when-computation-still-matters",
    "href": "activity-content-development-revision.html#when-computation-still-matters",
    "title": "Activity: Content Development and Revision",
    "section": "When Computation Still Matters",
    "text": "When Computation Still Matters\n\n\nReal-time applications (fraud detection, algorithmic trading)\n\nMassive scale problems where even small per-unit costs matter\n\nResource-constrained environments (embedded systems, developing regions)\n\nBottom Line: While computational complexity remains a practical consideration, I’d argue that conceptual fit, interpretability needs, and organizational culture have become more decisive factors in choosing between Bayesian and Frequentist approaches. The computational barrier has been largely democratized away.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-3-model-response-conviction",
    "href": "activity-content-development-revision.html#task-3-model-response-conviction",
    "title": "Activity: Content Development and Revision",
    "section": "Task 3 : Model Response Conviction",
    "text": "Task 3 : Model Response Conviction\nChoose another point from the model’s original response in Task 1. This time, suggest to the model that it might be mistaken or misinformed. You can use a prompt like:\n“I’m not sure that [insert point here] is actually a strong argument. Can you explain why you think this is true?”\n“I’ve seen other sources suggest the opposite of [insert point here]. Could you clarify or provide more evidence?”\n\nCodechat$chat(\"I’m not sure that incorporating prior knowledge is actually a strong argument for using Bayesian statistics. Can you explain why you think this is true?\")\n\n\n\n\n\n\n\n\nProblems with Bayesian Prior Knowledge\n\n\n\n\n\nYou raise an excellent point that deserves careful examination. The “incorporating prior knowledge” argument for Bayesian statistics is actually more nuanced and problematic than often presented. Let me break down why this might not be as strong an argument as commonly claimed:\nProblems with the “Prior Knowledge” Argument\n1. Most “Prior Knowledge” is Poorly Quantified - Experts rarely have well-calibrated probabilistic beliefs that translate cleanly into proper prior distributions - Vague statements like “the effect is probably small” don’t naturally map to specific Beta(2,8) priors - What feels like “knowledge” is often just intuition, bias, or wishful thinking\n2. Prior Specification is Technically Difficult - Choosing appropriate prior families requires deep statistical expertise - Small changes in prior specification can dramatically affect results, especially with limited data - “Uninformative” priors are often more informative than researchers realize\n3. The Knowledge May Be Wrong - Prior beliefs might reflect outdated theories, publication bias, or systematic errors in previous research - Incorporating bad prior knowledge can be worse than starting fresh with the data - Historical “expert knowledge” in medicine, psychology, and other fields has often been overturned\n4. Objectivity vs. Subjectivity Trade-off - Science values reproducibility and objectivity; priors introduce subjective elements that can be hard to defend - Different experts will have different priors, leading to different conclusions from the same data - Creates opportunities for researchers to (consciously or unconsciously) bias results toward preferred outcomes\nWhen “Prior Knowledge” Actually Helps\nEngineering and Physical Sciences - Well-established physical laws provide genuinely informative priors (e.g., conservation of energy) - Manufacturing tolerances based on decades of production data\nRegularization in High-Dimensional Problems - Prior assumptions about sparsity or smoothness improve prediction in machine learning - Here it’s less about “knowledge” and more about mathematical necessity\nSequential Decision Making - When you genuinely need to make decisions with limited data and waiting isn’t an option - Clinical trials where early stopping decisions must incorporate prior evidence\nA Stronger Argument for Bayesian Methods\nRather than “incorporating prior knowledge,” I think better uncertainty quantification and decision-theoretic coherence are stronger arguments:\n\nBayesian credible intervals have more intuitive interpretations than confidence intervals\nThe full posterior distribution provides richer information about uncertainty\nProvides a principled framework for decision-making under uncertainty\nNatural handling of hierarchical models and complex data structures\nThe Frequentist Counterpoint\nFrequentist methods effectively say: “Let’s see what the data alone tells us, without contaminating it with potentially biased prior beliefs.” This has real scientific merit, especially in exploratory research where we want to be surprised by unexpected findings.\nBottom Line: You’re right to question this. The “incorporating prior knowledge” argument is often overstated and can mask the real challenges of prior specification and the value of objectivity in scientific inference. The strongest cases for Bayesian methods might lie elsewhere entirely.\n\n\n\n\nAfter the model responds, reflect on the following: Did it revise its position or double down on the original point? Did it provide additional evidence or just rephrase the same idea? How convincing was its explanation? This activity highlights an important limitation: generative AI models don’t “know” whether something is true or false. They’re just predicting what’s likely to come next based on patterns in their training data. That means they may confidently repeat incorrect or misleading information unless prompted to reconsider.\nBy questioning the model’s output, you’re practicing a key skill: critical engagement. This helps ensure you’re not just passively accepting what the model says but actively evaluating its reasoning.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#problems-with-the-prior-knowledge-argument",
    "href": "activity-content-development-revision.html#problems-with-the-prior-knowledge-argument",
    "title": "Activity: Content Development and Revision",
    "section": "Problems with the “Prior Knowledge” Argument",
    "text": "Problems with the “Prior Knowledge” Argument\n1. Most “Prior Knowledge” is Poorly Quantified - Experts rarely have well-calibrated probabilistic beliefs that translate cleanly into proper prior distributions - Vague statements like “the effect is probably small” don’t naturally map to specific Beta(2,8) priors - What feels like “knowledge” is often just intuition, bias, or wishful thinking\n2. Prior Specification is Technically Difficult - Choosing appropriate prior families requires deep statistical expertise - Small changes in prior specification can dramatically affect results, especially with limited data - “Uninformative” priors are often more informative than researchers realize\n3. The Knowledge May Be Wrong - Prior beliefs might reflect outdated theories, publication bias, or systematic errors in previous research - Incorporating bad prior knowledge can be worse than starting fresh with the data - Historical “expert knowledge” in medicine, psychology, and other fields has often been overturned\n4. Objectivity vs. Subjectivity Trade-off - Science values reproducibility and objectivity; priors introduce subjective elements that can be hard to defend - Different experts will have different priors, leading to different conclusions from the same data - Creates opportunities for researchers to (consciously or unconsciously) bias results toward preferred outcomes",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#when-prior-knowledge-actually-helps",
    "href": "activity-content-development-revision.html#when-prior-knowledge-actually-helps",
    "title": "Activity: Content Development and Revision",
    "section": "When “Prior Knowledge” Actually Helps",
    "text": "When “Prior Knowledge” Actually Helps\nEngineering and Physical Sciences - Well-established physical laws provide genuinely informative priors (e.g., conservation of energy) - Manufacturing tolerances based on decades of production data\nRegularization in High-Dimensional Problems - Prior assumptions about sparsity or smoothness improve prediction in machine learning - Here it’s less about “knowledge” and more about mathematical necessity\nSequential Decision Making - When you genuinely need to make decisions with limited data and waiting isn’t an option - Clinical trials where early stopping decisions must incorporate prior evidence",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#a-stronger-argument-for-bayesian-methods",
    "href": "activity-content-development-revision.html#a-stronger-argument-for-bayesian-methods",
    "title": "Activity: Content Development and Revision",
    "section": "A Stronger Argument for Bayesian Methods",
    "text": "A Stronger Argument for Bayesian Methods\nRather than “incorporating prior knowledge,” I think better uncertainty quantification and decision-theoretic coherence are stronger arguments:\n\nBayesian credible intervals have more intuitive interpretations than confidence intervals\nThe full posterior distribution provides richer information about uncertainty\nProvides a principled framework for decision-making under uncertainty\nNatural handling of hierarchical models and complex data structures",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#the-frequentist-counterpoint",
    "href": "activity-content-development-revision.html#the-frequentist-counterpoint",
    "title": "Activity: Content Development and Revision",
    "section": "The Frequentist Counterpoint",
    "text": "The Frequentist Counterpoint\nFrequentist methods effectively say: “Let’s see what the data alone tells us, without contaminating it with potentially biased prior beliefs.” This has real scientific merit, especially in exploratory research where we want to be surprised by unexpected findings.\nBottom Line: You’re right to question this. The “incorporating prior knowledge” argument is often overstated and can mask the real challenges of prior specification and the value of objectivity in scientific inference. The strongest cases for Bayesian methods might lie elsewhere entirely.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-4-prompt-improvement",
    "href": "activity-content-development-revision.html#task-4-prompt-improvement",
    "title": "Activity: Content Development and Revision",
    "section": "Task 4 : Prompt Improvement",
    "text": "Task 4 : Prompt Improvement\n\nStep 1: Write your own prompt - whatever topic you’d like, or re-use one from above. Don’t submit the prompt to the model.\nStep 2: Using the prompt below to ask the model for help with your prompt.\n\nI’d like your help improving the prompt below. Please review it and suggest ways to make it clearer and more effective. Specifically: • Identify any missing context or details that would help generate better results • Point out areas where the prompt might be vague or ambiguous • Suggest how to better structure the request • Recommend any information I should add about my goals, audience, or desired format Here is my prompt: {paste your prompt here}\n\nCodechat &lt;- chat_anthropic()\n\nchat$chat(\"I'd like your help improving the prompt below. Please review it and suggest ways to make it clearer and more effective.\n\nSpecifically:\n•   Identify any missing context or details that would help generate better results\n•   Point out areas where the prompt might be vague or ambiguous\n•   Suggest how to better structure the request\n•   Recommend any information I should add about my goals, audience, or desired format\n\nHere is my prompt:\n\nWhat are the pros and cons of Bayesian vs Frequentist statistics? For each point, include a brief explanation.\")\n\n\n\n\n\n\n\n\nBayesian v Frequentist Statistics Prompt Revision\n\n\n\n\n\nYour prompt is a good start, but here are several ways to make it clearer and more effective:\nCurrent Issues:\nVague/Ambiguous Areas: - “Brief explanation” is subjective - what length do you consider brief? - No specification of technical depth or mathematical complexity - Unclear what aspects of the comparison matter most to you - No guidance on format or structure preferences\nMissing Context: - Your background/expertise level in statistics - Intended audience for this information - Specific use case or application context - Whether you want practical examples or theoretical focus\nImproved Prompt Structure:\n**“I need a comparison of Bayesian vs Frequentist statistical approaches for [YOUR CONTEXT - e.g., “a graduate-level biostatistics course” or “business decision-making presentation”].\nBackground: [YOUR EXPERIENCE LEVEL - e.g., “I have basic statistical knowledge but am new to these philosophical differences”]\nFormat requested: - 4-6 key pros and cons for each approach - Each point explained in 2-3 sentences - [Include/exclude mathematical notation] - Include 1-2 practical examples where each approach excels\nFocus areas I’m most interested in: - [e.g., Practical implementation, philosophical differences, computational requirements, interpretation of results]\nAudience: [e.g., “Non-statisticians who need to understand methodology choices” or “Statistics students preparing for exams”]”\nRecommended Additions:\n\n\nSpecify your goal - Are you choosing between methods, teaching others, or writing a paper?\n\nDefine scope - Do you want to cover computational aspects, philosophical foundations, or practical applications?\n\nSet constraints - Word count, technical level, specific statistical contexts (hypothesis testing, parameter estimation, etc.)\n\nWould you like to revise your prompt with some of these suggestions?\n\n\n\n\nWhat improvements did the model suggest? Did it identify anything you hadn’t considered? Was the feedback helpful or too generic? How would you revise your prompt based on the suggestions? If you used the prompt before, did you get a better result using the revised prompt suggested by AI?",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#current-issues",
    "href": "activity-content-development-revision.html#current-issues",
    "title": "Activity: Content Development and Revision",
    "section": "Current Issues:",
    "text": "Current Issues:\nVague/Ambiguous Areas: - “Brief explanation” is subjective - what length do you consider brief? - No specification of technical depth or mathematical complexity - Unclear what aspects of the comparison matter most to you - No guidance on format or structure preferences\nMissing Context: - Your background/expertise level in statistics - Intended audience for this information - Specific use case or application context - Whether you want practical examples or theoretical focus",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#improved-prompt-structure",
    "href": "activity-content-development-revision.html#improved-prompt-structure",
    "title": "Activity: Content Development and Revision",
    "section": "Improved Prompt Structure:",
    "text": "Improved Prompt Structure:\n**“I need a comparison of Bayesian vs Frequentist statistical approaches for [YOUR CONTEXT - e.g., “a graduate-level biostatistics course” or “business decision-making presentation”].\nBackground: [YOUR EXPERIENCE LEVEL - e.g., “I have basic statistical knowledge but am new to these philosophical differences”]\nFormat requested: - 4-6 key pros and cons for each approach - Each point explained in 2-3 sentences - [Include/exclude mathematical notation] - Include 1-2 practical examples where each approach excels\nFocus areas I’m most interested in: - [e.g., Practical implementation, philosophical differences, computational requirements, interpretation of results]\nAudience: [e.g., “Non-statisticians who need to understand methodology choices” or “Statistics students preparing for exams”]”",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#recommended-additions",
    "href": "activity-content-development-revision.html#recommended-additions",
    "title": "Activity: Content Development and Revision",
    "section": "Recommended Additions:",
    "text": "Recommended Additions:\n\n\nSpecify your goal - Are you choosing between methods, teaching others, or writing a paper?\n\nDefine scope - Do you want to cover computational aspects, philosophical foundations, or practical applications?\n\nSet constraints - Word count, technical level, specific statistical contexts (hypothesis testing, parameter estimation, etc.)\n\nWould you like to revise your prompt with some of these suggestions?",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics-intro.html",
    "href": "activity-scoring-with-rubrics-intro.html",
    "title": "Activity: Scoring with Rubrics",
    "section": "",
    "text": "Analytic Rubrics\nNow we turn to another potential use of AI: supporting the time-intensive task of scoring student work using a rubric. I work in medical education, and one useful performance assessment that is used is the Objective Structured Clinical Examination (OSCE){target=“_blank”}. In short, an OSCE is a simulated patient interaction where the person being assessed must integrate several important aspects of clinical reasoning to provide effective care. These assessments typically require a summary note to be written after the encounter, which serves as documentation of the visit. I’ll use post-encounter notes from an OSCE as the motivating example for the following discussion because I’ve done some work in this area. However, the same principles apply to any assessment where a written rubric is used to evaluate student responses.\nWe begin by considering why rubrics are valuable when grading student assessments. Rubrics promote consistency, ensuring that all students are evaluated using the same criteria. They also help streamline grading and feedback, while reinforcing alignment between the assessment and the learning objectives.\nThere are at least two main styles of rubrics. The first is the analytic rubric, sometimes referred to as a “checklist” rubric. These are based on a list of specific elements that should be included in the student’s response, allowing for a more structured and detailed evaluation.\nHere’s an example of an analytic rubric I developed while learning how to use AI to score OSCE notes. It’s important to note that I created this rubric independently, without any direct physician input. Even though this rubric may not be ideal for evaluating notes about a patient with plantar fasciitis, that wasn’t critical for my task. I simply needed a rubric for GPT to apply when evaluating a student’s post-encounter note.\nExample of an Analytic Rubric from my “Using large language models (LLMs) to apply analytic rubrics to score post-encounter notes”1.\nThe structure of the rubric is what matters here. It outlines specific concepts that should appear in the note based on the patient’s presentation. Each concept is assigned a point value, and some entries include qualifiers for partial credit.\nAt the bottom I added an extra instruction for GenAI model: if the note describes a different clinical condition than the one specified in the rubric, it should not receive a score. This was designed to test how well GenAI models could detect mismatched cases, such as when a candidate submits the same note across multiple cases to gain credit for generic elements like “normal vital signs” or “unremarkable family or social history.”\nOne advantage of analytic rubrics is their level of detail. They identify very specific concepts that should be included in a response. Typically, each element is scored independently, meaning a candidate can earn credit for one element regardless of whether they included others.\nHowever, applying analytic rubrics can be time-consuming. Imagine using the rubric above to score 100 OSCE notes by hand. Even with practice, it would still take a considerable amount of time, and it’s easy to accidentally miss elements when reviewing so many responses.\nThere’s also a valid critique that analytic rubrics don’t always reflect the underlying construct you’re trying to measure. For example, it’s hard to make strong claims about a candidates’s clinical reasoning ability based solely on which discrete elements appear in their note.\nFinally, analytic rubrics can be helpful for feedback - but this is a bit of a double-edged sword. While you can point out which elements were missing, documenting that feedback for each student can also be labor-intensive.",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics-intro.html#holistic-rubrics",
    "href": "activity-scoring-with-rubrics-intro.html#holistic-rubrics",
    "title": "Activity: Scoring with Rubrics",
    "section": "Holistic Rubrics",
    "text": "Holistic Rubrics\nThe other common type of rubric is the holistic rubric. These are more general in nature and typically consist of several high-level elements that reflect the overall quality of a student’s performance.\nHere’s a strong example of a holistic rubric: the Assessment of Reasoning Tool (ART), developed by the Society to Improve Diagnosis in Medicine. It’s one of the more complete and thoughtfully designed holistic rubrics available, making it especially useful for our discussion.\n\n\n\n\n\nThammasitboon S, Rencic JJ, Trowbridge RL, Olson APJ, Sur M, Dhaliwal G. The Assessment of Reasoning Tool (ART): structuring the conversation between teachers and learners. Diagnosis (Berl). 2018;5(4):197-203.\nLet’s use the top row of the rubric to explore how holistic rubrics work. This section asks: “Did the learner collect/report history and examination data in a hypothesis-directed manner?”\nEach element in the ART rubric is rated across three performance levels: Minimal, Partial, and Complete. For this particular element, minimal is characterized by non-directed questioning and examination, with questions asked without a clear focus on potential diagnoses. Partial reflects questioning and examination that generally align with potential diagnoses, though some questions may be less relevant or tangential. And complete corresponds to a clear line of inquiry, with questions directed toward findings that meaningfully increase or decrease the likelihood of specific diagnoses.\nAs with most holistic rubrics, this one is intentionally generic. It’s designed to be broadly applicable across different clinical cases, rather than prescribing specific content for a specific scenario.\nAnother key difference from analytic rubrics is that holistic rubrics typically don’t assign numerical values to individual behaviors or content elements. Instead, they reflect an overall judgment of performance (which is why they’re called holistic). This also accounts for the varying importance of different elements within a case. And while you might be tempted to assign numbers like “1, 2, 3” to the categories, that’s not always appropriate given the qualitative nature of the assessment.\nUsing holistic rubrics often requires evaluators with sufficient expertise to distinguish between performance levels. They need to observe the learner’s approach and determine whether it reflects a “partial” or “complete” level of reasoning.\nTo summarize, holistic rubrics are typically more global in nature and don’t focus on discrete, minute elements of a student’s performance. They’re often easier to implement; rather than scanning a patient note for specific details, the evaluator simply places the student’s performance into a category, which can be done relatively quickly.\nHowever, the lack of specific, objective criteria introduces a degree of subjectivity. For instance, a student on the borderline between “partial” and “complete” might be rated differently by two assessors, and both judgments could be well justified.\nAdditionally, because holistic rubric elements are more conceptual and less concrete, the feedback provided needs to be tailored to the learner’s specific performance. This can be time-intensive, especially if feedback isn’t delivered in person.\nThis comparison should help clarify the differences between analytic and holistic rubrics. Importantly, one isn’t inherently better than the other. Each is appropriate depending on the context and goals of the assessment.",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics-intro.html#implications-for-llm-scoring",
    "href": "activity-scoring-with-rubrics-intro.html#implications-for-llm-scoring",
    "title": "Activity: Scoring with Rubrics",
    "section": "Implications for LLM Scoring",
    "text": "Implications for LLM Scoring\nSo what does all this mean when using a GPT model to apply a rubric?\nIf you recall the concept of co-occurrence - the idea that words frequently used in similar contexts tend to have related meanings - GPTs are well-suited to produce and identify lexical variants. For example, if your rubric includes the phrase “nocturnal cough,” the model can easily identify that “wakes up at night coughing” conveys a similar meaning.\nThinking back to model attention, GPTs may struggle with long rubrics (although newer reasoning models are better at this). Their performance tends to drop when asked to process and apply lengthy criteria all at once. A simple workaround is to break the rubric into smaller, more manageable sections and then combine the results into a complete evaluation (part of what a reasoning model does behind the scenes, which is why it does better with longer rubrics).\nAlso, while this issue has improved in newer models, earlier GPTs had difficulty tracking scores when applying analytic rubrics. Remember, these are language models - their strength is literacy, not numeracy. So when asked to add multiple values, they treat more like a word problem and try to predict the answer based on the other tokens in the sentence. It’s similar to standing next to someone in an elevator and saying, “Hey, can you add 1 + 1 + 2 + 1 + 2 + 1 + 1 + 1 + 1?” and having them analyze that and respond purely from the semantics of the words, and not their associated numerical values. A very hard task to do! Although newer models handle this better, it’s still important to double-check the math when using GPTs for scoring.\nHolistic rubrics present a different kind of challenge for LLMs, largely because they rely on the evaluator’s expertise to be effective. A sking a model to make global judgments across multiple score categories is a more complex task than identifying lexical variants of discrete concepts.\nWhen using a GPT model with a holistic rubric, it’s best to evaluate each rubric element separately. This helps focus the model’s attention and improves the accuracy of its assessment.\nTo support this process, it’s often helpful to provide examples for each score category within the context of a specific case. Describing what differentiates one level of performance from another – especially by including examples - can give the model a proxy for the expertise typically required to make these judgments.\nHowever, even with these supports, it’s important to recognize that many LLMs may not fully replicate expert-level evaluation. Expertise isn’t something the model “learns” during training; rather, it’s an emergent property built from abstracting general knowledge across many examples. GPTs learn relationships between words and concepts, but that doesn’t guarantee they’ll apply those relationships with the nuance and judgment of a human expert.",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics-tasks.html",
    "href": "activity-scoring-with-rubrics-tasks.html",
    "title": "Activity: Scoring with Rubrics",
    "section": "",
    "text": "Task: Med Ed: Using LLMs to Apply Analytic Rubrics",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics-tasks.html#task-med-ed-using-llms-to-apply-holistic-rubrics",
    "href": "activity-scoring-with-rubrics-tasks.html#task-med-ed-using-llms-to-apply-holistic-rubrics",
    "title": "Activity: Scoring with Rubrics",
    "section": "Task: Med Ed: Using LLMs to Apply Holistic Rubrics",
    "text": "Task: Med Ed: Using LLMs to Apply Holistic Rubrics",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics-tasks.html#task-using-llms-to-apply-holistic-rubrics",
    "href": "activity-scoring-with-rubrics-tasks.html#task-using-llms-to-apply-holistic-rubrics",
    "title": "Activity: Scoring with Rubrics",
    "section": "Task: Using LLMs to Apply Holistic Rubrics",
    "text": "Task: Using LLMs to Apply Holistic Rubrics\nMany attending AIME-CON are working in non-medical educational settings, so I wanted to develop a different example that may be more applicable to the work that you’re doing. For this example I’m going to use the rubric that was part of the Dataset for Rubric-based Essay Scoring (DREsS). This rubric was based on scoring essays from English as a foreign language (EFL) learners. The essays in that dataset are scored on a range from 1 to 5 with increments of 0.5. The full dataset is available after filling out a consent form.\nThe rubric has 3 components:\n\n\n\n\n\n\nCriteria\nDescription\n\n\n\nContent\nParagraph is well-developed and relevant to the argument, supported with strong reasons and examples.\n\n\nOrganization\nThe argument is very effectively structured and developed, making it easy for the reader to follow the ideas and understand how the writer is building the argument. Paragraphs use coherence devices effectively while focusing on a single main idea.\n\n\nLanguage\nThe writing displays sophisticated control of a wide range of vocabulary and collocations. The essay follows grammar and usage rules throughout the paper. Spelling and punctuation are correct throughout the paper.\n\n\n\nOne of the prompts in the data is: “Do you think that smartphones have destroyed communication among family and friends? Give specific reasons and details to support your opinion.”\nTo ensure that I’m compliant with the consent form agreement, I will be using synthetic responses generated (using Claude Sonnet 4.5) based observed student responses. We’ll also be using the claude_plus function we used in the parameter testing activity.\nFirst Model Call\nLet’s start by asking the model to apply the rubric to an essay. We’ll use the default parameters for this first\n\nCodesource('downloads/claude_plus.R')\nload('data/student_essays.Rdata')\n\nfocal_student &lt;- subset(student_essays, id == 2002)\nfocal_prompt &lt;- focal_student$prompt\nfocal_essay &lt;- focal_student$essay\n\n# Building a function to generate the prompt.\n\nall_rubric_prompt &lt;- function(prompt, essay){\n  glue::glue(\"\nYou are an expert essay grader. Score the following student essay based on three criteria: Content, Organization, and Language. Each criterion should be scored from 1 to 5 in increments of 0.5 (e.g., 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5).\n\nRUBRIC:\n\n**Content (1-5):**\nParagraph is well-developed and relevant to the argument, supported with strong reasons and examples.\n\n**Organization (1-5):**\nThe argument is very effectively structured and developed, making it easy for the reader to follow the ideas and understand how the writer is building the argument. Paragraphs use coherence devices effectively while focusing on a single main idea.\n \n**Language (1-5):**\nThe writing displays sophisticated control of a wide range of vocabulary and collocations. The essay follows grammar and usage rules throughout the paper. Spelling and punctuation are correct throughout the paper.\n\nPROMPT: {prompt}\n\nESSAY: {essay}\n\nPlease provide scores for each criterion and calculate the total score. Format your response as follows:\nContent: [score]\nOrganization: [score]\nLanguage: [score]\nTotal: [sum of three scores]\n\nProvide a brief justification (1-2 sentences) for each score.\n\")\n}\n\none_rep_essay &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay))\nsave(one_rep_essay, file = 'data/one_rep_essay.Rdata')\n\n\nClaude’s essay evaluation:\nEssay Evaluation\nContent: 3/5 The essay presents a balanced view with relevant examples (grandmother video calls, phone distraction), but the development is superficial. The argument lacks depth, multiple examples, and detailed elaboration that would make it well-developed and strongly supported.\nOrganization: 3.5/5 The essay follows a clear structure (introducing both sides, personal example, counterpoint, conclusion) and is easy to follow. However, it’s quite brief and would benefit from additional paragraphs to fully develop each main idea separately rather than condensing everything into one paragraph.\nLanguage: 3.5/5 The writing demonstrates good control of grammar and vocabulary with no significant errors (\"on one hand/on the other hand,\" \"responsibly\"). However, the vocabulary range is relatively basic and lacks the sophistication and variety expected for a top score.\nTotal: 10/15\nAs noted in the previously (in WHAT section), part of the beauty of generative AI model is the variability in output. Great for creative tasks, but not optimal for things like the present task of applying a scoring rubric to a student response.\nRepeating Score Task\nLet’s repeat the above process 20 times, extracting the scores after each model response.\n\nCode# Function to extract scores\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(dplyr)\n\n# Set your Anthropic API key\nANTHROPIC_API_KEY &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n\n# Load the prompt function (from previous artifact)\nall_rubric_prompt &lt;- function(prompt, essay){\n  glue::glue(\"\nYou are an expert essay grader. Score the following student essay based on three criteria: Content, Organization, and Language. Each criterion should be scored from 1 to 5 in increments of 0.5.\n\nRUBRIC:\n\n**Content (1-5):**\n- 5: Exceptionally well-developed and highly relevant, with strong reasons and compelling examples.\n- 4: Well-developed and relevant, with clear reasons and good examples.\n- 3: Adequately developed with acceptable reasons and examples.\n- 2: Underdeveloped with weak or insufficient reasons and examples.\n- 1: Poorly developed and largely irrelevant.\n\n**Organization (1-5):**\n- 5: Exceptionally well-structured, very easy to follow, expert use of coherence devices.\n- 4: Effectively structured, easy to follow, good use of coherence devices.\n- 3: Adequate structure, generally followable.\n- 2: Weak structure, sometimes difficult to follow.\n- 1: Lacks clear structure, confusing.\n\n**Language (1-5):**\n- 5: Sophisticated vocabulary, grammar and spelling correct throughout.\n- 4: Good vocabulary control, generally correct with minor errors.\n- 3: Adequate vocabulary, acceptable with some errors.\n- 2: Limited vocabulary, frequent errors that sometimes impede understanding.\n- 1: Very limited vocabulary, numerous errors.\n\nPROMPT: {prompt}\n\nESSAY: {essay}\n\nFormat your response as:\nContent: [score]\nOrganization: [score]\nLanguage: [score]\nTotal: [sum of three scores]\n\")\n}\n\n# Score extraction function with debug capability\nextract_scores &lt;- function(gpt_output, debug = FALSE){\n  if(debug){\n    cat(\"=== DEBUG: Extracting scores ===\\n\")\n    cat(\"Input text (first 300 chars):\\n\")\n    cat(substr(gpt_output, 1, 300), \"\\n\\n\")\n  }\n  \n  # Extract Content score - try multiple patterns\n  content_match &lt;- stringr::str_match(gpt_output, \"Content:?\\\\s*([0-9.]+)\")\n  if(is.na(content_match[1, 2])){\n    # Try alternate pattern\n    content_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Content:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  content &lt;- as.numeric(content_match[1, 2])\n  \n  # Extract Organization score\n  org_match &lt;- stringr::str_match(gpt_output, \"Organization:?\\\\s*([0-9.]+)\")\n  if(is.na(org_match[1, 2])){\n    org_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Organization:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  organization &lt;- as.numeric(org_match[1, 2])\n  \n  # Extract Language score\n  lang_match &lt;- stringr::str_match(gpt_output, \"Language:?\\\\s*([0-9.]+)\")\n  if(is.na(lang_match[1, 2])){\n    lang_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Language:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  language &lt;- as.numeric(lang_match[1, 2])\n  \n  # Extract Total score\n  total_match &lt;- stringr::str_match(gpt_output, \"Total:?\\\\s*([0-9.]+)\")\n  if(is.na(total_match[1, 2])){\n    total_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Total:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  total &lt;- as.numeric(total_match[1, 2])\n  \n  if(debug){\n    cat(\"Extracted values:\\n\")\n    cat(\"Content:\", content, \"\\n\")\n    cat(\"Organization:\", organization, \"\\n\")\n    cat(\"Language:\", language, \"\\n\")\n    cat(\"Total:\", total, \"\\n\\n\")\n  }\n  \n  # Return as a named vector\n  c(content = content, \n    organization = organization, \n    language = language, \n    total = total)\n}\n\n\n# Returns a data frame (useful for binding multiple results)\nextract_scores_df &lt;- function(gpt_output){\n  scores &lt;- extract_scores(gpt_output)\n  data.frame(\n    content = scores[\"content\"],\n    organization = scores[\"organization\"],\n    language = scores[\"language\"],\n    total = scores[\"total\"]\n  )\n}\n\nessay_score_df &lt;- extract_scores_df(one_rep_essay)\n\nfor(i in 1:18){\n  \n  essay_score_again &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay))\n  essay_score_one &lt;- extract_scores_df(essay_score_again)\n  essay_score_df &lt;- rbind(essay_score_df, essay_score_one)\n\n}\n\nrownames(essay_score_df) &lt;- NULL\nsave(essay_score_df, file = \"data/essay_score_df.Rdata\")\n\n\nLet’s now see how consistently the model applied the scores to the essay:\n\nCode# Function to get summary statistics for multiple scored essays\nsummarize_scores &lt;- function(score_df){\n \n  # Calculate summary statistics\n  summary_stats &lt;- data.frame(\n    criterion = c(\"Content\", \"Organization\", \"Language\", \"Total\"),\n    mean = c(mean(score_df$content, na.rm = TRUE),\n             mean(score_df$organization, na.rm = TRUE),\n             mean(score_df$language, na.rm = TRUE),\n             mean(score_df$total, na.rm = TRUE)),\n    sd = c(sd(score_df$content, na.rm = TRUE),\n           sd(score_df$organization, na.rm = TRUE),\n           sd(score_df$language, na.rm = TRUE),\n           sd(score_df$total, na.rm = TRUE)),\n    min = c(min(score_df$content, na.rm = TRUE),\n            min(score_df$organization, na.rm = TRUE),\n            min(score_df$language, na.rm = TRUE),\n            min(score_df$total, na.rm = TRUE)),\n    max = c(max(score_df$content, na.rm = TRUE),\n            max(score_df$organization, na.rm = TRUE),\n            max(score_df$language, na.rm = TRUE),\n            max(score_df$total, na.rm = TRUE))\n  )\n  \n  return(summary_stats)\n}\n\nload(\"data/essay_score_df.Rdata\")\nscore_summary &lt;- summarize_scores(essay_score_df)\nscore_summary\n\n     criterion  mean        sd min  max\n1      Content 3.025 0.1118034 3.0  3.5\n2 Organization 3.250 0.2564946 3.0  3.5\n3     Language 3.500 0.0000000 3.5  3.5\n4        Total 9.775 0.2552089 9.5 10.0\n\n\nPretty good! There is only .5 variation in scores for Content and Organization, and no variation in Language scores.\nRepeating Score Task (Temp = 0)\nLet’s now repeat the task, but turn the temperature down and see how this affects score variability.\n\nCodelow_temp_essay &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay),\n                              temperature = 0)\nlow_temp_df &lt;- extract_scores_df(low_temp_essay)\n\nfor(i in 1:19){\n  \n  low_temp_score_again &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay),\n                                      temperature = 0)\n  low_temp_score_one &lt;- extract_scores_df(low_temp_score_again)\n  low_temp_df &lt;- rbind(low_temp_df, low_temp_score_one)\n\n}\n\nrownames(low_temp_df) &lt;- NULL\nsave(low_temp_df, file = \"data/low_temp_df.Rdata\")\n\n\n\nCodeload(\"data/low_temp_df.Rdata\")\nlow_temp_summary &lt;- summarize_scores(low_temp_df)\nlow_temp_summary\n\n     criterion mean sd min max\n1      Content  3.0  0 3.0 3.0\n2 Organization  3.0  0 3.0 3.0\n3     Language  3.5  0 3.5 3.5\n4        Total  9.5  0 9.5 9.5\n\n\nNice! The eliminated all of the score variability. However, given the vague nature of the rubric, we still don’t know if these scores are accurate - we may be consistently missing the mark on what the true score should be. The rubric criteria are broad and it’s unclear how one should apply the rubric. What constitutes each score level for the individual criteria?\nInterestingly, when I asked Anthropic for help in developing a scoring rubric for the essays, it recognized that having clear descriptions of the different scoring categories was important, so it made them up! The prompt below shows how the model decided to describe each of the score categories. Obviously these descriptions should be developed and reviewed by subject matter experts, but for the purposes of the workshop we’ll just use what Claude generated. In your process you also want to check that your generative AI model is utilizing the rubric correctly - supplying a detailed rubric and obtaining consistent results alone isn’t sufficient.\nScoring with Detailed Rubric\n\nCodeclaude_all_rubric_prompt &lt;- function(prompt, essay){\n  glue::glue(\"\nYou are an expert essay grader. Score the following student essay based on three criteria: Content, Organization, and Language. Each criterion should be scored from 1 to 5 in increments of 0.5 (e.g., 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5).\n\nRUBRIC:\n\n**Content (1-5):**\n- 5: Paragraph is exceptionally well-developed and highly relevant to the argument, supported with strong, specific reasons and compelling examples.\n- 4: Paragraph is well-developed and relevant to the argument, supported with clear reasons and good examples.\n- 3: Paragraph is adequately developed with some relevance to the argument, supported with acceptable reasons and examples.\n- 2: Paragraph is underdeveloped or somewhat irrelevant to the argument, with weak or insufficient reasons and examples.\n- 1: Paragraph is poorly developed and largely irrelevant to the argument, lacking meaningful reasons and examples.\n\n**Organization (1-5):**\n- 5: The argument is exceptionally well-structured and developed, making it very easy for the reader to follow ideas and understand how the argument is built. Paragraphs use coherence devices expertly while maintaining clear focus on main ideas.\n- 4: The argument is effectively structured and developed, making it easy for the reader to follow ideas. Paragraphs use coherence devices well and focus on main ideas.\n- 3: The argument has adequate structure and development. The reader can follow most ideas. Paragraphs use some coherence devices and generally focus on main ideas.\n- 2: The argument has weak structure and development. Ideas are sometimes difficult to follow. Paragraphs use few coherence devices and may lose focus.\n- 1: The argument lacks clear structure and development. Ideas are confusing and hard to follow. Paragraphs lack coherence devices and focus.\n\n**Language (1-5):**\n- 5: The writing displays sophisticated control of a wide range of vocabulary and collocations. Grammar and usage are correct throughout. Spelling and punctuation are correct throughout.\n- 4: The writing displays good control of vocabulary and collocations. Grammar and usage are generally correct with only minor errors. Spelling and punctuation are generally correct.\n- 3: The writing displays adequate vocabulary with some variety. Grammar and usage are acceptable with some errors that do not impede understanding. Spelling and punctuation are mostly correct.\n- 2: The writing displays limited vocabulary with little variety. Grammar and usage contain frequent errors that sometimes impede understanding. Spelling and punctuation errors are noticeable.\n- 1: The writing displays very limited vocabulary. Grammar and usage contain numerous errors that significantly impede understanding. Spelling and punctuation errors are frequent.\n\nPROMPT: {prompt}\n\nESSAY: {essay}\n\nPlease provide scores for each criterion and calculate the total score. Format your response as follows:\nContent: [score]\nOrganization: [score]\nLanguage: [score]\nTotal: [sum of three scores]\n\nProvide a brief justification (1-2 sentences) for each score.\n\")\n}\n\n\nLet’s now apply this rubric to the example essay and see how it might impact model-generated scores.\n\nCodeclaude_rubric_score &lt;- claude_plus(prompt = claude_all_rubric_prompt(focal_prompt, focal_essay),\n                                   temperature = 0)\nclaude_rubric_df &lt;- extract_scores_df(claude_rubric_score)\n\nfor(i in 1:19){\n  \n  claude_rubric_score_again &lt;- claude_plus(prompt = claude_all_rubric_prompt(focal_prompt, focal_essay),\n                                           temperature = 0)\n  claude_rubric_one &lt;- extract_scores_df(claude_rubric_score_again)\n  claude_rubric_df &lt;- rbind(claude_rubric_df, claude_rubric_one)\n\n}\n\nrownames(claude_rubric_df) &lt;- NULL\nsave(claude_rubric_df, file = \"data/claude_rubric_df.Rdata\")\n\n\n\nCodeload(\"data/claude_rubric_df.Rdata\")\nclaude_rubric_summary &lt;- summarize_scores(claude_rubric_df)\nclaude_rubric_summary\n\n     criterion mean sd min max\n1      Content  3.0  0 3.0 3.0\n2 Organization  3.0  0 3.0 3.0\n3     Language  3.5  0 3.5 3.5\n4        Total  9.5  0 9.5 9.5\n\n\nWhoa! These are the exact same scores as when we didn’t provide detailed rubric explanation in our prompt. This suggests to me that the model is generating a similar rubric each time through invisible instructions, but I can’t know this for sure.\nIt has also been empirically demonstrated (find the citations, Chris) that these models don’t apply rubrics well at the extremes - the lowest and highest scores. When I used Claude to help generate synthetic student responses, I asked it have a range of score values represented in the 10 generated essays. It’ll be useful to repeat the same experiment on all 10 essays to see if the scoring variability is consistent across the score range. (There’s a strong possibility that it will be - I asked Claude to generate the essays and now I’m asking it to score essays it generated.)\nBatch Scoring\nNow that we’re repeating the same task across a batch of essays, it’s a great time to introduce batch scoring! This generally works as you’d think - you submit multiple tasks at once to the API to complete in parallel - with the one wrinkle that Anthropic requires that the requests be in the JSONL format. In addition to being able to submit multiple requests at once, it’s also cheaper to use than single-call API interactions. As of this writing (Oct 19, 2025), you save 50% with batch pricing. This drops input costs from $3 per 1M tokens to $1.50 per 1M token, and output costs from $15 per 1M tokens to $7.50 per 1M tokens.\nCreating Batch Requests\n\nCode# Step 1: Read essays and create batch requests\ncreate_batch_requests &lt;- function(essays_df, n_repetitions = 20){\n\n  # Create a list to store batch requests\n  batch_requests &lt;- list()\n  request_counter &lt;- 1\n  \n  # Create n_repetitions requests for each essay (grouped by essay)\n  for(i in 1:nrow(essays_df)){\n    essay_prompt &lt;- claude_all_rubric_prompt(essays_df$prompt[i], essays_df$essay[i])\n    \n    # Create 20 requests for this essay\n    for(rep in 1:n_repetitions){\n      request &lt;- list(\n        custom_id = paste0(\"essay_\", essays_df$id[i], \"_rep_\", rep),\n        params = list(\n          model = \"claude-sonnet-4-20250514\",\n          max_tokens = 1024,\n          messages = list(\n            list(\n              role = \"user\",\n              content = essay_prompt\n            )\n          )\n        )\n      )\n      \n      batch_requests[[request_counter]] &lt;- request\n      request_counter &lt;- request_counter + 1\n    }\n  }\n  \n  cat(\"Created\", length(batch_requests), \"batch requests for\", nrow(essays_df), \"essays\\n\")\n  \n  return(batch_requests)\n}\n\nessay_batch_requests &lt;- create_batch_requests(student_essays)\n\n\nWriting Requests to JSONL\n\nCode# Step 2: Write requests to JSONL file\nwrite_batch_file &lt;- function(batch_requests, output_file = \"batch_requests.jsonl\"){\n  # Write each request as a JSON line\n  jsonl_lines &lt;- sapply(batch_requests, function(req){\n    toJSON(req, auto_unbox = TRUE)\n  })\n  \n  writeLines(jsonl_lines, output_file)\n  return(output_file)\n}\n\nessay_batch_jsonl &lt;- write_batch_file(essay_batch_requests)\n\n\nSubmitting Batch Request\n\nCode# Step 3: Submit batch job to Anthropic\nsubmit_batch &lt;- function(jsonl_file){\n  # Read and parse each line of the JSONL file\n  jsonl_lines &lt;- readLines(jsonl_file)\n  \n  # Parse each JSON line into a list\n  requests_list &lt;- lapply(jsonl_lines, function(line){\n    fromJSON(line, simplifyVector = FALSE)\n  })\n  \n  # Create batch\n  response &lt;- POST(\n    url = \"https://api.anthropic.com/v1/messages/batches\",\n    add_headers(\n      \"x-api-key\" = Sys.getenv(\"ANTHROPIC_API_KEY\"),\n      \"anthropic-version\" = \"2023-06-01\",\n      \"content-type\" = \"application/json\"\n    ),\n    body = toJSON(list(\n      requests = requests_list\n    ), auto_unbox = TRUE),\n    encode = \"json\"\n  )\n  \n  if(status_code(response) != 200){\n    stop(\"Error submitting batch: \", content(response, \"text\"))\n  }\n  \n  result &lt;- content(response, \"parsed\")\n  cat(\"Batch submitted successfully!\\n\")\n  cat(\"Batch ID:\", result$id, \"\\n\")\n  \n  return(result)\n}\n\nessay_batch_info &lt;- submit_batch(essay_batch_jsonl)\n\n# Batch submitted successfully!\n# Batch ID: msgbatch_018m5hGS9Lm3Gi28UorfsVTS\n\n\nChecking Batch Status\n\nCode# Step 4: Check batch status\ncheck_batch_status &lt;- function(batch_id){\n  response &lt;- GET(\n    url = paste0(\"https://api.anthropic.com/v1/messages/batches/\", batch_id),\n    add_headers(\n      \"x-api-key\" = Sys.getenv(\"ANTHROPIC_API_KEY\"),\n      \"anthropic-version\" = \"2023-06-01\"\n    )\n  )\n  \n  if(status_code(response) != 200){\n    stop(\"Error checking batch status: \", content(response, \"text\"))\n  }\n  \n  result &lt;- content(response, \"parsed\")\n  return(result)\n}\n\nessay_status &lt;- check_batch_status(essay_batch_info$id)\n# Finished!\n\nlibrary(lubridate)\nbatch_start_time &lt;- ymd_hms(essay_status$created_at)\nbatch_stop_time &lt;- ymd_hms(essay_status$ended_at)\nbatch_run_time &lt;- as.numeric(batch_stop_time - batch_start_time)\n# Time difference of 1.644671 mins\n# Less than 2 minutes to score 200 essays!\n\n\nSaving Batch Results\n\nCodeget_batch_results &lt;- function(batch_id){\n  response &lt;- GET(\n    url = paste0(\"https://api.anthropic.com/v1/messages/batches/\", batch_id, \"/results\"),\n    add_headers(\n      \"x-api-key\" = Sys.getenv(\"ANTHROPIC_API_KEY\"),\n      \"anthropic-version\" = \"2023-06-01\"\n    )\n  )\n  \n  if(status_code(response) != 200){\n    stop(\"Error retrieving results: \", content(response, \"text\"))\n  }\n  \n  # Parse JSONL results\n  results_text &lt;- content(response, \"text\", encoding = \"UTF-8\")\n  results_lines &lt;- strsplit(results_text, \"\\n\")[[1]]\n  results_list &lt;- lapply(results_lines[results_lines != \"\"], fromJSON)\n  \n  return(results_list)\n}\n\nessay_batch_results &lt;- get_batch_results(essay_batch_info$id)\nsave(essay_batch_results, file = \"data/essay_batch_results.Rdata\")\n\n\nExtracting Score from Results\n\nCode# Step 6: Extract scores from batch results\nextract_batch_scores &lt;- function(results_list, debug = FALSE){\n  scores_df &lt;- data.frame(\n    essay_id = character(),\n    repetition = integer(),\n    content = numeric(),\n    organization = numeric(),\n    language = numeric(),\n    total = numeric(),\n    stringsAsFactors = FALSE\n  )\n  \n  for(i in seq_along(results_list)){\n    result &lt;- results_list[[i]]\n    \n    if(result$result$type == \"succeeded\"){\n      # Extract the response text - handle different possible structures\n      message_content &lt;- result$result$message$content\n      \n      # Debug: Print structure if requested\n      if(debug && i == 1){\n        cat(\"=== DEBUG: First result structure ===\\n\")\n        cat(\"Content class:\", class(message_content), \"\\n\")\n        cat(\"Content length:\", length(message_content), \"\\n\")\n        if(is.list(message_content) && length(message_content) &gt; 0){\n          cat(\"First element class:\", class(message_content[[1]]), \"\\n\")\n          print(str(message_content[[1]]))\n        }\n      }\n      \n      # Extract text from various possible structures\n      response_text &lt;- NULL\n      \n      if(is.data.frame(message_content)){\n        # If it's a data frame, look for a 'text' column\n        if(\"text\" %in% names(message_content)){\n          response_text &lt;- message_content$text[1]\n        }\n      } else if(is.list(message_content) && length(message_content) &gt; 0){\n        first_elem &lt;- message_content[[1]]\n        \n        if(is.list(first_elem)){\n          # Check for $text field\n          if(!is.null(first_elem$text)){\n            response_text &lt;- first_elem$text\n          } else if(!is.null(first_elem$type) && first_elem$type == \"text\" && !is.null(first_elem$text)){\n            response_text &lt;- first_elem$text\n          }\n        } else if(is.character(first_elem)){\n          response_text &lt;- first_elem\n        }\n      } else if(is.character(message_content)){\n        response_text &lt;- message_content\n      }\n      \n      # If we still don't have text, try to extract it more aggressively\n      if(is.null(response_text) || length(response_text) == 0){\n        # Try to find any 'text' field in the structure\n        if(is.list(message_content)){\n          text_fields &lt;- unlist(lapply(message_content, function(x) {\n            if(is.list(x) && \"text\" %in% names(x)) x$text else NULL\n          }))\n          if(length(text_fields) &gt; 0) response_text &lt;- text_fields[1]\n        }\n      }\n      \n      if(is.null(response_text) || length(response_text) == 0){\n        warning(\"Could not extract text for \", result$custom_id)\n        if(debug){\n          cat(\"=== Full structure for \", result$custom_id, \" ===\\n\")\n          print(str(message_content))\n        }\n        next\n      }\n      \n      # Debug: Print first response text\n      if(debug && i == 1){\n        cat(\"\\n=== First response text (first 500 chars) ===\\n\")\n        cat(substr(response_text, 1, 500), \"\\n\\n\")\n      }\n      \n      # Extract scores using our previous function\n      scores &lt;- extract_scores(response_text, debug = debug && i == 1)\n      \n      # Debug: Check if scores were extracted\n      if(debug && i == 1){\n        cat(\"=== Extracted scores ===\\n\")\n        print(scores)\n      }\n      \n      # Parse custom_id to extract essay_id and repetition\n      # Format: essay_2001_rep_1\n      custom_id_parts &lt;- strsplit(result$custom_id, \"_\")[[1]]\n      essay_id &lt;- custom_id_parts[2]\n      repetition &lt;- as.integer(custom_id_parts[4])\n      \n      scores_df &lt;- rbind(scores_df, data.frame(\n        essay_id = essay_id,\n        repetition = repetition,\n        content = scores[\"content\"],\n        organization = scores[\"organization\"],\n        language = scores[\"language\"],\n        total = scores[\"total\"]\n      ))\n    } else {\n      warning(\"Request \", result$custom_id, \" failed: \", result$result$error$message)\n    }\n  }\n  \n  return(scores_df)\n}\n\n\nessay_batch_df &lt;- extract_batch_scores(essay_batch_results, debug = TRUE)\nsave(essay_batch_df, file = 'data/essay_batch_df.Rdata')\n\n\n\nCodelibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nCodelibrary(DT)\nload('data/essay_batch_df.Rdata')\n\nessay_summary_detailed &lt;- essay_batch_df %&gt;%\n  group_by(essay_id) %&gt;%\n  summarise(\n    n = n(),\n    # Content\n    content_mean = mean(content, na.rm = TRUE),\n    content_sd = sd(content, na.rm = TRUE),\n    content_min = min(content, na.rm = TRUE),\n    content_max = max(content, na.rm = TRUE),\n    # Organization\n    org_mean = mean(organization, na.rm = TRUE),\n    org_sd = sd(organization, na.rm = TRUE),\n    org_min = min(organization, na.rm = TRUE),\n    org_max = max(organization, na.rm = TRUE),\n    # Language\n    lang_mean = mean(language, na.rm = TRUE),\n    lang_sd = sd(language, na.rm = TRUE),\n    lang_min = min(language, na.rm = TRUE),\n    lang_max = max(language, na.rm = TRUE),\n    # Total\n    total_mean = mean(total, na.rm = TRUE),\n    total_sd = sd(total, na.rm = TRUE),\n    total_min = min(total, na.rm = TRUE),\n    total_max = max(total, na.rm = TRUE),\n  ) %&gt;%\n  arrange(total_mean)\n\nessay_summary_detailed &lt;- essay_summary_detailed %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 2)))\n\ndatatable(essay_summary_detailed, \n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"Summary Statistics by Essay (20 repetitions each)\")\n\n\n\n\n\nSo we did see some variation, but I also used the default temperature.\nBatch Scoring (Temp = 0)\nLet’s redo the batch call with temperature = 0.\n\nCodecreate_lowtemp_requests &lt;- function(essays_df, n_repetitions = 20){\n\n  # Create a list to store batch requests\n  batch_requests &lt;- list()\n  request_counter &lt;- 1\n  \n  # Create n_repetitions requests for each essay (grouped by essay)\n  for(i in 1:nrow(essays_df)){\n    essay_prompt &lt;- claude_all_rubric_prompt(essays_df$prompt[i], essays_df$essay[i])\n    \n    # Create 20 requests for this essay\n    for(rep in 1:n_repetitions){\n      request &lt;- list(\n        custom_id = paste0(\"essay_\", essays_df$id[i], \"_rep_\", rep),\n        params = list(\n          model = \"claude-sonnet-4-20250514\",\n          max_tokens = 1024,\n          temperature = 0, # added low temp\n          messages = list(\n            list(\n              role = \"user\",\n              content = essay_prompt\n            )\n          )\n        )\n      )\n      \n      batch_requests[[request_counter]] &lt;- request\n      request_counter &lt;- request_counter + 1\n    }\n  }\n  \n  cat(\"Created\", length(batch_requests), \"batch requests for\", nrow(essays_df), \"essays\\n\")\n  \n  return(batch_requests)\n}\n\nlowtemp_batch_requests &lt;- create_lowtemp_requests(student_essays)\nlowtemp_batch_jsonl &lt;- write_batch_file(lowtemp_batch_requests)\nlowtemp_batch_info &lt;- submit_batch(lowtemp_batch_jsonl)\n# Batch submitted successfully!\n# Batch ID: msgbatch_0184nJ7Kui9v6WVVh47939mn \n\nlowtemp_status &lt;- check_batch_status(lowtemp_batch_info$id)\nlowtemp_start_time &lt;- ymd_hms(lowtemp_status$created_at)\nlowtemp_stop_time &lt;- ymd_hms(lowtemp_status$ended_at)\nlowtemp_run_time &lt;- as.numeric(lowtemp_stop_time - lowtemp_start_time)\n# Time difference of 1.473892 mins\n\nlowtemp_batch_results &lt;- get_batch_results(lowtemp_batch_info$id)\nsave(lowtemp_batch_results, file = \"data/lowtemp_batch_results.Rdata\")\n\nlowtemp_batch_df &lt;- extract_batch_scores(lowtemp_batch_results)\nsave(lowtemp_batch_df, file = 'data/lowtemp_batch_df.Rdata')\n\n\n\nCodeload('data/lowtemp_batch_df.Rdata')\n\nlowtemp_summary_detailed &lt;- lowtemp_batch_df %&gt;%\n  group_by(essay_id) %&gt;%\n  summarise(\n    n = n(),\n    # Content\n    content_mean = mean(content, na.rm = TRUE),\n    content_sd = sd(content, na.rm = TRUE),\n    content_min = min(content, na.rm = TRUE),\n    content_max = max(content, na.rm = TRUE),\n    # Organization\n    org_mean = mean(organization, na.rm = TRUE),\n    org_sd = sd(organization, na.rm = TRUE),\n    org_min = min(organization, na.rm = TRUE),\n    org_max = max(organization, na.rm = TRUE),\n    # Language\n    lang_mean = mean(language, na.rm = TRUE),\n    lang_sd = sd(language, na.rm = TRUE),\n    lang_min = min(language, na.rm = TRUE),\n    lang_max = max(language, na.rm = TRUE),\n    # Total\n    total_mean = mean(total, na.rm = TRUE),\n    total_sd = sd(total, na.rm = TRUE),\n    total_min = min(total, na.rm = TRUE),\n    total_max = max(total, na.rm = TRUE),\n  ) %&gt;%\n  arrange(total_mean)\n\nlowtemp_summary_detailed &lt;- lowtemp_summary_detailed %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 2)))\n\ndatatable(lowtemp_summary_detailed, \n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"Summary Statistics by Essay (20 repetitions each) - Temperature = 0\")\n\n\n\n\n\nNow we’re seeing more consistent scores, and it took less than 90 seconds to process all 200 requests.\nBatch Processing Notes\nMore notes on batch processing (via the Anthropic page about batching; each provider will likely have different functionality):\n\nThis approach is well-suited to tasks that do not require immediate responses, with most batches finishing in less than 1 hour while reducing costs by 50% and increasing throughput.\nA Message Batch is limited to either 100,000 Message requests or 256 MB in size, whichever is reached first.\nWe process each batch as fast as possible, with most batches completing within 1 hour. You will be able to access batch results when all messages have completed or after 24 hours, whichever comes first. Batches will expire if processing does not complete within 24 hours.\nBatch results are available for 29 days after creation. After that, you may still view the Batch, but its results will no longer be available for download.\nRate limits apply to both Batches API HTTP requests and the number of requests within a batch waiting to be processed. See Message Batches API rate limits. Additionally, we may slow down processing based on current demand and your request volume. In that case, you may see more requests expiring after 24 hours.",
    "crumbs": [
      "Activity: Scoring with Rubrics"
    ]
  },
  {
    "objectID": "07-reference-materials.html#reference-guides",
    "href": "07-reference-materials.html#reference-guides",
    "title": "Reference Materials",
    "section": "Reference Guides",
    "text": "Reference Guides\nOpenAI Cookbook",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#newsletters",
    "href": "07-reference-materials.html#newsletters",
    "title": "Reference Materials",
    "section": "Newsletters",
    "text": "Newsletters\n\nTL:DR\n\n\nSuperhuman\n\n\nThe Neuron\n\n\nGary Marcus Substack\n\n\nJack Anthropic Substack (The Signal?)\n\n\nMichael Jabbour Substack\n\n\nOne Useful Thing",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#books",
    "href": "07-reference-materials.html#books",
    "title": "Reference Materials",
    "section": "Books",
    "text": "Books\nAI Engineering by Chip Huyen is a slightly more advanced read for those interested in building AI products.\nPrompt Engineering for Generative AI: Future-Proof Inputs for Reliable AI Outputs by James Phoenix and Mike Taylor is also a good resource for learning more about prompt engineering. Focused more on earlier (non-reasoning) models, but some important parts carry through.\nBrave New Words by Sal Khan (of Khan Academy fame) is an interesting perspective on how AI will change education.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#blogs",
    "href": "07-reference-materials.html#blogs",
    "title": "Reference Materials",
    "section": "Blogs",
    "text": "Blogs\nAre there any?",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#training",
    "href": "07-reference-materials.html#training",
    "title": "Reference Materials",
    "section": "Training",
    "text": "Training\n\nDataCamp",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#sec-r-packages",
    "href": "07-reference-materials.html#sec-r-packages",
    "title": "Reference Materials",
    "section": "LLM-specific R Packages",
    "text": "LLM-specific R Packages\nA number of packages have been developed to more easily facilitate interacting with LLMs via R. Many of these packages are useful (we’ll cover some of those in the workshop), whereas other packages include some developer design decisions that don’t work particularly well for my usual workflows. I’ve also found that some packages aren’t often updated / maintained. The syntax to interact with API models can change as new models are released (e.g., ChatGPT5), which can render some of the package functionality obsolete.\nBelow is a non-exhaustive list of packages that I’ve found to interact with LLMs. This is not meant to be exhaustive or a curated list; it’s only to provide you with information about the packages you’ll be using in the workshop (and others) in the case you find them helpful for your workflow. All package summaries were initially generated with AI. Some summaries have been edited, some have not.\n\nellmer\nellmer Overview CRAN Documentation\nellmer is an R package that provides a unified interface for interacting with large language models from over 17 providers including OpenAI, Anthropic, Google Gemini, and AWS Bedrock. It supports advanced features like streaming outputs, tool/function calling, structured data extraction, and multimodal inputs. Chat objects are stateful and maintain conversation context, enabling both interactive console-based conversations and programmatic use in R scripts and applications.\n\n\ntidyprompt\ntidyprompt Overview CRAN Documentation\ntidyprompt is an R package that provides a compositional framework (“prompt wraps”) for building prompts enriched with logic, validation, and extraction functions when interacting with LLMs. It supports structured output, retry/feedback loops, reasoning strategies (e.g. ReAct or chain-of-thought), and even autonomous R code or function calling as part of an LLM dialogue. The package is provider-agnostic, meaning its features can layer on top of any chat completion API (e.g. via ellmer) to produce more robust, predictable interactions.\n\n\ntidyllm\ntidyllm Overview CRAN Documentation\ntidyllm provides a tidy, pipeline-friendly interface for interacting with multiple LLM APIs (e.g. Claude, OpenAI, Gemini, Mistral) and local models via Ollama. It supports multimodal inputs (text, images, PDFs), maintains conversational history, handles batching and rate limits, and allows structured schema-based extraction of responses. The design emphasizes composability and integration into typical R data workflows.\n\n\nchattr\nchattr Overview CRAN Documentation\nchattr is an R package that enables interactive communication with large language models directly within RStudio using a Shiny gadget or from the console. It enriches prompts with contextual information (e.g. loaded data frames) and integrates with various back-ends (e.g. OpenAI, Copilot, local LlamaGPT) via the ellmer interface. The package is geared toward exploratory workflows and rapid prototyping of LLM-assisted analysis.\n\n\nLLMAgentR\nLLMAgentR Overview CRAN Documentation\nLLMAgentR is an R package for constructing language model “agents” using a modular, graph-based execution framework inspired by LangChain/LangGraph architectures. It offers a suite of agent types (e.g. code generation, data wrangling, SQL agents, document summarization) that iteratively reason, generate R code, execute, debug, and explain results. The package aims to support reproducible AI workflows for analysis, research, and automation by integrating LLM reasoning and domain logic.\n\n\nPacketLLM\nPacketLLM Overview CRAN Documentation\nPacketLLM offers an interactive RStudio gadget interface for chatting with OpenAI LLMs (e.g. GPT-5 and variants) directly within the R environment. It supports multiple simultaneous conversation tabs, file upload (e.g. .R, PDF, DOCX) as contextual input, and per-conversation system message configuration. API calls are handled asynchronously (via promises + future) to avoid blocking the R console during model interactions.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "03c-calling-other-llms.html",
    "href": "03c-calling-other-llms.html",
    "title": "\n6  Call other (non-Gen AI) LLMs\n",
    "section": "",
    "text": "Although not the focus of our workshop, there are many use cases in educational measurement when a different LLM model might be useful. See, for example, DeBERTa, a strong non-generative LLM that has improvements over the BERT and RoBERTa models (original paper by He, Liu, Gao, and Chen (2020) that introduces the model is here; see this ‘towards data science’ article for a higher-level overview).\nAs per ChatGPT5: “DeBERTa (Decoding-enhanced BERT with Disentangled Attention) is useful in educational research because it offers high representational accuracy for text understanding tasks—such as rubric-based scoring, feedback classification, or analyzing written responses—without requiring generative capabilities. Its disentangled attention mechanism separates word content and position information, improving sensitivity to subtle linguistic and contextual cues (e.g., reasoning quality, coherence, stance). Combined with enhanced pretraining (including next-sentence prediction and span masking), DeBERTa often outperforms earlier encoder models like BERT and RoBERTa on natural language understanding benchmarks, making it a strong choice for reliable, fine-grained analysis of student writing or assessment data.”\nDeBERTa and many other LLMs are available through Hugging Face (huggingface.co). Many of these models are open-source and can be downloaded and run locally. If you’re like me and don’t have the technical expertise required to implement such a workflow, the good news is that you can call many of these models through a Hugging Face API using similiar syntax as to what we’ll be using today. You’ll have to sign up for an account, and the free account provides you1 with a fair amount of capabilities: 100GB private storage limit, 1000 API calls (per 5-minute window), 5,000 resolvers(per 5-minute window; delayed API calls), and 200 pages.\nHere’s the a list of other models available though Hugging Face.\nHere’s an example of a simple fill-mask task calling the BERT model through the Hugging Face API.",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Call other (non-Gen AI) LLMs</span>"
    ]
  },
  {
    "objectID": "03c-calling-other-llms.html#footnotes",
    "href": "03c-calling-other-llms.html#footnotes",
    "title": "\n6  Call other (non-Gen AI) LLMs\n",
    "section": "",
    "text": "As of this writing (October 21, 2025).↩︎",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Call other (non-Gen AI) LLMs</span>"
    ]
  },
  {
    "objectID": "activity-shiny-app-development.html",
    "href": "activity-shiny-app-development.html",
    "title": "Activity: Shiny App Development",
    "section": "",
    "text": "Data Visualization with Existing Data\nNow it’s time to make an app! When building an app via vibe coding, I’ve found that it’s best to ask the model to add features to the app in chunks. Providing a long list of “must-haves” to the model in a single prompt can overwhelm the model, and it can be difficult to make tweaks to small parts of the app.\nFor my process, I usually outline the basic functionality of what I want the app to do in the first prompt, knowing that there will be additional functionality that I want to build out later. I will sometimes write out a list of the functionality that I want to be integrated into the app. I then review the list and see if there is a logical structure to order that I want the components built - do any features depend on having other functionality in place. From there I then start the process of iterating with the model to develop the code that I want.\nNote: Working with a model to vibe code can use a lot of tokens quickly. I have the lowest-tier plans with both Anthropic and ChatGPT, and the only time I’ve been told that I need to wait to submit another prompt is when I’ve been engaged in extensive vibe-coding sessions.",
    "crumbs": [
      "Activity: Shiny App Development"
    ]
  },
  {
    "objectID": "activity-shiny-app-development.html#integrating-llm-calls-in-an-app",
    "href": "activity-shiny-app-development.html#integrating-llm-calls-in-an-app",
    "title": "Activity: Shiny App Development",
    "section": "Integrating LLM Calls in an App",
    "text": "Integrating LLM Calls in an App\nNow let’s pull together a few of the things you learned today and build an app that integrates calling an LLM! This purpose of this simple app is to help educators / test developers generate items given certain parameters - student grade level and school subject.\nWhen developing a prompt for vibe coding, I often ask a model to help me improve my first prompt.\n\n\n\n\n\n\nInitial Prompt that I wrote:\n\n\n\n\n\nPrompt:\nI want to make an R Shiny app to facilitate making multiple-choice questions for a variety of grade levels for a variety of subjects.\nOn the left is a drop-down menu is has two selectable options: grade level (1st, 3rd, 5th, 7th) and topic (geography, science, social studies). This will serve to an input for a function that will call a generative AI model to generate a multiple-choice item based on the selected topic that is appropriate for the selected grade level. The generative AI model will return a question [question], 5 answer options [option A : option E], and the repeat the correct answer option [correct answer : ]. Here is an example:\n[Question : What is the main source of energy for the water cycle?]\nIn the second column: Those responses will each populate in a different text box column that corresponds to the LLM output (question, option A, option B, option C, option D, option E, correct answer) with instructions to the user to “change the item as desired”. The user can then makes changes if they would like, and then re-submit the item. Alternatively, if the user does not want to make changes or is happy with the submitted changes, the user can select “submit item” which will then save the item to a certain location on the computer.\nI want the user to ask the app to re-generate parts of an MCQ question. So the user can “save” or “lock” aspects of the LLM response (such as the item stem or some of the responses). I also want the user to be able to provide instructions to the LLM when asking for the item to be re-generated. So if the item is on volcanos and the LLM make an item about their location, the user can say “focus the question on lava” and the LLM will take this information and re-generate the item.\nThere should be two user input option buttons after finishing the task. “Update item” where the submitted information becomes the pre-populated item info, or “Save Item” when the information is satisfactory and should be saved.\nOnce the user selects “Save Item” when the item is finished, I would like this information saved to a list in R. So whatever is in the “Question” text box is written to a list entry called question, whatever is in the “Option A” text box is written to a list called optionA, and so on.\nWrite that syntax.\n\n\n\n\n\n\n\n\n\nThe prompt to the LLM to improve the prompt\n\n\n\n\n\nHere’s a prompt to ask an LLM to build an app that I’d like to include as an example for my workshop. Your task is to review the prompt and make improvements to it’s organization, language, and structure so it’s more appropriate to serve as an LLM prompt. Please ask clarifying questions after reviewing the prompt if it will help you in your refinement. The prompt:\n\n\n\n\n\n\n\n\n\nFinal LLM-assisted Prompt\n\n\n\n\n\nYou are an expert R Shiny developer.\nBuild a complete, runnable single-file R Shiny app (app.R) that helps users generate, edit, and save multiple-choice questions (MCQs) by grade level and subject.\n🎯 Goals\n\nLet users select a Grade Level and Subject.\nCall a generative AI helper function to produce a question and five answer options (A–E), plus the correct answer.\nDisplay the generated text in editable text boxes so users can make changes.\nProvide two buttons:\n\n“Update item” – commits edited text to the current item.\n“Save item” – appends the finalized item to a persistent list stored in the app’s working directory.\n\n\nStore all items (including Grade and Subject metadata) in an .rds file named saved_items.rds.\n⚙️ App Specifications\nUI\nLeft sidebar:\n\nselectInput(\"grade\", \"Grade Level\", choices = c(\"1st\", \"3rd\", \"5th\", \"7th\"))\nselectInput(\"subject\", \"Subject\", choices = c(\"Geography\", \"Science\", \"Social Studies\"))\nactionButton(\"generate\", \"Generate Item\")\n\nMain panel:\nEditable text boxes for:\n\nQuestion\nOption A\nOption B\nOption C\nOption D\nOption E\nCorrect Answer\n\n(Each labeled clearly with a hint: “Change the item as desired.”)\nAction buttons:\n\nactionButton(\"update_item\", \"Update Item\")\nactionButton(\"save_item\", \"Save Item\")\n\nDisplay area showing:\n\nA message or notification confirming updates/saves\nA summary table (renderTable) listing saved items (Grade, Subject, Question stem, Timestamp)\nServer Logic\nMaintain a reactiveValues() object named current_item containing:\n\ngrade, subject, question, optionA, optionB, optionC, optionD, optionE, correctAnswer, and timestamp.\n\nGenerate Item\n\nUse observeEvent(input$generate, ...) to call a helper function generate_item_via_ai(grade, subject).\nThe helper function returns a named list of 7 fields based on the AI output format below.\nPopulate each text input area with the returned values.\n\nAI Output Format\nThe model should return text formatted exactly as:\nQuestion: &lt;text&gt;\nOption A: &lt;text&gt;\nOption B: &lt;text&gt;\nOption C: &lt;text&gt;\nOption D: &lt;text&gt;\nOption E: &lt;text&gt;\nCorrect Answer: Option &lt;A|B|C|D|E&gt;: &lt;repeat the correct option text verbatim&gt;\nExample:\nQuestion: What is the main source of energy for the water cycle?\nOption A: The Moon\nOption B: The Sun\nOption C: The Wind\nOption D: The Earth's Core\nOption E: Ocean Currents\nCorrect Answer: Option B: The Sun\nHelper Function\nImplement generate_item_via_ai() with a live model call to an Anthropic model.\nUpdate Item\n\nWhen “Update Item” is clicked, read all text inputs and overwrite the current_item values.\nValidate that all fields are filled and that exactly five options (A–E) exist.\n\nSave Item\n\nValidate fields again before saving.\nAppend current_item to a reactive list saved_items() (using reactiveVal).\nInclude metadata fields: Grade, Subject, and Timestamp.\nSave the entire list as an .rds file (saveRDS(saved_items(), \"saved_items.rds\")) in the working directory.\nOn app startup, if “saved_items.rds” exists, load it with readRDS().\n\nFeedback\n\nUse showNotification() for user feedback (e.g., “Item saved successfully!” or “Missing correct answer.”).\nDisplay a summary table of all saved items.\n🧩 Implementation Details\n\nUse base shiny and bslib (optional for theming).\nKeep code in one self-contained file (app.R).\nComment each section clearly to support learning (especially reactivity).\nEnforce exactly five options (A–E) at all times.\nSave items to the working directory only—no external path handling.\nMaintain minimal accessibility (no additional ARIA labeling for this demo).\n✅ Deliverable\nOutput only a fully runnable app.R file in one code block.\nThe app should work immediately when run with shiny::runApp().\n\n\n\n\nWorking App with LLM Integration\nYou can either download the app below, or copy and paste the chunk of code in a new .R file. Then highlight and execute the text. It may take a minute or two the first time the app runs for it to be built, but a new window should open and the app should run. In the background the app is using the ANTHROPIC_API_KEY that was provided to you as part of the workshop. You’ll have to get your own Anthropic API Key to continue running the app outside of the workshop hours.\n 📥 Download mcq_maker_app \n\n\n\n\n\n\nView the App Code Generated by Claude\n\n\n\n\n\nGiven the prompt above, here is what Claude produced:\n\nCode# MCQ Generator Shiny App\n# Single-file app with field locking and AI-powered regeneration\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(httr2)\n\n# ==============================================================================\n# AI HELPER FUNCTIONS\n# ==============================================================================\n\n#' Generate a complete MCQ item via AI\n#'\n#' @param grade Grade level (e.g., \"1st\", \"3rd\", \"5th\", \"7th\")\n#' @param subject Subject (e.g., \"Geography\", \"Science\", \"Social Studies\")\n#' @return Named list with: question, optionA, optionB, optionC, optionD, optionE, correctAnswer\ngenerate_item_via_ai &lt;- function(grade, subject) {\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n  if (api_key == \"\") {\n    stop(\"ANTHROPIC_API_KEY not found in .Renviron\")\n  }\n  \n  system_prompt &lt;- \"You are an expert assessment item writer. Produce age-appropriate MCQs with exactly five options (A-E) and a single correct answer. Follow the output format strictly.\"\n  \n  user_prompt &lt;- sprintf(\n    \"Context: Grade = %s, Subject = %s\\n\\nGenerate a multiple-choice question suitable for this grade level and subject.\\n\\nOutput Format (required):\\nQuestion: &lt;text&gt;\\nOption A: &lt;text&gt;\\nOption B: &lt;text&gt;\\nOption C: &lt;text&gt;\\nOption D: &lt;text&gt;\\nOption E: &lt;text&gt;\\nCorrect Answer: Option &lt;A|B|C|D|E&gt;: &lt;repeat the correct option text verbatim&gt;\\n\\nEnsure the correct answer key and text match one of A-E. Keep language at the specified grade level.\",\n    grade, subject\n  )\n  \n  response &lt;- request(\"https://api.anthropic.com/v1/messages\") |&gt;\n    req_headers(\n      \"x-api-key\" = api_key,\n      \"anthropic-version\" = \"2023-06-01\",\n      \"content-type\" = \"application/json\"\n    ) |&gt;\n    req_body_json(list(\n      model = \"claude-sonnet-4-20250514\",\n      max_tokens = 1024,\n      system = system_prompt,\n      messages = list(\n        list(role = \"user\", content = user_prompt)\n      )\n    )) |&gt;\n    req_perform() |&gt;\n    resp_body_json()\n  \n  content_text &lt;- response$content[[1]]$text\n  return(parse_ai_output(content_text))\n}\n\n#' Re-generate an MCQ item via AI with locked fields\n#'\n#' @param grade Grade level\n#' @param subject Subject\n#' @param instructions Free-text instructions for regeneration\n#' @param locked_values Named list of locked fields (subset of: question, optionA-E, correctAnswer)\n#' @return Named list with all 7 fields (locked fields unchanged)\nregenerate_item_via_ai &lt;- function(grade, subject, instructions, locked_values) {\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n  if (api_key == \"\") {\n    stop(\"ANTHROPIC_API_KEY not found in .Renviron\")\n  }\n  \n  system_prompt &lt;- \"You are an expert assessment item writer. Produce age-appropriate MCQs with exactly five options (A-E) and a single correct answer. Follow the output format strictly.\"\n  \n  locked_fields_text &lt;- \"\"\n  if (length(locked_values) &gt; 0) {\n    locked_fields_text &lt;- \"\\n\\nLocked fields (echo these unchanged):\\n\"\n    for (field_name in names(locked_values)) {\n      locked_fields_text &lt;- paste0(locked_fields_text, field_name, \": \", locked_values[[field_name]], \"\\n\")\n    }\n  }\n  \n  instructions_text &lt;- \"\"\n  if (!is.null(instructions) && nchar(trimws(instructions)) &gt; 0) {\n    instructions_text &lt;- paste0(\"\\n\\nInstructions: \", instructions)\n  }\n  \n  user_prompt &lt;- sprintf(\n    \"Context: Grade = %s, Subject = %s%s%s\\n\\nRe-generate a multiple-choice question. Echo all locked fields unchanged. Generate new content for unlocked fields that is consistent with locked fields, grade, subject, and instructions.\\n\\nOutput Format (required):\\nQuestion: &lt;text&gt;\\nOption A: &lt;text&gt;\\nOption B: &lt;text&gt;\\nOption C: &lt;text&gt;\\nOption D: &lt;text&gt;\\nOption E: &lt;text&gt;\\nCorrect Answer: Option &lt;A|B|C|D|E&gt;: &lt;repeat the correct option text verbatim&gt;\\n\\nReturn all fields. Ensure the correct answer key and text match one of A-E. Keep language at the specified grade level.\",\n    grade, subject, locked_fields_text, instructions_text\n  )\n  \n  response &lt;- request(\"https://api.anthropic.com/v1/messages\") |&gt;\n    req_headers(\n      \"x-api-key\" = api_key,\n      \"anthropic-version\" = \"2023-06-01\",\n      \"content-type\" = \"application/json\"\n    ) |&gt;\n    req_body_json(list(\n      model = \"claude-sonnet-4-20250514\",\n      max_tokens = 1024,\n      system = system_prompt,\n      messages = list(\n        list(role = \"user\", content = user_prompt)\n      )\n    )) |&gt;\n    req_perform() |&gt;\n    resp_body_json()\n  \n  content_text &lt;- response$content[[1]]$text\n  parsed &lt;- parse_ai_output(content_text)\n  \n  # Validate locked fields\n  for (field_name in names(locked_values)) {\n    if (!identical(parsed[[field_name]], locked_values[[field_name]])) {\n      warning(paste(\"AI violated lock on field:\", field_name))\n      parsed[[field_name]] &lt;- locked_values[[field_name]]\n    }\n  }\n  \n  return(parsed)\n}\n\n#' Parse AI output text into structured format\nparse_ai_output &lt;- function(text) {\n  lines &lt;- strsplit(text, \"\\n\")[[1]]\n  lines &lt;- trimws(lines)\n  lines &lt;- lines[nchar(lines) &gt; 0]\n  \n  result &lt;- list()\n  \n  for (line in lines) {\n    if (grepl(\"^Question:\", line, ignore.case = TRUE)) {\n      result$question &lt;- trimws(sub(\"^Question:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option A:\", line, ignore.case = TRUE)) {\n      result$optionA &lt;- trimws(sub(\"^Option A:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option B:\", line, ignore.case = TRUE)) {\n      result$optionB &lt;- trimws(sub(\"^Option B:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option C:\", line, ignore.case = TRUE)) {\n      result$optionC &lt;- trimws(sub(\"^Option C:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option D:\", line, ignore.case = TRUE)) {\n      result$optionD &lt;- trimws(sub(\"^Option D:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option E:\", line, ignore.case = TRUE)) {\n      result$optionE &lt;- trimws(sub(\"^Option E:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Correct Answer:\", line, ignore.case = TRUE)) {\n      result$correctAnswer &lt;- trimws(sub(\"^Correct Answer:\", \"\", line, ignore.case = TRUE))\n    }\n  }\n  \n  return(result)\n}\n\n# ==============================================================================\n# VALIDATION FUNCTIONS\n# ==============================================================================\n\nvalidate_item &lt;- function(item) {\n  errors &lt;- c()\n  \n  # Check question\n  if (is.null(item$question) || nchar(trimws(item$question)) == 0) {\n    errors &lt;- c(errors, \"Question cannot be empty\")\n  }\n  \n  # Check all options\n  required_options &lt;- c(\"optionA\", \"optionB\", \"optionC\", \"optionD\", \"optionE\")\n  for (opt in required_options) {\n    if (is.null(item[[opt]]) || nchar(trimws(item[[opt]])) == 0) {\n      errors &lt;- c(errors, paste(\"Option\", substr(opt, 7, 7), \"cannot be empty\"))\n    }\n  }\n  \n  # Check correct answer format\n  if (is.null(item$correctAnswer) || nchar(trimws(item$correctAnswer)) == 0) {\n    errors &lt;- c(errors, \"Correct Answer cannot be empty\")\n  } else {\n    # Extract key (A-E)\n    correct_key &lt;- extract_correct_key(item$correctAnswer)\n    if (is.na(correct_key)) {\n      errors &lt;- c(errors, \"Correct Answer must start with 'Option A:', 'Option B:', etc.\")\n    } else {\n      # Verify the text matches the option\n      option_field &lt;- paste0(\"option\", correct_key)\n      expected_text &lt;- item[[option_field]]\n      actual_text &lt;- extract_correct_text(item$correctAnswer)\n      \n      if (!identical(trimws(expected_text), trimws(actual_text))) {\n        errors &lt;- c(errors, paste0(\"Correct Answer text must match Option \", correct_key, \" exactly\"))\n      }\n    }\n  }\n  \n  if (length(errors) &gt; 0) {\n    return(list(valid = FALSE, errors = errors))\n  } else {\n    return(list(valid = TRUE, errors = NULL))\n  }\n}\n\nextract_correct_key &lt;- function(correct_answer) {\n  match &lt;- regexpr(\"Option ([A-E]):\", correct_answer, ignore.case = TRUE)\n  if (match &gt; 0) {\n    key_text &lt;- regmatches(correct_answer, match)\n    key &lt;- sub(\"Option ([A-E]):.*\", \"\\\\1\", key_text, ignore.case = TRUE)\n    return(toupper(key))\n  }\n  return(NA)\n}\n\nextract_correct_text &lt;- function(correct_answer) {\n  text &lt;- sub(\"^Option [A-E]:\\\\s*\", \"\", correct_answer, ignore.case = TRUE)\n  return(trimws(text))\n}\n\n# ==============================================================================\n# UI\n# ==============================================================================\n\nui &lt;- page_sidebar(\n  title = \"MCQ Generator\",\n  theme = bs_theme(version = 5, bootswatch = \"flatly\"),\n  \n  sidebar = sidebar(\n    width = 300,\n    h4(\"Configuration\"),\n    selectInput(\n      \"grade\",\n      \"Grade Level\",\n      choices = c(\"1st\", \"3rd\", \"5th\", \"7th\"),\n      selected = \"3rd\"\n    ),\n    selectInput(\n      \"subject\",\n      \"Subject\",\n      choices = c(\"Geography\", \"Science\", \"Social Studies\"),\n      selected = \"Science\"\n    ),\n    actionButton(\n      \"generate\",\n      \"Generate Item\",\n      icon = icon(\"wand-magic-sparkles\"),\n      class = \"btn-primary w-100 mb-3\"\n    ),\n    hr(),\n    h4(\"Re-generation\"),\n    textAreaInput(\n      \"regen_instructions\",\n      \"Re-generation Instructions (optional)\",\n      placeholder = \"e.g., focus the question on lava; make distractors conceptually distinct\",\n      rows = 3\n    ),\n    actionButton(\n      \"regen_unlocked\",\n      \"Re-generate Unlocked Fields\",\n      icon = icon(\"rotate\"),\n      class = \"btn-warning w-100\"\n    ),\n    helpText(\"Locked fields will remain unchanged. Unlocked fields will be re-generated.\")\n  ),\n  \n  # Main content\n  div(\n    class = \"container-fluid\",\n    h3(\"Edit Item\"),\n    helpText(\"Change the item as desired. Lock fields you want to preserve before re-generation.\"),\n    \n    # Question\n    div(\n      class = \"row mb-3\",\n      div(\n        class = \"col-md-10\",\n        textAreaInput(\n          \"question\",\n          \"Question\",\n          value = \"\",\n          rows = 3,\n          width = \"100%\"\n        )\n      ),\n      div(\n        class = \"col-md-2\",\n        br(),\n        checkboxInput(\"lock_question\", \"Lock\", FALSE)\n      )\n    ),\n    \n    # Options A-E\n    lapply(LETTERS[1:5], function(letter) {\n      div(\n        class = \"row mb-2\",\n        div(\n          class = \"col-md-10\",\n          textInput(\n            paste0(\"option\", letter),\n            paste(\"Option\", letter),\n            value = \"\",\n            width = \"100%\"\n          )\n        ),\n        div(\n          class = \"col-md-2\",\n          br(),\n          checkboxInput(paste0(\"lock_option\", letter), \"Lock\", FALSE)\n        )\n      )\n    }),\n    \n    # Correct Answer\n    div(\n      class = \"row mb-3\",\n      div(\n        class = \"col-md-10\",\n        textInput(\n          \"correctAnswer\",\n          \"Correct Answer (format: Option X: text)\",\n          value = \"\",\n          width = \"100%\",\n          placeholder = \"e.g., Option A: Pacific Ocean\"\n        )\n      ),\n      div(\n        class = \"col-md-2\",\n        br(),\n        checkboxInput(\"lock_correctAnswer\", \"Lock\", FALSE)\n      )\n    ),\n    \n    # Action buttons\n    div(\n      class = \"row mb-4\",\n      div(\n        class = \"col-md-6\",\n        actionButton(\n          \"update_item\",\n          \"Update Item\",\n          icon = icon(\"check\"),\n          class = \"btn-success w-100\"\n        )\n      ),\n      div(\n        class = \"col-md-6\",\n        actionButton(\n          \"save_item\",\n          \"Save Item\",\n          icon = icon(\"save\"),\n          class = \"btn-info w-100\"\n        )\n      )\n    ),\n    \n    hr(),\n    \n    # Feedback area\n    h3(\"Saved Items\"),\n    uiOutput(\"feedback_message\"),\n    tableOutput(\"saved_items_table\")\n  )\n)\n\n# ==============================================================================\n# SERVER\n# ==============================================================================\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive values for current item\n  current_item &lt;- reactiveValues(\n    grade = NULL,\n    subject = NULL,\n    question = \"\",\n    optionA = \"\",\n    optionB = \"\",\n    optionC = \"\",\n    optionD = \"\",\n    optionE = \"\",\n    correctAnswer = \"\",\n    timestamp = NULL\n  )\n  \n  # Saved items list\n  saved_items &lt;- reactiveVal(list())\n  \n  # Load saved items on startup\n  observe({\n    if (file.exists(\"saved_items.rds\")) {\n      tryCatch({\n        loaded &lt;- readRDS(\"saved_items.rds\")\n        saved_items(loaded)\n        showNotification(\n          paste(\"Loaded\", length(loaded), \"saved items\"),\n          type = \"message\"\n        )\n      }, error = function(e) {\n        showNotification(\n          paste(\"Error loading saved items:\", e$message),\n          type = \"error\"\n        )\n      })\n    }\n  })\n  \n  # Generate Item\n  observeEvent(input$generate, {\n    tryCatch({\n      showNotification(\"Generating item...\", type = \"message\", duration = 2)\n      \n      item &lt;- generate_item_via_ai(input$grade, input$subject)\n      \n      # Update UI fields\n      updateTextAreaInput(session, \"question\", value = item$question)\n      updateTextInput(session, \"optionA\", value = item$optionA)\n      updateTextInput(session, \"optionB\", value = item$optionB)\n      updateTextInput(session, \"optionC\", value = item$optionC)\n      updateTextInput(session, \"optionD\", value = item$optionD)\n      updateTextInput(session, \"optionE\", value = item$optionE)\n      updateTextInput(session, \"correctAnswer\", value = item$correctAnswer)\n      \n      # Update current_item\n      current_item$grade &lt;- input$grade\n      current_item$subject &lt;- input$subject\n      current_item$question &lt;- item$question\n      current_item$optionA &lt;- item$optionA\n      current_item$optionB &lt;- item$optionB\n      current_item$optionC &lt;- item$optionC\n      current_item$optionD &lt;- item$optionD\n      current_item$optionE &lt;- item$optionE\n      current_item$correctAnswer &lt;- item$correctAnswer\n      current_item$timestamp &lt;- Sys.time()\n      \n      showNotification(\"Item generated successfully!\", type = \"message\")\n    }, error = function(e) {\n      showNotification(\n        paste(\"Error generating item:\", e$message),\n        type = \"error\",\n        duration = 5\n      )\n    })\n  })\n  \n  # Re-generate Unlocked Fields\n  observeEvent(input$regen_unlocked, {\n    tryCatch({\n      showNotification(\"Re-generating unlocked fields...\", type = \"message\", duration = 2)\n      \n      # Build locked_values from checkbox states\n      locked_values &lt;- list()\n      \n      if (input$lock_question) {\n        locked_values$question &lt;- input$question\n      }\n      if (input$lock_optionA) {\n        locked_values$optionA &lt;- input$optionA\n      }\n      if (input$lock_optionB) {\n        locked_values$optionB &lt;- input$optionB\n      }\n      if (input$lock_optionC) {\n        locked_values$optionC &lt;- input$optionC\n      }\n      if (input$lock_optionD) {\n        locked_values$optionD &lt;- input$optionD\n      }\n      if (input$lock_optionE) {\n        locked_values$optionE &lt;- input$optionE\n      }\n      if (input$lock_correctAnswer) {\n        locked_values$correctAnswer &lt;- input$correctAnswer\n      }\n      \n      # Call regeneration\n      item &lt;- regenerate_item_via_ai(\n        input$grade,\n        input$subject,\n        input$regen_instructions,\n        locked_values\n      )\n      \n      # Validate that locked fields were respected\n      violated_locks &lt;- c()\n      for (field in names(locked_values)) {\n        if (!identical(trimws(item[[field]]), trimws(locked_values[[field]]))) {\n          violated_locks &lt;- c(violated_locks, field)\n          # Restore locked value\n          item[[field]] &lt;- locked_values[[field]]\n        }\n      }\n      \n      if (length(violated_locks) &gt; 0) {\n        showNotification(\n          paste(\"Warning: AI violated locks on:\", paste(violated_locks, collapse = \", \"), \". Values restored.\"),\n          type = \"warning\",\n          duration = 5\n        )\n      }\n      \n      # Update UI fields\n      updateTextAreaInput(session, \"question\", value = item$question)\n      updateTextInput(session, \"optionA\", value = item$optionA)\n      updateTextInput(session, \"optionB\", value = item$optionB)\n      updateTextInput(session, \"optionC\", value = item$optionC)\n      updateTextInput(session, \"optionD\", value = item$optionD)\n      updateTextInput(session, \"optionE\", value = item$optionE)\n      updateTextInput(session, \"correctAnswer\", value = item$correctAnswer)\n      \n      # Update current_item\n      current_item$grade &lt;- input$grade\n      current_item$subject &lt;- input$subject\n      current_item$question &lt;- item$question\n      current_item$optionA &lt;- item$optionA\n      current_item$optionB &lt;- item$optionB\n      current_item$optionC &lt;- item$optionC\n      current_item$optionD &lt;- item$optionD\n      current_item$optionE &lt;- item$optionE\n      current_item$correctAnswer &lt;- item$correctAnswer\n      current_item$timestamp &lt;- Sys.time()\n      \n      showNotification(\"Item re-generated successfully!\", type = \"message\")\n    }, error = function(e) {\n      showNotification(\n        paste(\"Error re-generating item:\", e$message),\n        type = \"error\",\n        duration = 5\n      )\n    })\n  })\n  \n  # Update Item\n  observeEvent(input$update_item, {\n    item &lt;- list(\n      question = input$question,\n      optionA = input$optionA,\n      optionB = input$optionB,\n      optionC = input$optionC,\n      optionD = input$optionD,\n      optionE = input$optionE,\n      correctAnswer = input$correctAnswer\n    )\n    \n    validation &lt;- validate_item(item)\n    \n    if (!validation$valid) {\n      showNotification(\n        paste(\"Validation errors:\", paste(validation$errors, collapse = \"; \")),\n        type = \"error\",\n        duration = 5\n      )\n    } else {\n      current_item$grade &lt;- input$grade\n      current_item$subject &lt;- input$subject\n      current_item$question &lt;- item$question\n      current_item$optionA &lt;- item$optionA\n      current_item$optionB &lt;- item$optionB\n      current_item$optionC &lt;- item$optionC\n      current_item$optionD &lt;- item$optionD\n      current_item$optionE &lt;- item$optionE\n      current_item$correctAnswer &lt;- item$correctAnswer\n      current_item$timestamp &lt;- Sys.time()\n      \n      showNotification(\"Item updated successfully!\", type = \"message\")\n    }\n  })\n  \n  # Save Item\n  observeEvent(input$save_item, {\n    item &lt;- list(\n      grade = input$grade,\n      subject = input$subject,\n      question = input$question,\n      optionA = input$optionA,\n      optionB = input$optionB,\n      optionC = input$optionC,\n      optionD = input$optionD,\n      optionE = input$optionE,\n      correctAnswer = input$correctAnswer\n    )\n    \n    validation &lt;- validate_item(item)\n    \n    if (!validation$valid) {\n      showNotification(\n        paste(\"Cannot save. Validation errors:\", paste(validation$errors, collapse = \"; \")),\n        type = \"error\",\n        duration = 5\n      )\n    } else {\n      # Add timestamp\n      item$timestamp &lt;- Sys.time()\n      \n      # Append to saved items\n      current_saved &lt;- saved_items()\n      current_saved[[length(current_saved) + 1]] &lt;- item\n      saved_items(current_saved)\n      \n      # Save to file\n      tryCatch({\n        saveRDS(current_saved, \"saved_items.rds\")\n        showNotification(\n          paste(\"Item saved successfully! Total items:\", length(current_saved)),\n          type = \"message\",\n          duration = 3\n        )\n      }, error = function(e) {\n        showNotification(\n          paste(\"Error saving to file:\", e$message),\n          type = \"error\",\n          duration = 5\n        )\n      })\n    }\n  })\n  \n  # Render saved items table\n  output$saved_items_table &lt;- renderTable({\n    items &lt;- saved_items()\n    \n    if (length(items) == 0) {\n      return(data.frame(\n        Message = \"No items saved yet\"\n      ))\n    }\n    \n    # Create summary table\n    summary_df &lt;- do.call(rbind, lapply(seq_along(items), function(i) {\n      item &lt;- items[[i]]\n      data.frame(\n        Index = i,\n        Grade = item$grade,\n        Subject = item$subject,\n        Question = substr(item$question, 1, 60),\n        Timestamp = as.character(item$timestamp),\n        stringsAsFactors = FALSE\n      )\n    }))\n    \n    summary_df\n  }, striped = TRUE, hover = TRUE, bordered = TRUE)\n  \n  # Feedback message\n  output$feedback_message &lt;- renderUI({\n    items &lt;- saved_items()\n    if (length(items) &gt; 0) {\n      div(\n        class = \"alert alert-info\",\n        icon(\"circle-info\"),\n        paste(\"Total saved items:\", length(items))\n      )\n    }\n  })\n}\n\n# ==============================================================================\n# RUN APP\n# ==============================================================================\n\nshinyApp(ui = ui, server = server)\n\n\n\n\n\nScreenshots of the App with LLM Integration\n\n\n\n\n\n\nInitial App State\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAfter LLM Item Generation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPartial Item Re-generation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRe-generated Item\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDirect Item Editing",
    "crumbs": [
      "Activity: Shiny App Development"
    ]
  },
  {
    "objectID": "activity-shiny-app-development.html#view-the-app-code-generated-by-claude",
    "href": "activity-shiny-app-development.html#view-the-app-code-generated-by-claude",
    "title": "Activity: Shiny App Development",
    "section": "View the App Code Generated by Claude",
    "text": "View the App Code Generated by Claude\n\nCode# MCQ Generator Shiny App\n# Single-file app with field locking and AI-powered regeneration\n\nlibrary(shiny)\nlibrary(bslib)\nlibrary(httr2)\n\n# ==============================================================================\n# AI HELPER FUNCTIONS\n# ==============================================================================\n\n#' Generate a complete MCQ item via AI\n#'\n#' @param grade Grade level (e.g., \"1st\", \"3rd\", \"5th\", \"7th\")\n#' @param subject Subject (e.g., \"Geography\", \"Science\", \"Social Studies\")\n#' @return Named list with: question, optionA, optionB, optionC, optionD, optionE, correctAnswer\ngenerate_item_via_ai &lt;- function(grade, subject) {\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n  if (api_key == \"\") {\n    stop(\"ANTHROPIC_API_KEY not found in .Renviron\")\n  }\n  \n  system_prompt &lt;- \"You are an expert assessment item writer. Produce age-appropriate MCQs with exactly five options (A-E) and a single correct answer. Follow the output format strictly.\"\n  \n  user_prompt &lt;- sprintf(\n    \"Context: Grade = %s, Subject = %s\\n\\nGenerate a multiple-choice question suitable for this grade level and subject.\\n\\nOutput Format (required):\\nQuestion: &lt;text&gt;\\nOption A: &lt;text&gt;\\nOption B: &lt;text&gt;\\nOption C: &lt;text&gt;\\nOption D: &lt;text&gt;\\nOption E: &lt;text&gt;\\nCorrect Answer: Option &lt;A|B|C|D|E&gt;: &lt;repeat the correct option text verbatim&gt;\\n\\nEnsure the correct answer key and text match one of A-E. Keep language at the specified grade level.\",\n    grade, subject\n  )\n  \n  response &lt;- request(\"https://api.anthropic.com/v1/messages\") |&gt;\n    req_headers(\n      \"x-api-key\" = api_key,\n      \"anthropic-version\" = \"2023-06-01\",\n      \"content-type\" = \"application/json\"\n    ) |&gt;\n    req_body_json(list(\n      model = \"claude-sonnet-4-20250514\",\n      max_tokens = 1024,\n      system = system_prompt,\n      messages = list(\n        list(role = \"user\", content = user_prompt)\n      )\n    )) |&gt;\n    req_perform() |&gt;\n    resp_body_json()\n  \n  content_text &lt;- response$content[[1]]$text\n  return(parse_ai_output(content_text))\n}\n\n#' Re-generate an MCQ item via AI with locked fields\n#'\n#' @param grade Grade level\n#' @param subject Subject\n#' @param instructions Free-text instructions for regeneration\n#' @param locked_values Named list of locked fields (subset of: question, optionA-E, correctAnswer)\n#' @return Named list with all 7 fields (locked fields unchanged)\nregenerate_item_via_ai &lt;- function(grade, subject, instructions, locked_values) {\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n  if (api_key == \"\") {\n    stop(\"ANTHROPIC_API_KEY not found in .Renviron\")\n  }\n  \n  system_prompt &lt;- \"You are an expert assessment item writer. Produce age-appropriate MCQs with exactly five options (A-E) and a single correct answer. Follow the output format strictly.\"\n  \n  locked_fields_text &lt;- \"\"\n  if (length(locked_values) &gt; 0) {\n    locked_fields_text &lt;- \"\\n\\nLocked fields (echo these unchanged):\\n\"\n    for (field_name in names(locked_values)) {\n      locked_fields_text &lt;- paste0(locked_fields_text, field_name, \": \", locked_values[[field_name]], \"\\n\")\n    }\n  }\n  \n  instructions_text &lt;- \"\"\n  if (!is.null(instructions) && nchar(trimws(instructions)) &gt; 0) {\n    instructions_text &lt;- paste0(\"\\n\\nInstructions: \", instructions)\n  }\n  \n  user_prompt &lt;- sprintf(\n    \"Context: Grade = %s, Subject = %s%s%s\\n\\nRe-generate a multiple-choice question. Echo all locked fields unchanged. Generate new content for unlocked fields that is consistent with locked fields, grade, subject, and instructions.\\n\\nOutput Format (required):\\nQuestion: &lt;text&gt;\\nOption A: &lt;text&gt;\\nOption B: &lt;text&gt;\\nOption C: &lt;text&gt;\\nOption D: &lt;text&gt;\\nOption E: &lt;text&gt;\\nCorrect Answer: Option &lt;A|B|C|D|E&gt;: &lt;repeat the correct option text verbatim&gt;\\n\\nReturn all fields. Ensure the correct answer key and text match one of A-E. Keep language at the specified grade level.\",\n    grade, subject, locked_fields_text, instructions_text\n  )\n  \n  response &lt;- request(\"https://api.anthropic.com/v1/messages\") |&gt;\n    req_headers(\n      \"x-api-key\" = api_key,\n      \"anthropic-version\" = \"2023-06-01\",\n      \"content-type\" = \"application/json\"\n    ) |&gt;\n    req_body_json(list(\n      model = \"claude-sonnet-4-20250514\",\n      max_tokens = 1024,\n      system = system_prompt,\n      messages = list(\n        list(role = \"user\", content = user_prompt)\n      )\n    )) |&gt;\n    req_perform() |&gt;\n    resp_body_json()\n  \n  content_text &lt;- response$content[[1]]$text\n  parsed &lt;- parse_ai_output(content_text)\n  \n  # Validate locked fields\n  for (field_name in names(locked_values)) {\n    if (!identical(parsed[[field_name]], locked_values[[field_name]])) {\n      warning(paste(\"AI violated lock on field:\", field_name))\n      parsed[[field_name]] &lt;- locked_values[[field_name]]\n    }\n  }\n  \n  return(parsed)\n}\n\n#' Parse AI output text into structured format\nparse_ai_output &lt;- function(text) {\n  lines &lt;- strsplit(text, \"\\n\")[[1]]\n  lines &lt;- trimws(lines)\n  lines &lt;- lines[nchar(lines) &gt; 0]\n  \n  result &lt;- list()\n  \n  for (line in lines) {\n    if (grepl(\"^Question:\", line, ignore.case = TRUE)) {\n      result$question &lt;- trimws(sub(\"^Question:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option A:\", line, ignore.case = TRUE)) {\n      result$optionA &lt;- trimws(sub(\"^Option A:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option B:\", line, ignore.case = TRUE)) {\n      result$optionB &lt;- trimws(sub(\"^Option B:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option C:\", line, ignore.case = TRUE)) {\n      result$optionC &lt;- trimws(sub(\"^Option C:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option D:\", line, ignore.case = TRUE)) {\n      result$optionD &lt;- trimws(sub(\"^Option D:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Option E:\", line, ignore.case = TRUE)) {\n      result$optionE &lt;- trimws(sub(\"^Option E:\", \"\", line, ignore.case = TRUE))\n    } else if (grepl(\"^Correct Answer:\", line, ignore.case = TRUE)) {\n      result$correctAnswer &lt;- trimws(sub(\"^Correct Answer:\", \"\", line, ignore.case = TRUE))\n    }\n  }\n  \n  return(result)\n}\n\n# ==============================================================================\n# VALIDATION FUNCTIONS\n# ==============================================================================\n\nvalidate_item &lt;- function(item) {\n  errors &lt;- c()\n  \n  # Check question\n  if (is.null(item$question) || nchar(trimws(item$question)) == 0) {\n    errors &lt;- c(errors, \"Question cannot be empty\")\n  }\n  \n  # Check all options\n  required_options &lt;- c(\"optionA\", \"optionB\", \"optionC\", \"optionD\", \"optionE\")\n  for (opt in required_options) {\n    if (is.null(item[[opt]]) || nchar(trimws(item[[opt]])) == 0) {\n      errors &lt;- c(errors, paste(\"Option\", substr(opt, 7, 7), \"cannot be empty\"))\n    }\n  }\n  \n  # Check correct answer format\n  if (is.null(item$correctAnswer) || nchar(trimws(item$correctAnswer)) == 0) {\n    errors &lt;- c(errors, \"Correct Answer cannot be empty\")\n  } else {\n    # Extract key (A-E)\n    correct_key &lt;- extract_correct_key(item$correctAnswer)\n    if (is.na(correct_key)) {\n      errors &lt;- c(errors, \"Correct Answer must start with 'Option A:', 'Option B:', etc.\")\n    } else {\n      # Verify the text matches the option\n      option_field &lt;- paste0(\"option\", correct_key)\n      expected_text &lt;- item[[option_field]]\n      actual_text &lt;- extract_correct_text(item$correctAnswer)\n      \n      if (!identical(trimws(expected_text), trimws(actual_text))) {\n        errors &lt;- c(errors, paste0(\"Correct Answer text must match Option \", correct_key, \" exactly\"))\n      }\n    }\n  }\n  \n  if (length(errors) &gt; 0) {\n    return(list(valid = FALSE, errors = errors))\n  } else {\n    return(list(valid = TRUE, errors = NULL))\n  }\n}\n\nextract_correct_key &lt;- function(correct_answer) {\n  match &lt;- regexpr(\"Option ([A-E]):\", correct_answer, ignore.case = TRUE)\n  if (match &gt; 0) {\n    key_text &lt;- regmatches(correct_answer, match)\n    key &lt;- sub(\"Option ([A-E]):.*\", \"\\\\1\", key_text, ignore.case = TRUE)\n    return(toupper(key))\n  }\n  return(NA)\n}\n\nextract_correct_text &lt;- function(correct_answer) {\n  text &lt;- sub(\"^Option [A-E]:\\\\s*\", \"\", correct_answer, ignore.case = TRUE)\n  return(trimws(text))\n}\n\n# ==============================================================================\n# UI\n# ==============================================================================\n\nui &lt;- page_sidebar(\n  title = \"MCQ Generator\",\n  theme = bs_theme(version = 5, bootswatch = \"flatly\"),\n  \n  sidebar = sidebar(\n    width = 300,\n    h4(\"Configuration\"),\n    selectInput(\n      \"grade\",\n      \"Grade Level\",\n      choices = c(\"1st\", \"3rd\", \"5th\", \"7th\"),\n      selected = \"3rd\"\n    ),\n    selectInput(\n      \"subject\",\n      \"Subject\",\n      choices = c(\"Geography\", \"Science\", \"Social Studies\"),\n      selected = \"Science\"\n    ),\n    actionButton(\n      \"generate\",\n      \"Generate Item\",\n      icon = icon(\"wand-magic-sparkles\"),\n      class = \"btn-primary w-100 mb-3\"\n    ),\n    hr(),\n    h4(\"Re-generation\"),\n    textAreaInput(\n      \"regen_instructions\",\n      \"Re-generation Instructions (optional)\",\n      placeholder = \"e.g., focus the question on lava; make distractors conceptually distinct\",\n      rows = 3\n    ),\n    actionButton(\n      \"regen_unlocked\",\n      \"Re-generate Unlocked Fields\",\n      icon = icon(\"rotate\"),\n      class = \"btn-warning w-100\"\n    ),\n    helpText(\"Locked fields will remain unchanged. Unlocked fields will be re-generated.\")\n  ),\n  \n  # Main content\n  div(\n    class = \"container-fluid\",\n    h3(\"Edit Item\"),\n    helpText(\"Change the item as desired. Lock fields you want to preserve before re-generation.\"),\n    \n    # Question\n    div(\n      class = \"row mb-3\",\n      div(\n        class = \"col-md-10\",\n        textAreaInput(\n          \"question\",\n          \"Question\",\n          value = \"\",\n          rows = 3,\n          width = \"100%\"\n        )\n      ),\n      div(\n        class = \"col-md-2\",\n        br(),\n        checkboxInput(\"lock_question\", \"Lock\", FALSE)\n      )\n    ),\n    \n    # Options A-E\n    lapply(LETTERS[1:5], function(letter) {\n      div(\n        class = \"row mb-2\",\n        div(\n          class = \"col-md-10\",\n          textInput(\n            paste0(\"option\", letter),\n            paste(\"Option\", letter),\n            value = \"\",\n            width = \"100%\"\n          )\n        ),\n        div(\n          class = \"col-md-2\",\n          br(),\n          checkboxInput(paste0(\"lock_option\", letter), \"Lock\", FALSE)\n        )\n      )\n    }),\n    \n    # Correct Answer\n    div(\n      class = \"row mb-3\",\n      div(\n        class = \"col-md-10\",\n        textInput(\n          \"correctAnswer\",\n          \"Correct Answer (format: Option X: text)\",\n          value = \"\",\n          width = \"100%\",\n          placeholder = \"e.g., Option A: Pacific Ocean\"\n        )\n      ),\n      div(\n        class = \"col-md-2\",\n        br(),\n        checkboxInput(\"lock_correctAnswer\", \"Lock\", FALSE)\n      )\n    ),\n    \n    # Action buttons\n    div(\n      class = \"row mb-4\",\n      div(\n        class = \"col-md-6\",\n        actionButton(\n          \"update_item\",\n          \"Update Item\",\n          icon = icon(\"check\"),\n          class = \"btn-success w-100\"\n        )\n      ),\n      div(\n        class = \"col-md-6\",\n        actionButton(\n          \"save_item\",\n          \"Save Item\",\n          icon = icon(\"save\"),\n          class = \"btn-info w-100\"\n        )\n      )\n    ),\n    \n    hr(),\n    \n    # Feedback area\n    h3(\"Saved Items\"),\n    uiOutput(\"feedback_message\"),\n    tableOutput(\"saved_items_table\")\n  )\n)\n\n# ==============================================================================\n# SERVER\n# ==============================================================================\n\nserver &lt;- function(input, output, session) {\n  \n  # Reactive values for current item\n  current_item &lt;- reactiveValues(\n    grade = NULL,\n    subject = NULL,\n    question = \"\",\n    optionA = \"\",\n    optionB = \"\",\n    optionC = \"\",\n    optionD = \"\",\n    optionE = \"\",\n    correctAnswer = \"\",\n    timestamp = NULL\n  )\n  \n  # Saved items list\n  saved_items &lt;- reactiveVal(list())\n  \n  # Load saved items on startup\n  observe({\n    if (file.exists(\"saved_items.rds\")) {\n      tryCatch({\n        loaded &lt;- readRDS(\"saved_items.rds\")\n        saved_items(loaded)\n        showNotification(\n          paste(\"Loaded\", length(loaded), \"saved items\"),\n          type = \"message\"\n        )\n      }, error = function(e) {\n        showNotification(\n          paste(\"Error loading saved items:\", e$message),\n          type = \"error\"\n        )\n      })\n    }\n  })\n  \n  # Generate Item\n  observeEvent(input$generate, {\n    tryCatch({\n      showNotification(\"Generating item...\", type = \"message\", duration = 2)\n      \n      item &lt;- generate_item_via_ai(input$grade, input$subject)\n      \n      # Update UI fields\n      updateTextAreaInput(session, \"question\", value = item$question)\n      updateTextInput(session, \"optionA\", value = item$optionA)\n      updateTextInput(session, \"optionB\", value = item$optionB)\n      updateTextInput(session, \"optionC\", value = item$optionC)\n      updateTextInput(session, \"optionD\", value = item$optionD)\n      updateTextInput(session, \"optionE\", value = item$optionE)\n      updateTextInput(session, \"correctAnswer\", value = item$correctAnswer)\n      \n      # Update current_item\n      current_item$grade &lt;- input$grade\n      current_item$subject &lt;- input$subject\n      current_item$question &lt;- item$question\n      current_item$optionA &lt;- item$optionA\n      current_item$optionB &lt;- item$optionB\n      current_item$optionC &lt;- item$optionC\n      current_item$optionD &lt;- item$optionD\n      current_item$optionE &lt;- item$optionE\n      current_item$correctAnswer &lt;- item$correctAnswer\n      current_item$timestamp &lt;- Sys.time()\n      \n      showNotification(\"Item generated successfully!\", type = \"message\")\n    }, error = function(e) {\n      showNotification(\n        paste(\"Error generating item:\", e$message),\n        type = \"error\",\n        duration = 5\n      )\n    })\n  })\n  \n  # Re-generate Unlocked Fields\n  observeEvent(input$regen_unlocked, {\n    tryCatch({\n      showNotification(\"Re-generating unlocked fields...\", type = \"message\", duration = 2)\n      \n      # Build locked_values from checkbox states\n      locked_values &lt;- list()\n      \n      if (input$lock_question) {\n        locked_values$question &lt;- input$question\n      }\n      if (input$lock_optionA) {\n        locked_values$optionA &lt;- input$optionA\n      }\n      if (input$lock_optionB) {\n        locked_values$optionB &lt;- input$optionB\n      }\n      if (input$lock_optionC) {\n        locked_values$optionC &lt;- input$optionC\n      }\n      if (input$lock_optionD) {\n        locked_values$optionD &lt;- input$optionD\n      }\n      if (input$lock_optionE) {\n        locked_values$optionE &lt;- input$optionE\n      }\n      if (input$lock_correctAnswer) {\n        locked_values$correctAnswer &lt;- input$correctAnswer\n      }\n      \n      # Call regeneration\n      item &lt;- regenerate_item_via_ai(\n        input$grade,\n        input$subject,\n        input$regen_instructions,\n        locked_values\n      )\n      \n      # Validate that locked fields were respected\n      violated_locks &lt;- c()\n      for (field in names(locked_values)) {\n        if (!identical(trimws(item[[field]]), trimws(locked_values[[field]]))) {\n          violated_locks &lt;- c(violated_locks, field)\n          # Restore locked value\n          item[[field]] &lt;- locked_values[[field]]\n        }\n      }\n      \n      if (length(violated_locks) &gt; 0) {\n        showNotification(\n          paste(\"Warning: AI violated locks on:\", paste(violated_locks, collapse = \", \"), \". Values restored.\"),\n          type = \"warning\",\n          duration = 5\n        )\n      }\n      \n      # Update UI fields\n      updateTextAreaInput(session, \"question\", value = item$question)\n      updateTextInput(session, \"optionA\", value = item$optionA)\n      updateTextInput(session, \"optionB\", value = item$optionB)\n      updateTextInput(session, \"optionC\", value = item$optionC)\n      updateTextInput(session, \"optionD\", value = item$optionD)\n      updateTextInput(session, \"optionE\", value = item$optionE)\n      updateTextInput(session, \"correctAnswer\", value = item$correctAnswer)\n      \n      # Update current_item\n      current_item$grade &lt;- input$grade\n      current_item$subject &lt;- input$subject\n      current_item$question &lt;- item$question\n      current_item$optionA &lt;- item$optionA\n      current_item$optionB &lt;- item$optionB\n      current_item$optionC &lt;- item$optionC\n      current_item$optionD &lt;- item$optionD\n      current_item$optionE &lt;- item$optionE\n      current_item$correctAnswer &lt;- item$correctAnswer\n      current_item$timestamp &lt;- Sys.time()\n      \n      showNotification(\"Item re-generated successfully!\", type = \"message\")\n    }, error = function(e) {\n      showNotification(\n        paste(\"Error re-generating item:\", e$message),\n        type = \"error\",\n        duration = 5\n      )\n    })\n  })\n  \n  # Update Item\n  observeEvent(input$update_item, {\n    item &lt;- list(\n      question = input$question,\n      optionA = input$optionA,\n      optionB = input$optionB,\n      optionC = input$optionC,\n      optionD = input$optionD,\n      optionE = input$optionE,\n      correctAnswer = input$correctAnswer\n    )\n    \n    validation &lt;- validate_item(item)\n    \n    if (!validation$valid) {\n      showNotification(\n        paste(\"Validation errors:\", paste(validation$errors, collapse = \"; \")),\n        type = \"error\",\n        duration = 5\n      )\n    } else {\n      current_item$grade &lt;- input$grade\n      current_item$subject &lt;- input$subject\n      current_item$question &lt;- item$question\n      current_item$optionA &lt;- item$optionA\n      current_item$optionB &lt;- item$optionB\n      current_item$optionC &lt;- item$optionC\n      current_item$optionD &lt;- item$optionD\n      current_item$optionE &lt;- item$optionE\n      current_item$correctAnswer &lt;- item$correctAnswer\n      current_item$timestamp &lt;- Sys.time()\n      \n      showNotification(\"Item updated successfully!\", type = \"message\")\n    }\n  })\n  \n  # Save Item\n  observeEvent(input$save_item, {\n    item &lt;- list(\n      grade = input$grade,\n      subject = input$subject,\n      question = input$question,\n      optionA = input$optionA,\n      optionB = input$optionB,\n      optionC = input$optionC,\n      optionD = input$optionD,\n      optionE = input$optionE,\n      correctAnswer = input$correctAnswer\n    )\n    \n    validation &lt;- validate_item(item)\n    \n    if (!validation$valid) {\n      showNotification(\n        paste(\"Cannot save. Validation errors:\", paste(validation$errors, collapse = \"; \")),\n        type = \"error\",\n        duration = 5\n      )\n    } else {\n      # Add timestamp\n      item$timestamp &lt;- Sys.time()\n      \n      # Append to saved items\n      current_saved &lt;- saved_items()\n      current_saved[[length(current_saved) + 1]] &lt;- item\n      saved_items(current_saved)\n      \n      # Save to file\n      tryCatch({\n        saveRDS(current_saved, \"saved_items.rds\")\n        showNotification(\n          paste(\"Item saved successfully! Total items:\", length(current_saved)),\n          type = \"message\",\n          duration = 3\n        )\n      }, error = function(e) {\n        showNotification(\n          paste(\"Error saving to file:\", e$message),\n          type = \"error\",\n          duration = 5\n        )\n      })\n    }\n  })\n  \n  # Render saved items table\n  output$saved_items_table &lt;- renderTable({\n    items &lt;- saved_items()\n    \n    if (length(items) == 0) {\n      return(data.frame(\n        Message = \"No items saved yet\"\n      ))\n    }\n    \n    # Create summary table\n    summary_df &lt;- do.call(rbind, lapply(seq_along(items), function(i) {\n      item &lt;- items[[i]]\n      data.frame(\n        Index = i,\n        Grade = item$grade,\n        Subject = item$subject,\n        Question = substr(item$question, 1, 60),\n        Timestamp = as.character(item$timestamp),\n        stringsAsFactors = FALSE\n      )\n    }))\n    \n    summary_df\n  }, striped = TRUE, hover = TRUE, bordered = TRUE)\n  \n  # Feedback message\n  output$feedback_message &lt;- renderUI({\n    items &lt;- saved_items()\n    if (length(items) &gt; 0) {\n      div(\n        class = \"alert alert-info\",\n        icon(\"circle-info\"),\n        paste(\"Total saved items:\", length(items))\n      )\n    }\n  })\n}\n\n# ==============================================================================\n# RUN APP\n# ==============================================================================\n\nshinyApp(ui = ui, server = server)\n\n\n:::\nScreenshots of the App with LLM Integration\nInitial state of the app.\n\n\n\n\nAfter an LLM has been used to generate an item.\n\n\n\n\nI’ve locked some of the response options and asked for new distractors. Because I didn’t lock the item stem, model should then generate a new question with the same correct response, as well as new distractors.\n Result after the re-generation. The item stem and unlocked distractors have been updated.\n Example of directly editing the response options. This feature is extremely helpful when SMEs or item editors are making changes to the generated item.",
    "crumbs": [
      "Activity: Shiny App Development"
    ]
  },
  {
    "objectID": "activity-shiny-app-development.html#build-your-own-app",
    "href": "activity-shiny-app-development.html#build-your-own-app",
    "title": "Activity: Shiny App Development",
    "section": "Build your own app!",
    "text": "Build your own app!\nNow it’s time for you to build a Shiny app!",
    "crumbs": [
      "Activity: Shiny App Development"
    ]
  },
  {
    "objectID": "activity-shiny-app-development.html#data-visualization-with-existing-data",
    "href": "activity-shiny-app-development.html#data-visualization-with-existing-data",
    "title": "Activity: Shiny App Development",
    "section": "Prompt: Build a Shiny App to Explore the ChickWeight Dataset (no APIs)",
    "text": "Initial Prompt to Generate a Prompt\n\n\n\n\n\nHelp me generate a high-quality prompt that I can supply to an LLM to generate a Shiny app to evaluate the ChickWeight dataset. Have it include some interesting layout options, such as a sidebar or tabset. Also have some interesting pop-up displays of some sort - so when the user hovers over something (a graph, a selection box, etc) a pop-up is displayed that provides more context or provides instructions.\n\n\n\n\n\n\n\n\n\nPrompt Generated by ChatGPT5\n\n\n\n\nPrompt: Build a Shiny App to Explore the ChickWeight Dataset (no APIs)\nRole: You are an expert R Shiny developer.\nTask: Create a complete, runnable single-file Shiny app (app.R) that explores the base R datasets::ChickWeight dataset. The app must include: (1) a sidebar layout with global filters, (2) a tabset in the main area for multiple linked views, and (3) hover pop-ups / tooltips that provide instructions or context when users hover over inputs or UI elements.No external APIs or LLMs—everything is local to R.\n\nData\nUse datasets::ChickWeight as the only data source. Variables:\n- weight (numeric), Time (days), Chick (ID), Diet (1–4).\nPackages (keep dependencies modest)\n\nRequired: shiny, ggplot2, dplyr, tidyr, DT, bslib\n\nOptional (strongly preferred for interactivity): plotly for hover tooltips on charts\n\nDo not use packages that need system libraries; keep it CRAN-friendly.\n\nUI & Layout\nUse bslib for a clean Bootstrap-5 theme (e.g., bslib::bs_theme(preset = \"minty\")).\nOverall structure: shiny::fluidPage() → shiny::sidebarLayout()\n- Sidebar panel (global filters): - Diet filter: checkboxGroupInput for Diet 1–4 (default: all) - Add a hover tooltip explaining diet codes (use bslib::tooltip() or bslib::popover() on the input label or an adjacent info icon). - Time range: sliderInput for days (0–21) - Add a tooltip: “Drag to focus growth interval; all summaries respect this window.” - Chick highlight: selectizeInput allowing multiple IDs to highlight (default: none) - Tooltip: “Type to find a Chick ID; selected IDs are emphasized in plots and table.” - Smoothing: checkboxInput to add LOESS mean curve per Diet (and optional CI ribbon) - Tooltip: “Show smoothed average by Diet; use with caution for small n.” - Scale: radioButtons for y-axis (“Linear”, “Log10”) - Tooltip: short rationale for log-scale. - Reset filters: actionButton(\"reset\", \"Reset Filters\")\n\n\nMain panel (tabset) Use tabsetPanel(id = \"tabs\", type = \"pills\") with the following tabs:\n\n\nOverview\n\nA valueBox-like summary row (simple cards) showing:\n\nNumber of visible chicks, average terminal weight (within time window), and number of timepoints.\n\n\nA short paragraph (Markdown) describing the dataset and how filters apply.\nA popover (hover info icon) that explains the study design and why repeated measures matter.\n\n\n\nGrowth Curves\n\n\nPrimary plot: weight vs. time lines, one line per Chick, colored by Diet.\nOptional mean LOESS curve per Diet + semi-transparent ribbon (if smoothing is on).\n\nHighlight selected Chick IDs (thicker line/alpha=1).\nIf plotly is available, convert ggplot to ggplotly() so that hover shows:\n\nChick, Diet, Time, Weight, and (if computed) per-Chick slope for current window.\n\n\nAdd a small info icon near the tab title with a bslib::popover() explaining how to read growth curves.\n\n\n\nDiet Comparison\n\nSide-by-side boxplots/violins of final weight (within the chosen time window) by Diet.\nA secondary bar chart ranking Diets by mean terminal weight (or mean Δweight per Chick).\nHover/tooltip on the plot area describing how “final weight” is derived (max Time within window).\n\n\n\nChick Table\n\nA DT::datatable listing only the currently visible chicks (after filters), with columns:\n\nChick, Diet, n_timepoints (within window), first_weight, last_weight, Δweight, and slope (simple lm(weight ~ Time) over the window).\n\n\n\nRow selection → highlights the corresponding Chick line in the Growth Curves tab (reactively).\nAdd a download button to export this table as CSV (no server storage).\nTooltip on the table header: “Click column headers to sort; use search to filter rows.”\n\n\n\nAbout / Help\n\nConcise prose on interpretation, caveats (missing values, small samples), and how smoothing/log scale change the view.\nA bslib::popover() on “Caveats” that appears on hover for a few seconds.\n\n\n\n\n\nServer Requirements (high-level behavior)\n\nCreate a single reactive filtered dataset based on Diet and Time range. All downstream outputs depend on this.\n\nLinked reactivity:\n\nChanging filters updates all tabs’ outputs.\nSelecting/changing Chick IDs highlights them in plots and the table.\nSelecting rows in the table highlights those lines in the Growth Curves plot.\n\n\n\nDerived metrics (computed on the filtered data):\n\nPer-Chick Δweight = (max weight − min weight) for times within the window.\nPer-Chick slope from lm(weight ~ Time) within the window; display as g/day.\nPer-Diet summaries (mean terminal weight, median Δweight).\n\n\n\nValidation/empty states:\n\nIf filters remove all data, show a friendly empty-state message instead of blank plots/tables.\n\n\n\nScale handling:\n\nIf log scale is selected, transform y-axis appropriately and suppress non-positive weights (with a note).\n\n\n\nReset button restores default filters.\n\nHover Pop-ups / Tooltips (must-have)\nUse bslib::tooltip() and/or bslib::popover() to attach hover help to: - Diet filter label (explain diet codes 1–4) - Time range slider label (explain windows) - Chick select label (how highlighting works) - Growth Curves tab title or an adjacent info icon (how to interpret lines and smoothing) - Diet Comparison plot container (how “final weight” is determined) - Table header (sorting/search tips)\nIf plotly is used, ensure hovertext on the graph shows: Chick, Diet, Time, Weight, and Slope (if available).\nQuality & Style\n\nClean, minimal design using bslib; sensible spacing and headings.\nClear code comments explaining each reactive object and output.\nNo external services or files; everything runs locally with shiny::runApp().\nKeep code in one file app.R.\n\nDeliverable\nOutput only a complete app.R in a single code block, ready to run.\nAcceptance Criteria\n\nApp launches with sidebar + tabset and all tabs functioning.\nFilters, highlights, and tabs are fully reactive and consistent.\nHover pop-ups/tooltips appear on specified inputs/areas using bslib.\nGrowth Curves and Diet Comparison are informative and respect filters/smoothing/scale.\nTable reflects the filtered/visible cohort and supports row-selection highlighting.\nCSV download works for the current table view.\n\n\n\n\n\n\n\n\n\nApp Syntax Generated by Claude\n\n\n\n\n\nYou can either download the app below, or copy and paste the chunk of code in a new .R file. Then highlight and execute the text. It may take a minute or two the first time the app runs for it to be built, but a new window should open and the app should run.\n 📥 Download ChickWeight_app \n\nCode# ==============================================================================\n# Shiny App: ChickWeight Explorer\n# A single-file app exploring the datasets::ChickWeight dataset\n# ==============================================================================\n\n# Load required packages\nlibrary(shiny)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(DT)\nlibrary(bslib)\nlibrary(plotly)  # For interactive hover tooltips\n\n# Load the data\ndata(\"ChickWeight\", package = \"datasets\")\n\n# ==============================================================================\n# UI\n# ==============================================================================\n\nui &lt;- fluidPage(\n  theme = bs_theme(preset = \"minty\", version = 5),\n  \n  titlePanel(\"ChickWeight Dataset Explorer\"),\n  \n  sidebarLayout(\n    # --------------------------------------------------------------------------\n    # SIDEBAR: Global Filters with Tooltips\n    # --------------------------------------------------------------------------\n    sidebarPanel(\n      width = 3,\n      \n      h4(\"Filters\"),\n      \n      # Diet filter with tooltip\n      div(\n        style = \"display: flex; align-items: center; gap: 5px;\",\n        tags$label(\"Diet\", `for` = \"diet_filter\"),\n        bslib::tooltip(\n          icon(\"circle-info\"),\n          \"Diet types 1–4 represent different protein supplements. Select which diets to include in the analysis.\",\n          placement = \"right\"\n        )\n      ),\n      checkboxGroupInput(\n        \"diet_filter\",\n        label = NULL,\n        choices = 1:4,\n        selected = 1:4,\n        inline = TRUE\n      ),\n      \n      # Time range slider with tooltip\n      div(\n        style = \"display: flex; align-items: center; gap: 5px; margin-top: 15px;\",\n        tags$label(\"Time Range (days)\", `for` = \"time_range\"),\n        bslib::tooltip(\n          icon(\"circle-info\"),\n          \"Drag to focus on a specific growth interval. All summaries and plots respect this time window.\",\n          placement = \"right\"\n        )\n      ),\n      sliderInput(\n        \"time_range\",\n        label = NULL,\n        min = 0,\n        max = 21,\n        value = c(0, 21),\n        step = 1\n      ),\n      \n      # Chick highlight with tooltip\n      div(\n        style = \"display: flex; align-items: center; gap: 5px; margin-top: 15px;\",\n        tags$label(\"Highlight Chicks\", `for` = \"chick_highlight\"),\n        bslib::tooltip(\n          icon(\"circle-info\"),\n          \"Type to find a Chick ID. Selected chicks are emphasized in plots and highlighted in the table.\",\n          placement = \"right\"\n        )\n      ),\n      selectizeInput(\n        \"chick_highlight\",\n        label = NULL,\n        choices = unique(ChickWeight$Chick),\n        selected = NULL,\n        multiple = TRUE,\n        options = list(placeholder = \"Select chick(s)...\")\n      ),\n      \n      # Smoothing option with tooltip\n      div(\n        style = \"margin-top: 15px;\",\n        checkboxInput(\n          \"smoothing\",\n          label = div(\n            \"Add LOESS smoothing\",\n            bslib::tooltip(\n              icon(\"circle-info\"),\n              \"Show smoothed average trend by Diet with confidence ribbon. Use with caution for small sample sizes.\",\n              placement = \"right\"\n            )\n          ),\n          value = FALSE\n        )\n      ),\n      \n      # Scale option with tooltip\n      div(\n        style = \"margin-top: 15px;\",\n        tags$label(\"Y-axis Scale\"),\n        bslib::tooltip(\n          icon(\"circle-info\"),\n          \"Log scale can reveal proportional growth patterns and reduce the influence of outliers.\",\n          placement = \"right\"\n        ),\n        radioButtons(\n          \"scale\",\n          label = NULL,\n          choices = c(\"Linear\", \"Log10\"),\n          selected = \"Linear\"\n        )\n      ),\n      \n      # Reset button\n      actionButton(\n        \"reset\",\n        \"Reset Filters\",\n        class = \"btn-warning\",\n        style = \"margin-top: 20px; width: 100%;\"\n      )\n    ),\n    \n    # --------------------------------------------------------------------------\n    # MAIN PANEL: Tabset\n    # --------------------------------------------------------------------------\n    mainPanel(\n      width = 9,\n      \n      tabsetPanel(\n        id = \"tabs\",\n        type = \"pills\",\n        \n        # ---- Tab 1: Overview ----\n        tabPanel(\n          \"Overview\",\n          br(),\n          fluidRow(\n            column(4, uiOutput(\"summary_chicks\")),\n            column(4, uiOutput(\"summary_weight\")),\n            column(4, uiOutput(\"summary_timepoints\"))\n          ),\n          hr(),\n          div(\n            h4(\n              \"About the Dataset\",\n              bslib::popover(\n                icon(\"circle-info\"),\n                title = \"Study Design\",\n                \"This is a repeated-measures experiment tracking chick growth over time under four different diet regimens. Each chick is measured at regular intervals (days 0, 2, 4, ..., 21).\",\n                placement = \"right\"\n              )\n            ),\n            p(\"The ChickWeight dataset contains measurements from an experiment on the effect of diet on early growth of chicks. \n              The body weights of chicks were measured at birth and every second day thereafter until day 21. \n              Four different protein diets were tested on different groups of newly hatched chicks.\"),\n            p(strong(\"Variables:\"), \"Weight (grams), Time (days), Chick (ID), Diet (1–4)\"),\n            p(strong(\"Sample size:\"), \"50 chicks total across 4 diet groups\"),\n            p(\"Use the sidebar filters to focus on specific diets, time windows, or individual chicks. \n              All visualizations and summaries automatically update based on your selections.\")\n          )\n        ),\n        \n        # ---- Tab 2: Growth Curves ----\n        tabPanel(\n          div(\n            \"Growth Curves\",\n            bslib::tooltip(\n              icon(\"circle-info\"),\n              \"Each line represents one chick's growth trajectory. Hover over lines to see details. Use the smoothing option to view average trends by diet.\",\n              placement = \"top\"\n            )\n          ),\n          br(),\n          uiOutput(\"growth_empty_state\"),\n          plotlyOutput(\"growth_plot\", height = \"600px\")\n        ),\n        \n        # ---- Tab 3: Diet Comparison ----\n        tabPanel(\n          div(\n            \"Diet Comparison\",\n            bslib::tooltip(\n              icon(\"circle-info\"),\n              \"Final weight is defined as the maximum weight observed within the selected time window for each chick.\",\n              placement = \"top\"\n            )\n          ),\n          br(),\n          uiOutput(\"diet_empty_state\"),\n          fluidRow(\n            column(6, plotlyOutput(\"diet_boxplot\", height = \"400px\")),\n            column(6, plotlyOutput(\"diet_barplot\", height = \"400px\"))\n          )\n        ),\n        \n        # ---- Tab 4: Chick Table ----\n        tabPanel(\n          div(\n            \"Chick Table\",\n            bslib::tooltip(\n              icon(\"circle-info\"),\n              \"Click column headers to sort. Use the search box to filter rows. Select rows to highlight chicks in the Growth Curves tab.\",\n              placement = \"top\"\n            )\n          ),\n          br(),\n          downloadButton(\"download_table\", \"Download CSV\", class = \"btn-primary\"),\n          br(), br(),\n          DTOutput(\"chick_table\")\n        ),\n        \n        # ---- Tab 5: About / Help ----\n        tabPanel(\n          \"About / Help\",\n          br(),\n          h4(\"Interpretation Guide\"),\n          p(\"This app provides multiple perspectives on chick growth patterns:\"),\n          tags$ul(\n            tags$li(strong(\"Growth Curves:\"), \"Show individual trajectories. Look for parallel growth patterns, \n                    divergence over time, or individual outliers.\"),\n            tags$li(strong(\"Diet Comparison:\"), \"Summarizes final outcomes. Boxplots reveal distribution shape and outliers; \n                    bar charts show average performance.\"),\n            tags$li(strong(\"Chick Table:\"), \"Provides quantitative metrics including growth rate (slope) and total weight gain (Δweight).\")\n          ),\n          \n          hr(),\n          h4(\n            \"Caveats\",\n            bslib::popover(\n              icon(\"triangle-exclamation\"),\n              title = \"Important Considerations\",\n              HTML(\"&lt;ul&gt;\n                &lt;li&gt;Some chicks have missing measurements at later timepoints&lt;/li&gt;\n                &lt;li&gt;Sample sizes vary by diet group&lt;/li&gt;\n                &lt;li&gt;LOESS smoothing can be misleading with small sample sizes&lt;/li&gt;\n                &lt;li&gt;Log scale is only appropriate for positive weights&lt;/li&gt;\n                &lt;/ul&gt;\"),\n              placement = \"right\"\n            )\n          ),\n          tags$ul(\n            tags$li(strong(\"Missing data:\"), \"Not all chicks were measured at all timepoints. \n                    The 'final weight' is the maximum observed within your selected time window.\"),\n            tags$li(strong(\"Small samples:\"), \"Some diet groups have fewer chicks. Be cautious when comparing groups of very different sizes.\"),\n            tags$li(strong(\"Smoothing caveats:\"), \"LOESS smoothing assumes smooth, continuous growth. \n                    It may overfit with small samples or hide important individual variation.\"),\n            tags$li(strong(\"Log scale:\"), \"Useful for comparing proportional growth rates across different starting weights. \n                    However, it can make small absolute differences appear large.\")\n          ),\n          \n          hr(),\n          h4(\"Tips\"),\n          tags$ul(\n            tags$li(\"Start with all diets selected to get the full picture, then narrow down.\"),\n            tags$li(\"Use the time slider to focus on specific growth phases (e.g., days 10–21 for late growth).\"),\n            tags$li(\"Highlight specific chicks to track individuals across multiple views.\"),\n            tags$li(\"Row selection in the table automatically highlights those chicks in the growth curves plot.\")\n          )\n        )\n      )\n    )\n  )\n)\n\n# ==============================================================================\n# SERVER\n# ==============================================================================\n\nserver &lt;- function(input, output, session) {\n  \n  # ----------------------------------------------------------------------------\n  # Reactive: Filtered Dataset\n  # ----------------------------------------------------------------------------\n  filtered_data &lt;- reactive({\n    req(input$diet_filter, input$time_range)\n    \n    ChickWeight %&gt;%\n      filter(\n        Diet %in% input$diet_filter,\n        Time &gt;= input$time_range[1],\n        Time &lt;= input$time_range[2]\n      )\n  })\n  \n  # ----------------------------------------------------------------------------\n  # Reactive: Highlighted Chicks (from sidebar OR table selection)\n  # ----------------------------------------------------------------------------\n  highlighted_chicks &lt;- reactive({\n    # Combine sidebar selection and table row selection\n    sidebar_selected &lt;- input$chick_highlight\n    table_selected &lt;- if (!is.null(input$chick_table_rows_selected)) {\n      summary_table()[input$chick_table_rows_selected, ]$Chick\n    } else {\n      NULL\n    }\n    unique(c(sidebar_selected, as.character(table_selected)))\n  })\n  \n  # ----------------------------------------------------------------------------\n  # Reactive: Per-Chick Summary Metrics\n  # ----------------------------------------------------------------------------\n  summary_table &lt;- reactive({\n    req(nrow(filtered_data()) &gt; 0)\n    \n    filtered_data() %&gt;%\n      group_by(Chick, Diet) %&gt;%\n      summarise(\n        n_timepoints = n(),\n        first_weight = min(weight, na.rm = TRUE),\n        last_weight = max(weight, na.rm = TRUE),\n        delta_weight = last_weight - first_weight,\n        slope = if (n() &gt; 1) {\n          coef(lm(weight ~ Time))[2]\n        } else {\n          NA_real_\n        },\n        .groups = \"drop\"\n      ) %&gt;%\n      mutate(\n        slope = round(slope, 2),\n        delta_weight = round(delta_weight, 1),\n        first_weight = round(first_weight, 1),\n        last_weight = round(last_weight, 1)\n      )\n  })\n  \n  # ----------------------------------------------------------------------------\n  # Observer: Reset Filters\n  # ----------------------------------------------------------------------------\n  observeEvent(input$reset, {\n    updateCheckboxGroupInput(session, \"diet_filter\", selected = 1:4)\n    updateSliderInput(session, \"time_range\", value = c(0, 21))\n    updateSelectizeInput(session, \"chick_highlight\", selected = character(0))\n    updateCheckboxInput(session, \"smoothing\", value = FALSE)\n    updateRadioButtons(session, \"scale\", selected = \"Linear\")\n  })\n  \n  # ----------------------------------------------------------------------------\n  # TAB 1: Overview - Summary Cards\n  # ----------------------------------------------------------------------------\n  output$summary_chicks &lt;- renderUI({\n    n_chicks &lt;- filtered_data() %&gt;% pull(Chick) %&gt;% unique() %&gt;% length()\n    div(\n      class = \"card text-center\",\n      style = \"background-color: #e3f2fd; padding: 20px;\",\n      h3(n_chicks, style = \"color: #1976d2; margin: 0;\"),\n      p(\"Visible Chicks\", style = \"margin: 5px 0 0 0;\")\n    )\n  })\n  \n  output$summary_weight &lt;- renderUI({\n    avg_terminal &lt;- filtered_data() %&gt;%\n      group_by(Chick) %&gt;%\n      summarise(terminal = max(weight, na.rm = TRUE)) %&gt;%\n      pull(terminal) %&gt;%\n      mean(na.rm = TRUE) %&gt;%\n      round(1)\n    \n    div(\n      class = \"card text-center\",\n      style = \"background-color: #f3e5f5; padding: 20px;\",\n      h3(paste0(avg_terminal, \" g\"), style = \"color: #7b1fa2; margin: 0;\"),\n      p(\"Avg Terminal Weight\", style = \"margin: 5px 0 0 0;\")\n    )\n  })\n  \n  output$summary_timepoints &lt;- renderUI({\n    n_points &lt;- filtered_data() %&gt;% pull(Time) %&gt;% unique() %&gt;% length()\n    div(\n      class = \"card text-center\",\n      style = \"background-color: #e8f5e9; padding: 20px;\",\n      h3(n_points, style = \"color: #388e3c; margin: 0;\"),\n      p(\"Timepoints in Window\", style = \"margin: 5px 0 0 0;\")\n    )\n  })\n  \n  # ----------------------------------------------------------------------------\n  # TAB 2: Growth Curves Plot\n  # ----------------------------------------------------------------------------\n  output$growth_empty_state &lt;- renderUI({\n    if (nrow(filtered_data()) == 0) {\n      div(\n        class = \"alert alert-warning\",\n        icon(\"triangle-exclamation\"),\n        \" No data available with current filter settings. Try adjusting the filters.\"\n      )\n    }\n  })\n  \n  output$growth_plot &lt;- renderPlotly({\n    req(nrow(filtered_data()) &gt; 0)\n    \n    # Prepare data with highlighting\n    plot_data &lt;- filtered_data() %&gt;%\n      mutate(\n        highlight = if (length(highlighted_chicks()) &gt; 0) {\n          Chick %in% highlighted_chicks()\n        } else {\n          FALSE\n        },\n        alpha_val = ifelse(highlight, 1, 0.4),\n        size_val = ifelse(highlight, 1.2, 0.6)\n      )\n    \n    # Add slope to hover text\n    slope_data &lt;- summary_table() %&gt;%\n      select(Chick, slope)\n    \n    plot_data &lt;- plot_data %&gt;%\n      left_join(slope_data, by = \"Chick\") %&gt;%\n      mutate(\n        hover_text = paste0(\n          \"Chick: \", Chick, \"&lt;br&gt;\",\n          \"Diet: \", Diet, \"&lt;br&gt;\",\n          \"Time: \", Time, \" days&lt;br&gt;\",\n          \"Weight: \", round(weight, 1), \" g&lt;br&gt;\",\n          \"Slope: \", ifelse(!is.na(slope), paste0(slope, \" g/day\"), \"N/A\")\n        )\n      )\n    \n    # Base plot\n    p &lt;- ggplot(plot_data, aes(x = Time, y = weight, group = Chick, color = factor(Diet))) +\n      geom_line(aes(alpha = alpha_val, size = size_val, text = hover_text)) +\n      scale_alpha_identity() +\n      scale_size_identity() +\n      scale_color_brewer(palette = \"Set1\", name = \"Diet\") +\n      labs(\n        title = \"Chick Growth Trajectories\",\n        x = \"Time (days)\",\n        y = \"Weight (grams)\"\n      ) +\n      theme_minimal(base_size = 14) +\n      theme(legend.position = \"right\")\n    \n    # Add LOESS smoothing if requested\n    if (input$smoothing) {\n      p &lt;- p + geom_smooth(\n        aes(group = Diet, color = factor(Diet)),\n        method = \"loess\",\n        se = TRUE,\n        alpha = 0.2,\n        linewidth = 1.5,\n        inherit.aes = FALSE,\n        data = filtered_data() %&gt;% mutate(Diet = factor(Diet))\n      )\n    }\n    \n    # Apply scale transformation\n    if (input$scale == \"Log10\") {\n      # Filter out non-positive weights for log scale\n      plot_data_log &lt;- plot_data %&gt;% filter(weight &gt; 0)\n      if (nrow(plot_data_log) &lt; nrow(plot_data)) {\n        showNotification(\"Some weights ≤ 0 were excluded for log scale.\", type = \"warning\")\n      }\n      p &lt;- p + scale_y_log10()\n    }\n    \n    # Convert to plotly for interactivity\n    ggplotly(p, tooltip = \"text\") %&gt;%\n      layout(hovermode = \"closest\")\n  })\n  \n  # ----------------------------------------------------------------------------\n  # TAB 3: Diet Comparison\n  # ----------------------------------------------------------------------------\n  output$diet_empty_state &lt;- renderUI({\n    if (nrow(filtered_data()) == 0) {\n      div(\n        class = \"alert alert-warning\",\n        icon(\"triangle-exclamation\"),\n        \" No data available with current filter settings. Try adjusting the filters.\"\n      )\n    }\n  })\n  \n  output$diet_boxplot &lt;- renderPlotly({\n    req(nrow(filtered_data()) &gt; 0)\n    \n    # Get final weight per chick\n    final_weights &lt;- filtered_data() %&gt;%\n      group_by(Chick, Diet) %&gt;%\n      summarise(final_weight = max(weight, na.rm = TRUE), .groups = \"drop\")\n    \n    p &lt;- ggplot(final_weights, aes(x = factor(Diet), y = final_weight, fill = factor(Diet))) +\n      geom_violin(alpha = 0.6) +\n      geom_boxplot(width = 0.2, alpha = 0.8, outlier.shape = NA) +\n      geom_jitter(width = 0.1, alpha = 0.4, size = 2) +\n      scale_fill_brewer(palette = \"Set1\", name = \"Diet\") +\n      labs(\n        title = \"Final Weight Distribution by Diet\",\n        x = \"Diet\",\n        y = \"Final Weight (grams)\"\n      ) +\n      theme_minimal(base_size = 14) +\n      theme(legend.position = \"none\")\n    \n    ggplotly(p) %&gt;%\n      layout(hovermode = \"closest\")\n  })\n  \n  output$diet_barplot &lt;- renderPlotly({\n    req(nrow(filtered_data()) &gt; 0)\n    \n    # Mean delta weight by diet\n    diet_summary &lt;- summary_table() %&gt;%\n      group_by(Diet) %&gt;%\n      summarise(\n        mean_delta = mean(delta_weight, na.rm = TRUE),\n        .groups = \"drop\"\n      ) %&gt;%\n      arrange(desc(mean_delta))\n    \n    p &lt;- ggplot(diet_summary, aes(x = reorder(factor(Diet), mean_delta), y = mean_delta, fill = factor(Diet))) +\n      geom_col(alpha = 0.8) +\n      geom_text(aes(label = round(mean_delta, 1)), vjust = -0.5, size = 4) +\n      scale_fill_brewer(palette = \"Set1\", name = \"Diet\") +\n      labs(\n        title = \"Mean Weight Gain by Diet\",\n        x = \"Diet (ranked)\",\n        y = \"Mean Δ Weight (grams)\"\n      ) +\n      theme_minimal(base_size = 14) +\n      theme(legend.position = \"none\")\n    \n    ggplotly(p) %&gt;%\n      layout(hovermode = \"closest\")\n  })\n  \n  # ----------------------------------------------------------------------------\n  # TAB 4: Chick Table\n  # ----------------------------------------------------------------------------\n  output$chick_table &lt;- renderDT({\n    req(nrow(summary_table()) &gt; 0)\n    \n    datatable(\n      summary_table(),\n      options = list(\n        pageLength = 15,\n        searching = TRUE,\n        ordering = TRUE,\n        dom = 'frtip'\n      ),\n      selection = \"multiple\",\n      rownames = FALSE,\n      colnames = c(\n        \"Chick\", \"Diet\", \"N Timepoints\", \"First Weight (g)\",\n        \"Last Weight (g)\", \"Δ Weight (g)\", \"Slope (g/day)\"\n      )\n    ) %&gt;%\n      formatStyle(\n        \"Chick\",\n        target = \"row\",\n        backgroundColor = styleEqual(\n          highlighted_chicks(),\n          rep(\"lightyellow\", length(highlighted_chicks()))\n        )\n      )\n  })\n  \n  # Download handler for table\n  output$download_table &lt;- downloadHandler(\n    filename = function() {\n      paste0(\"chick_summary_\", Sys.Date(), \".csv\")\n    },\n    content = function(file) {\n      write.csv(summary_table(), file, row.names = FALSE)\n    }\n  )\n}\n\n# ==============================================================================\n# Run the App\n# ==============================================================================\nshinyApp(ui = ui, server = server)\n\n\n\n\n\nScreenshots of ChickWeight App\n\n\n\n\n\n\nFirst tab: Overview\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSecond tab: Growth Curves\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThird tab: Diet Comparison Plots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFourth tab: CSV Download\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFifth tab: About/Help Page",
    "crumbs": [
      "Activity: Shiny App Development"
    ]
  },
  {
    "objectID": "activity-shiny-app-development.html#prompt-build-a-shiny-app-to-explore-the-chickweight-dataset-no-apis",
    "href": "activity-shiny-app-development.html#prompt-build-a-shiny-app-to-explore-the-chickweight-dataset-no-apis",
    "title": "Activity: Shiny App Development",
    "section": "",
    "text": "Role: You are an expert R Shiny developer.\nTask: Create a complete, runnable single-file Shiny app (app.R) that explores the base R datasets::ChickWeight dataset. The app must include: (1) a sidebar layout with global filters, (2) a tabset in the main area for multiple linked views, and (3) hover pop-ups / tooltips that provide instructions or context when users hover over inputs or UI elements.No external APIs or LLMs—everything is local to R.\n\nData\nUse datasets::ChickWeight as the only data source. Variables:\n- weight (numeric), Time (days), Chick (ID), Diet (1–4).\nPackages (keep dependencies modest)\n\nRequired: shiny, ggplot2, dplyr, tidyr, DT, bslib\n\nOptional (strongly preferred for interactivity): plotly for hover tooltips on charts\n\nDo not use packages that need system libraries; keep it CRAN-friendly.\n\nUI & Layout\nUse bslib for a clean Bootstrap-5 theme (e.g., bslib::bs_theme(preset = \"minty\")).\nOverall structure: shiny::fluidPage() → shiny::sidebarLayout()\n- Sidebar panel (global filters): - Diet filter: checkboxGroupInput for Diet 1–4 (default: all) - Add a hover tooltip explaining diet codes (use bslib::tooltip() or bslib::popover() on the input label or an adjacent info icon). - Time range: sliderInput for days (0–21) - Add a tooltip: “Drag to focus growth interval; all summaries respect this window.” - Chick highlight: selectizeInput allowing multiple IDs to highlight (default: none) - Tooltip: “Type to find a Chick ID; selected IDs are emphasized in plots and table.” - Smoothing: checkboxInput to add LOESS mean curve per Diet (and optional CI ribbon) - Tooltip: “Show smoothed average by Diet; use with caution for small n.” - Scale: radioButtons for y-axis (“Linear”, “Log10”) - Tooltip: short rationale for log-scale. - Reset filters: actionButton(\"reset\", \"Reset Filters\")\n\n\nMain panel (tabset) Use tabsetPanel(id = \"tabs\", type = \"pills\") with the following tabs:\n\n\nOverview\n\nA valueBox-like summary row (simple cards) showing:\n\nNumber of visible chicks, average terminal weight (within time window), and number of timepoints.\n\n\nA short paragraph (Markdown) describing the dataset and how filters apply.\nA popover (hover info icon) that explains the study design and why repeated measures matter.\n\n\n\nGrowth Curves\n\n\nPrimary plot: weight vs. time lines, one line per Chick, colored by Diet.\nOptional mean LOESS curve per Diet + semi-transparent ribbon (if smoothing is on).\n\nHighlight selected Chick IDs (thicker line/alpha=1).\nIf plotly is available, convert ggplot to ggplotly() so that hover shows:\n\nChick, Diet, Time, Weight, and (if computed) per-Chick slope for current window.\n\n\nAdd a small info icon near the tab title with a bslib::popover() explaining how to read growth curves.\n\n\n\nDiet Comparison\n\nSide-by-side boxplots/violins of final weight (within the chosen time window) by Diet.\nA secondary bar chart ranking Diets by mean terminal weight (or mean Δweight per Chick).\nHover/tooltip on the plot area describing how “final weight” is derived (max Time within window).\n\n\n\nChick Table\n\nA DT::datatable listing only the currently visible chicks (after filters), with columns:\n\nChick, Diet, n_timepoints (within window), first_weight, last_weight, Δweight, and slope (simple lm(weight ~ Time) over the window).\n\n\n\nRow selection → highlights the corresponding Chick line in the Growth Curves tab (reactively).\nAdd a download button to export this table as CSV (no server storage).\nTooltip on the table header: “Click column headers to sort; use search to filter rows.”\n\n\n\nAbout / Help\n\nConcise prose on interpretation, caveats (missing values, small samples), and how smoothing/log scale change the view.\nA bslib::popover() on “Caveats” that appears on hover for a few seconds.\n\n\n\n\n\nServer Requirements (high-level behavior)\n\nCreate a single reactive filtered dataset based on Diet and Time range. All downstream outputs depend on this.\n\nLinked reactivity:\n\nChanging filters updates all tabs’ outputs.\nSelecting/changing Chick IDs highlights them in plots and the table.\nSelecting rows in the table highlights those lines in the Growth Curves plot.\n\n\n\nDerived metrics (computed on the filtered data):\n\nPer-Chick Δweight = (max weight − min weight) for times within the window.\nPer-Chick slope from lm(weight ~ Time) within the window; display as g/day.\nPer-Diet summaries (mean terminal weight, median Δweight).\n\n\n\nValidation/empty states:\n\nIf filters remove all data, show a friendly empty-state message instead of blank plots/tables.\n\n\n\nScale handling:\n\nIf log scale is selected, transform y-axis appropriately and suppress non-positive weights (with a note).\n\n\n\nReset button restores default filters.\n\nHover Pop-ups / Tooltips (must-have)\nUse bslib::tooltip() and/or bslib::popover() to attach hover help to: - Diet filter label (explain diet codes 1–4) - Time range slider label (explain windows) - Chick select label (how highlighting works) - Growth Curves tab title or an adjacent info icon (how to interpret lines and smoothing) - Diet Comparison plot container (how “final weight” is determined) - Table header (sorting/search tips)\nIf plotly is used, ensure hovertext on the graph shows: Chick, Diet, Time, Weight, and Slope (if available).\nQuality & Style\n\nClean, minimal design using bslib; sensible spacing and headings.\nClear code comments explaining each reactive object and output.\nNo external services or files; everything runs locally with shiny::runApp().\nKeep code in one file app.R.\n\nDeliverable\nOutput only a complete app.R in a single code block, ready to run.\nAcceptance Criteria\n\nApp launches with sidebar + tabset and all tabs functioning.\nFilters, highlights, and tabs are fully reactive and consistent.\nHover pop-ups/tooltips appear on specified inputs/areas using bslib.\nGrowth Curves and Diet Comparison are informative and respect filters/smoothing/scale.\nTable reflects the filtered/visible cohort and supports row-selection highlighting.\nCSV download works for the current table view.",
    "crumbs": [
      "Activity: Shiny App Development"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#task-med-ed-using-llms-to-apply-analytic-rubrics",
    "href": "activity-scoring-with-rubrics.html#task-med-ed-using-llms-to-apply-analytic-rubrics",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "",
    "text": "View model application of Med Ed Analytic Rubric\n\n\n\n\n\n\nCodelibrary(stringr)\nlibrary(knitr)\n\n# Some cleaning for the quarto output; not strictly necessary\nanalytic_response &lt;- str_replace_all(analytic_response, \"#\", \"*\")\n\nanalytic_response &lt;- str_replace_all(analytic_response, \n                                      \"(?m)^(\\\\*+)\\\\s+\", \n                                      \"\\\\1\")\n\nanalytic_response &lt;- str_replace_all(analytic_response, \n                                      \"(?m)^(\\\\*+)(.*?)(?&lt;!\\\\*)(\\\\n)\", \n                                      \"\\\\1\\\\2\\\\1\\\\3\")\n\nanalytic_response &lt;- str_replace_all(analytic_response,\n                                      \"\\\\*\\\\*(\\\\n)\\\\*\\\\*\",\n                                      \"**\\\\1\\\\1**\")\nknitr::asis_output(analytic_response)\n\nOSCE Post-Encounter Note Scoring Rubric\n1. Chief concern of chest pain\nStatus: INCLUDED\nJustification: The note clearly states “45-year-old male presents with Chest pain” in the opening sentence, explicitly identifying chest pain as the chief concern.**\n2. Episodic pattern of symptoms\nStatus: INCLUDED\nJustification: The note states “Recalls previous less severe episodes,” which indicates a pattern of recurring symptoms, demonstrating episodic nature.**\n3. Poorly controlled history of hypertension\nStatus: INCLUDED\nJustification: The note mentions “H/O high blood pressure” (history of hypertension) and current vitals show “positive for hypertension,” indicating the hypertension is not well controlled despite treatment.**\n4. Vitals indicate hypertension\nStatus: INCLUDED\nJustification: The note explicitly states “Current vitals positive for hypertension,” directly confirming elevated blood pressure on vital signs.**\n5. Pain radiates to the back\nStatus: NOT INCLUDED\nJustification: There is no mention of radiation of pain to the back or any other location. The note only describes “pressure-like chest discomfort” without documenting radiation patterns.**\n6. Likely diagnosis of acute coronary syndrome (ACS), NSTEMI, or STEMI\nStatus: NOT INCLUDED\nJustification: While the note mentions “possible angina” as a consideration, it does not specifically identify ACS, NSTEMI, or STEMI as likely diagnoses. The differential includes anxiety and GERD with equal weight, suggesting the student did not commit to an acute coronary syndrome diagnosis.**\n\nSummary: 4 of 6 criteria met",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#task-med-ed-using-llms-to-apply-holistic-rubrics",
    "href": "activity-scoring-with-rubrics.html#task-med-ed-using-llms-to-apply-holistic-rubrics",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "\n20.2 Task: Med Ed: Using LLMs to Apply Holistic Rubrics",
    "text": "20.2 Task: Med Ed: Using LLMs to Apply Holistic Rubrics\n\nCodesource('downloads/claude_plus.R')\n\nholistic_rubric &lt;- \"Each criterion should be rated as Insufficient (0 points), Developing (1 point), or Proficient (2 points) based on the quality of the response.\n\nChief complaint is clearly documented\n- Insufficient: Missing or unclear\n- Developing: Present but vague\n- Proficient: Clear and specific\n\nRelevant history is included\n- Insufficient: Minimal or missing key details\n- Developing: Some relevant information\n- Proficient: Comprehensive and pertinent\n\nPhysical exam findings are documented\n- Insufficient: Absent or incomplete\n- Developing: Basic findings noted\n- Proficient: Thorough and organized\n\nAssessment includes appropriate differential diagnoses\n- Insufficient: Missing or inappropriate\n- Developing: Limited differential\n- Proficient: Well-developed differential\n\nPlan is reasonable and addresses the patient’s concerns\n- Insufficient: Inadequate or missing\n- Developing: Partially addresses concerns\n- Proficient: Complete and appropriate\n\nDocumentation is organized\n- Insufficient: Disorganized or difficult to follow\n- Developing: Somewhat structured\n- Proficient: Logical and clear structure\"\n\nosce_holistic &lt;- function(student_note, rubric){\n  glue::glue(\"\n  Using the rubric provided below, score the following OSCE post-encounter note. \n\n  Here is the student note: {student_note}\n    \n  Here is the rubric: {rubric}\n    \n  Return a completed rubric, indicating justifications for each component.\n  \")\n}\n\nholistic_prompt &lt;- osce_holistic(osce_note, holistic_rubric)\n\nholistic_response &lt;- claude_plus(holistic_prompt,\n                                 temperature = 0)\n\n\n\n\n\n\n\n\nView model application of Med Ed Holistic Rubric\n\n\n\n\n\n\nCode# Some cleaning for the quarto output; not strictly necessary\nholistic_response &lt;- str_replace_all(holistic_response, \"#\", \"*\")\n\nholistic_response &lt;- str_replace_all(holistic_response, \n                                      \"(?m)^(\\\\*+)\\\\s+\", \n                                      \"\\\\1\")\n\nholistic_response &lt;- str_replace_all(holistic_response, \n                                      \"(?m)^(\\\\*+)(.*?)(?&lt;!\\\\*)(\\\\n)\", \n                                      \"\\\\1\\\\2\\\\1\\\\3\")\n\nholistic_response &lt;- str_replace_all(holistic_response,\n                                      \"\\\\*\\\\*(\\\\n)\\\\*\\\\*\",\n                                      \"**\\\\1\\\\1**\")\nknitr::asis_output(holistic_response)\n\nOSCE Post-Encounter Note Scoring Rubric\nStudent Performance Evaluation\n1. Chief complaint is clearly documented\nScore: Proficient (2 points)\nJustification: The chief complaint is clearly stated at the beginning: “45-year-old male presents with Chest pain.” This is specific, concise, and immediately identifies the primary concern.*\n\n2. Relevant history is included\nScore: Developing (1 point)\nJustification: The note includes some relevant information (pressure-like quality, timing with exertion, associated SOB, previous episodes, medication history). However, key details are missing such as: duration of current episode, radiation of pain, associated symptoms (diaphoresis, nausea), cardiac risk factors (smoking, diabetes, family history), medication compliance, and specific details about previous episodes. The history is present but not comprehensive enough for a chest pain presentation.*\n\n3. Physical exam findings are documented\nScore: Insufficient (0 points)\nJustification: Only vital signs are mentioned (hypertension and tachycardia). There is no documentation of actual physical examination findings such as cardiovascular exam (heart sounds, murmurs, JVD), lung exam, chest wall tenderness, peripheral pulses, or general appearance. For a chest pain case, a focused physical examination is essential and is essentially absent here.*\n\n4. Assessment includes appropriate differential diagnoses\nScore: Developing (1 point)\nJustification: The note identifies three possible diagnoses: angina (most appropriate given the presentation), anxiety, and GERD. While these are reasonable considerations, the differential is limited. Missing considerations include acute coronary syndrome/MI, pulmonary embolism, aortic dissection, musculoskeletal pain, and pericarditis. The differentials mentioned are appropriate but not well-developed or prioritized.*\n\n5. Plan is reasonable and addresses the patient’s concerns\nScore: Insufficient (0 points)\nJustification: There is no plan documented whatsoever. The note does not include any diagnostic workup (ECG, cardiac enzymes, chest X-ray), treatment interventions, disposition decisions, or follow-up instructions. This is a critical omission, especially for a potentially life-threatening presentation like chest pain.*\n\n6. Documentation is organized\nScore: Developing (1 point)\nJustification: The note has a basic structure with history elements followed by assessment, but it lacks the standard SOAP format or clear section headers. The information flows somewhat logically but would benefit from better organization with distinct sections for History of Present Illness, Past Medical History, Physical Exam, Assessment, and Plan.*\n\nTotal Score: 5/12 points (42%)\nOverall Comments: This note captures the essential presenting complaint and some relevant history but falls short in documentation of physical examination findings and completely lacks a management plan. For a high-stakes presentation like chest pain, more thorough documentation is essential for patient safety and clinical decision-making.\n\n\n\n\n\nWhat do you think about the model’s application of the holistic rubric? How might you have scored the note differently? What did the model do well (either in scoring or its justification), and where do you think it could have improved?\nAs you can see, applying a holistic rubric is generally more difficult for an AI model because it lacks the context of the clinical encounter. Some methods to improve the model performance in these instances would be to submit each rubric element individually and provide examples of each scoring level, and then empirically verify that the model is applying the rubric as intended. In general, using generative AI to apply holistic rubrics may take more work than having the model apply an analytic rubric.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#task-using-llms-to-apply-holistic-rubrics",
    "href": "activity-scoring-with-rubrics.html#task-using-llms-to-apply-holistic-rubrics",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "\n20.3 Task: Using LLMs to Apply Holistic Rubrics",
    "text": "20.3 Task: Using LLMs to Apply Holistic Rubrics\nMany attending AIME-CON are working in non-medical educational settings, so I wanted to develop a different example that may be more applicable to the work that you’re doing. For this example I’m going to use the rubric that was part of the Dataset for Rubric-based Essay Scoring (DREsS). This rubric was based on scoring essays from English as a foreign language (EFL) learners. The essays in that dataset are scored on a range from 1 to 5 with increments of 0.5. The full dataset is available after filling out a consent form.\nThe rubric has 3 components:\n\n\n\n\n\n\nCriteria\nDescription\n\n\n\nContent\nParagraph is well-developed and relevant to the argument, supported with strong reasons and examples.\n\n\nOrganization\nThe argument is very effectively structured and developed, making it easy for the reader to follow the ideas and understand how the writer is building the argument. Paragraphs use coherence devices effectively while focusing on a single main idea.\n\n\nLanguage\nThe writing displays sophisticated control of a wide range of vocabulary and collocations. The essay follows grammar and usage rules throughout the paper. Spelling and punctuation are correct throughout the paper.\n\n\n\nOne of the prompts in the data is: “Do you think that smartphones have destroyed communication among family and friends? Give specific reasons and details to support your opinion.”\nTo ensure that I’m compliant with the consent form agreement, I will be using synthetic responses generated (using Claude Sonnet 4.5) based observed student responses. We’ll also be using the claude_plus function we used in the parameter testing activity.\n\n20.3.1 First Model Call\nLet’s start by asking the model to apply the rubric to an essay. We’ll use the default parameters for this first\n\nCodesource('downloads/claude_plus.R')\nload('data/student_essays.Rdata')\n\nfocal_student &lt;- subset(student_essays, id == 2002)\nfocal_prompt &lt;- focal_student$prompt\nfocal_essay &lt;- focal_student$essay\n\n# Building a function to generate the prompt.\n\nall_rubric_prompt &lt;- function(prompt, essay){\n  glue::glue(\"\nYou are an expert essay grader. Score the following student essay based on three criteria: Content, Organization, and Language. Each criterion should be scored from 1 to 5 in increments of 0.5 (e.g., 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5).\n\nRUBRIC:\n\n**Content (1-5):**\nParagraph is well-developed and relevant to the argument, supported with strong reasons and examples.\n\n**Organization (1-5):**\nThe argument is very effectively structured and developed, making it easy for the reader to follow the ideas and understand how the writer is building the argument. Paragraphs use coherence devices effectively while focusing on a single main idea.\n \n**Language (1-5):**\nThe writing displays sophisticated control of a wide range of vocabulary and collocations. The essay follows grammar and usage rules throughout the paper. Spelling and punctuation are correct throughout the paper.\n\nPROMPT: {prompt}\n\nESSAY: {essay}\n\nPlease provide scores for each criterion and calculate the total score. Format your response as follows:\nContent: [score]\nOrganization: [score]\nLanguage: [score]\nTotal: [sum of three scores]\n\nProvide a brief justification (1-2 sentences) for each score.\n\")\n}\n\none_rep_essay &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay))\nsave(one_rep_essay, file = 'data/one_rep_essay.Rdata')\n\n\nClaude’s essay evaluation:\nEssay Evaluation\nContent: 3/5 The essay presents a balanced view with relevant examples (grandmother video calls, phone distraction), but the development is superficial. The argument lacks depth, multiple examples, and detailed elaboration that would make it well-developed and strongly supported.\nOrganization: 3.5/5 The essay follows a clear structure (introducing both sides, personal example, counterpoint, conclusion) and is easy to follow. However, it’s quite brief and would benefit from additional paragraphs to fully develop each main idea separately rather than condensing everything into one paragraph.\nLanguage: 3.5/5 The writing demonstrates good control of grammar and vocabulary with no significant errors (\"on one hand/on the other hand,\" \"responsibly\"). However, the vocabulary range is relatively basic and lacks the sophistication and variety expected for a top score.\nTotal: 10/15\nAs noted in the previously (in WHAT section), part of the beauty of generative AI model is the variability in output. Great for creative tasks, but not optimal for things like the present task of applying a scoring rubric to a student response.\n\n20.3.2 Repeating Score Task\nLet’s repeat the above process 20 times in a loop, extracting the scores after each model response.\n\nCode# Function to extract scores\n\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(glue)\nlibrary(dplyr)\n\n# Set your Anthropic API key\nANTHROPIC_API_KEY &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n\n# Load the prompt function (from previous artifact)\nall_rubric_prompt &lt;- function(prompt, essay){\n  glue::glue(\"\nYou are an expert essay grader. Score the following student essay based on three criteria: Content, Organization, and Language. Each criterion should be scored from 1 to 5 in increments of 0.5.\n\nRUBRIC:\n\n**Content (1-5):**\n- 5: Exceptionally well-developed and highly relevant, with strong reasons and compelling examples.\n- 4: Well-developed and relevant, with clear reasons and good examples.\n- 3: Adequately developed with acceptable reasons and examples.\n- 2: Underdeveloped with weak or insufficient reasons and examples.\n- 1: Poorly developed and largely irrelevant.\n\n**Organization (1-5):**\n- 5: Exceptionally well-structured, very easy to follow, expert use of coherence devices.\n- 4: Effectively structured, easy to follow, good use of coherence devices.\n- 3: Adequate structure, generally followable.\n- 2: Weak structure, sometimes difficult to follow.\n- 1: Lacks clear structure, confusing.\n\n**Language (1-5):**\n- 5: Sophisticated vocabulary, grammar and spelling correct throughout.\n- 4: Good vocabulary control, generally correct with minor errors.\n- 3: Adequate vocabulary, acceptable with some errors.\n- 2: Limited vocabulary, frequent errors that sometimes impede understanding.\n- 1: Very limited vocabulary, numerous errors.\n\nPROMPT: {prompt}\n\nESSAY: {essay}\n\nFormat your response as:\nContent: [score]\nOrganization: [score]\nLanguage: [score]\nTotal: [sum of three scores]\n\")\n}\n\n# Score extraction function with debug capability\nextract_scores &lt;- function(gpt_output, debug = FALSE){\n  if(debug){\n    cat(\"=== DEBUG: Extracting scores ===\\n\")\n    cat(\"Input text (first 300 chars):\\n\")\n    cat(substr(gpt_output, 1, 300), \"\\n\\n\")\n  }\n  \n  # Extract Content score - try multiple patterns\n  content_match &lt;- stringr::str_match(gpt_output, \"Content:?\\\\s*([0-9.]+)\")\n  if(is.na(content_match[1, 2])){\n    # Try alternate pattern\n    content_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Content:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  content &lt;- as.numeric(content_match[1, 2])\n  \n  # Extract Organization score\n  org_match &lt;- stringr::str_match(gpt_output, \"Organization:?\\\\s*([0-9.]+)\")\n  if(is.na(org_match[1, 2])){\n    org_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Organization:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  organization &lt;- as.numeric(org_match[1, 2])\n  \n  # Extract Language score\n  lang_match &lt;- stringr::str_match(gpt_output, \"Language:?\\\\s*([0-9.]+)\")\n  if(is.na(lang_match[1, 2])){\n    lang_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Language:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  language &lt;- as.numeric(lang_match[1, 2])\n  \n  # Extract Total score\n  total_match &lt;- stringr::str_match(gpt_output, \"Total:?\\\\s*([0-9.]+)\")\n  if(is.na(total_match[1, 2])){\n    total_match &lt;- stringr::str_match(gpt_output, \"\\\\*\\\\*Total:?\\\\*\\\\*[^0-9]*([0-9.]+)\")\n  }\n  total &lt;- as.numeric(total_match[1, 2])\n  \n  if(debug){\n    cat(\"Extracted values:\\n\")\n    cat(\"Content:\", content, \"\\n\")\n    cat(\"Organization:\", organization, \"\\n\")\n    cat(\"Language:\", language, \"\\n\")\n    cat(\"Total:\", total, \"\\n\\n\")\n  }\n  \n  # Return as a named vector\n  c(content = content, \n    organization = organization, \n    language = language, \n    total = total)\n}\n\n\n# Returns a data frame (useful for binding multiple results)\nextract_scores_df &lt;- function(gpt_output){\n  scores &lt;- extract_scores(gpt_output)\n  data.frame(\n    content = scores[\"content\"],\n    organization = scores[\"organization\"],\n    language = scores[\"language\"],\n    total = scores[\"total\"]\n  )\n}\n\nessay_score_df &lt;- extract_scores_df(one_rep_essay)\n\nfor(i in 1:18){\n  \n  essay_score_again &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay))\n  essay_score_one &lt;- extract_scores_df(essay_score_again)\n  essay_score_df &lt;- rbind(essay_score_df, essay_score_one)\n\n}\n\nrownames(essay_score_df) &lt;- NULL\nsave(essay_score_df, file = \"data/essay_score_df.Rdata\")\n\n\nLet’s now see how consistently the model applied the scores to the essay:\n\nCode# Function to get summary statistics for multiple scored essays\nsummarize_scores &lt;- function(score_df){\n \n  # Calculate summary statistics\n  summary_stats &lt;- data.frame(\n    criterion = c(\"Content\", \"Organization\", \"Language\", \"Total\"),\n    mean = c(mean(score_df$content, na.rm = TRUE),\n             mean(score_df$organization, na.rm = TRUE),\n             mean(score_df$language, na.rm = TRUE),\n             mean(score_df$total, na.rm = TRUE)),\n    sd = c(sd(score_df$content, na.rm = TRUE),\n           sd(score_df$organization, na.rm = TRUE),\n           sd(score_df$language, na.rm = TRUE),\n           sd(score_df$total, na.rm = TRUE)),\n    min = c(min(score_df$content, na.rm = TRUE),\n            min(score_df$organization, na.rm = TRUE),\n            min(score_df$language, na.rm = TRUE),\n            min(score_df$total, na.rm = TRUE)),\n    max = c(max(score_df$content, na.rm = TRUE),\n            max(score_df$organization, na.rm = TRUE),\n            max(score_df$language, na.rm = TRUE),\n            max(score_df$total, na.rm = TRUE))\n  )\n  \n  return(summary_stats)\n}\n\nload(\"data/essay_score_df.Rdata\")\nscore_summary &lt;- summarize_scores(essay_score_df)\nscore_summary\n\n     criterion  mean        sd min  max\n1      Content 3.025 0.1118034 3.0  3.5\n2 Organization 3.250 0.2564946 3.0  3.5\n3     Language 3.500 0.0000000 3.5  3.5\n4        Total 9.775 0.2552089 9.5 10.0\n\n\nPretty good! There is only .5 variation in scores for Content and Organization, and no variation in Language scores.\n\n20.3.3 Repeating Score Task (Temp = 0)\nLet’s now repeat the task, but turn the temperature down and see how this affects score variability.\n\nCodelow_temp_essay &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay),\n                              temperature = 0)\nlow_temp_df &lt;- extract_scores_df(low_temp_essay)\n\nfor(i in 1:19){\n  \n  low_temp_score_again &lt;- claude_plus(prompt = all_rubric_prompt(focal_prompt, focal_essay),\n                                      temperature = 0)\n  low_temp_score_one &lt;- extract_scores_df(low_temp_score_again)\n  low_temp_df &lt;- rbind(low_temp_df, low_temp_score_one)\n\n}\n\nrownames(low_temp_df) &lt;- NULL\nsave(low_temp_df, file = \"data/low_temp_df.Rdata\")\n\n\n\nCodeload(\"data/low_temp_df.Rdata\")\nlow_temp_summary &lt;- summarize_scores(low_temp_df)\nlow_temp_summary\n\n     criterion mean sd min max\n1      Content  3.0  0 3.0 3.0\n2 Organization  3.0  0 3.0 3.0\n3     Language  3.5  0 3.5 3.5\n4        Total  9.5  0 9.5 9.5\n\n\nNice! The eliminated all of the score variability. However, given the vague nature of the rubric, we still don’t know if these scores are accurate - we may be consistently missing the mark on what the true score should be. The rubric criteria are broad and it’s unclear how one should apply the rubric. What constitutes each score level for the individual criteria?\nInterestingly, when I asked Anthropic for help in developing a scoring rubric for the essays, it recognized that having clear descriptions of the different scoring categories was important, so it made them up! The prompt below shows how the model decided to describe each of the score categories. Obviously these descriptions should be developed and reviewed by subject matter experts, but for the purposes of the workshop we’ll just use what Claude generated. In your process you also want to check that your generative AI model is utilizing the rubric correctly - supplying a detailed rubric and obtaining consistent results alone isn’t sufficient.\n\n20.3.4 Scoring with Detailed Rubric\n\nCodeclaude_all_rubric_prompt &lt;- function(prompt, essay){\n  glue::glue(\"\nYou are an expert essay grader. Score the following student essay based on three criteria: Content, Organization, and Language. Each criterion should be scored from 1 to 5 in increments of 0.5 (e.g., 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5).\n\nRUBRIC:\n\n**Content (1-5):**\n- 5: Paragraph is exceptionally well-developed and highly relevant to the argument, supported with strong, specific reasons and compelling examples.\n- 4: Paragraph is well-developed and relevant to the argument, supported with clear reasons and good examples.\n- 3: Paragraph is adequately developed with some relevance to the argument, supported with acceptable reasons and examples.\n- 2: Paragraph is underdeveloped or somewhat irrelevant to the argument, with weak or insufficient reasons and examples.\n- 1: Paragraph is poorly developed and largely irrelevant to the argument, lacking meaningful reasons and examples.\n\n**Organization (1-5):**\n- 5: The argument is exceptionally well-structured and developed, making it very easy for the reader to follow ideas and understand how the argument is built. Paragraphs use coherence devices expertly while maintaining clear focus on main ideas.\n- 4: The argument is effectively structured and developed, making it easy for the reader to follow ideas. Paragraphs use coherence devices well and focus on main ideas.\n- 3: The argument has adequate structure and development. The reader can follow most ideas. Paragraphs use some coherence devices and generally focus on main ideas.\n- 2: The argument has weak structure and development. Ideas are sometimes difficult to follow. Paragraphs use few coherence devices and may lose focus.\n- 1: The argument lacks clear structure and development. Ideas are confusing and hard to follow. Paragraphs lack coherence devices and focus.\n\n**Language (1-5):**\n- 5: The writing displays sophisticated control of a wide range of vocabulary and collocations. Grammar and usage are correct throughout. Spelling and punctuation are correct throughout.\n- 4: The writing displays good control of vocabulary and collocations. Grammar and usage are generally correct with only minor errors. Spelling and punctuation are generally correct.\n- 3: The writing displays adequate vocabulary with some variety. Grammar and usage are acceptable with some errors that do not impede understanding. Spelling and punctuation are mostly correct.\n- 2: The writing displays limited vocabulary with little variety. Grammar and usage contain frequent errors that sometimes impede understanding. Spelling and punctuation errors are noticeable.\n- 1: The writing displays very limited vocabulary. Grammar and usage contain numerous errors that significantly impede understanding. Spelling and punctuation errors are frequent.\n\nPROMPT: {prompt}\n\nESSAY: {essay}\n\nPlease provide scores for each criterion and calculate the total score. Format your response as follows:\nContent: [score]\nOrganization: [score]\nLanguage: [score]\nTotal: [sum of three scores]\n\nProvide a brief justification (1-2 sentences) for each score.\n\")\n}\n\n\nLet’s now apply this rubric to the example essay and see how it might impact model-generated scores.\n\nCodeclaude_rubric_score &lt;- claude_plus(prompt = claude_all_rubric_prompt(focal_prompt, focal_essay),\n                                   temperature = 0)\nclaude_rubric_df &lt;- extract_scores_df(claude_rubric_score)\n\nfor(i in 1:19){\n  \n  claude_rubric_score_again &lt;- claude_plus(prompt = claude_all_rubric_prompt(focal_prompt, focal_essay),\n                                           temperature = 0)\n  claude_rubric_one &lt;- extract_scores_df(claude_rubric_score_again)\n  claude_rubric_df &lt;- rbind(claude_rubric_df, claude_rubric_one)\n\n}\n\nrownames(claude_rubric_df) &lt;- NULL\nsave(claude_rubric_df, file = \"data/claude_rubric_df.Rdata\")\n\n\n\nCodeload(\"data/claude_rubric_df.Rdata\")\nclaude_rubric_summary &lt;- summarize_scores(claude_rubric_df)\nclaude_rubric_summary\n\n     criterion mean sd min max\n1      Content  3.0  0 3.0 3.0\n2 Organization  3.0  0 3.0 3.0\n3     Language  3.5  0 3.5 3.5\n4        Total  9.5  0 9.5 9.5\n\n\nWhoa! These are the exact same scores as when we didn’t provide detailed rubric explanation in our prompt. This suggests to me that the model is generating a similar rubric each time through invisible instructions, but I can’t know this for sure.\nIt has also been empirically demonstrated (find the citations, Chris) that these models don’t apply rubrics well at the extremes - the lowest and highest scores. When I used Claude to help generate synthetic student responses, I asked it have a range of score values represented in the 10 generated essays. It’ll be useful to repeat the same experiment on all 10 essays to see if the scoring variability is consistent across the score range. (There’s a strong possibility that it will be - I asked Claude to generate the essays and now I’m asking it to score essays it generated.)\n\n20.3.5 Batch Scoring\nNow that we’re repeating the same task across a batch of essays, it’s a great time to introduce batch scoring! This generally works as you’d think - you submit multiple tasks at once to the API to complete in parallel - with the one wrinkle that Anthropic requires that the requests be in the JSONL format. In addition to being able to submit multiple requests at once, it’s also cheaper to use than single-call API interactions. As of this writing (Oct 19, 2025), you save 50% with batch pricing. This drops input costs from $3 per 1M tokens to $1.50 per 1M token, and output costs from $15 per 1M tokens to $7.50 per 1M tokens.\n\n20.3.5.1 Creating Batch Requests\n\nCode# Step 1: Read essays and create batch requests\ncreate_batch_requests &lt;- function(essays_df, n_repetitions = 20){\n\n  # Create a list to store batch requests\n  batch_requests &lt;- list()\n  request_counter &lt;- 1\n  \n  # Create n_repetitions requests for each essay (grouped by essay)\n  for(i in 1:nrow(essays_df)){\n    essay_prompt &lt;- claude_all_rubric_prompt(essays_df$prompt[i], essays_df$essay[i])\n    \n    # Create 20 requests for this essay\n    for(rep in 1:n_repetitions){\n      request &lt;- list(\n        custom_id = paste0(\"essay_\", essays_df$id[i], \"_rep_\", rep),\n        params = list(\n          model = \"claude-sonnet-4-20250514\",\n          max_tokens = 1024,\n          messages = list(\n            list(\n              role = \"user\",\n              content = essay_prompt\n            )\n          )\n        )\n      )\n      \n      batch_requests[[request_counter]] &lt;- request\n      request_counter &lt;- request_counter + 1\n    }\n  }\n  \n  cat(\"Created\", length(batch_requests), \"batch requests for\", nrow(essays_df), \"essays\\n\")\n  \n  return(batch_requests)\n}\n\nessay_batch_requests &lt;- create_batch_requests(student_essays)\n\n\n\n20.3.5.2 Writing Requests to JSONL\n\nCode# Step 2: Write requests to JSONL file\nwrite_batch_file &lt;- function(batch_requests, output_file = \"batch_requests.jsonl\"){\n  # Write each request as a JSON line\n  jsonl_lines &lt;- sapply(batch_requests, function(req){\n    toJSON(req, auto_unbox = TRUE)\n  })\n  \n  writeLines(jsonl_lines, output_file)\n  return(output_file)\n}\n\nessay_batch_jsonl &lt;- write_batch_file(essay_batch_requests)\n\n\n\n20.3.5.3 Submitting Batch Request\n\nCode# Step 3: Submit batch job to Anthropic\nsubmit_batch &lt;- function(jsonl_file){\n  # Read and parse each line of the JSONL file\n  jsonl_lines &lt;- readLines(jsonl_file)\n  \n  # Parse each JSON line into a list\n  requests_list &lt;- lapply(jsonl_lines, function(line){\n    fromJSON(line, simplifyVector = FALSE)\n  })\n  \n  # Create batch\n  response &lt;- POST(\n    url = \"https://api.anthropic.com/v1/messages/batches\",\n    add_headers(\n      \"x-api-key\" = Sys.getenv(\"ANTHROPIC_API_KEY\"),\n      \"anthropic-version\" = \"2023-06-01\",\n      \"content-type\" = \"application/json\"\n    ),\n    body = toJSON(list(\n      requests = requests_list\n    ), auto_unbox = TRUE),\n    encode = \"json\"\n  )\n  \n  if(status_code(response) != 200){\n    stop(\"Error submitting batch: \", content(response, \"text\"))\n  }\n  \n  result &lt;- content(response, \"parsed\")\n  cat(\"Batch submitted successfully!\\n\")\n  cat(\"Batch ID:\", result$id, \"\\n\")\n  \n  return(result)\n}\n\nessay_batch_info &lt;- submit_batch(essay_batch_jsonl)\n\n# Batch submitted successfully!\n# Batch ID: msgbatch_018m5hGS9Lm3Gi28UorfsVTS\n\n\n\n20.3.5.4 Checking Batch Status\n\nCode# Step 4: Check batch status\ncheck_batch_status &lt;- function(batch_id){\n  response &lt;- GET(\n    url = paste0(\"https://api.anthropic.com/v1/messages/batches/\", batch_id),\n    add_headers(\n      \"x-api-key\" = Sys.getenv(\"ANTHROPIC_API_KEY\"),\n      \"anthropic-version\" = \"2023-06-01\"\n    )\n  )\n  \n  if(status_code(response) != 200){\n    stop(\"Error checking batch status: \", content(response, \"text\"))\n  }\n  \n  result &lt;- content(response, \"parsed\")\n  return(result)\n}\n\nessay_status &lt;- check_batch_status(essay_batch_info$id)\n# Finished!\n\nlibrary(lubridate)\nbatch_start_time &lt;- ymd_hms(essay_status$created_at)\nbatch_stop_time &lt;- ymd_hms(essay_status$ended_at)\nbatch_run_time &lt;- as.numeric(batch_stop_time - batch_start_time)\n# Time difference of 1.644671 mins\n# Less than 2 minutes to score 200 essays!\n\n\n\n20.3.5.5 Saving Batch Results\n\nCodeget_batch_results &lt;- function(batch_id){\n  response &lt;- GET(\n    url = paste0(\"https://api.anthropic.com/v1/messages/batches/\", batch_id, \"/results\"),\n    add_headers(\n      \"x-api-key\" = Sys.getenv(\"ANTHROPIC_API_KEY\"),\n      \"anthropic-version\" = \"2023-06-01\"\n    )\n  )\n  \n  if(status_code(response) != 200){\n    stop(\"Error retrieving results: \", content(response, \"text\"))\n  }\n  \n  # Parse JSONL results\n  results_text &lt;- content(response, \"text\", encoding = \"UTF-8\")\n  results_lines &lt;- strsplit(results_text, \"\\n\")[[1]]\n  results_list &lt;- lapply(results_lines[results_lines != \"\"], fromJSON)\n  \n  return(results_list)\n}\n\nessay_batch_results &lt;- get_batch_results(essay_batch_info$id)\nsave(essay_batch_results, file = \"data/essay_batch_results.Rdata\")\n\n\n\n20.3.5.6 Extracting Score from Results\n\nCode# Step 6: Extract scores from batch results\nextract_batch_scores &lt;- function(results_list, debug = FALSE){\n  scores_df &lt;- data.frame(\n    essay_id = character(),\n    repetition = integer(),\n    content = numeric(),\n    organization = numeric(),\n    language = numeric(),\n    total = numeric(),\n    stringsAsFactors = FALSE\n  )\n  \n  for(i in seq_along(results_list)){\n    result &lt;- results_list[[i]]\n    \n    if(result$result$type == \"succeeded\"){\n      # Extract the response text - handle different possible structures\n      message_content &lt;- result$result$message$content\n      \n      # Debug: Print structure if requested\n      if(debug && i == 1){\n        cat(\"=== DEBUG: First result structure ===\\n\")\n        cat(\"Content class:\", class(message_content), \"\\n\")\n        cat(\"Content length:\", length(message_content), \"\\n\")\n        if(is.list(message_content) && length(message_content) &gt; 0){\n          cat(\"First element class:\", class(message_content[[1]]), \"\\n\")\n          print(str(message_content[[1]]))\n        }\n      }\n      \n      # Extract text from various possible structures\n      response_text &lt;- NULL\n      \n      if(is.data.frame(message_content)){\n        # If it's a data frame, look for a 'text' column\n        if(\"text\" %in% names(message_content)){\n          response_text &lt;- message_content$text[1]\n        }\n      } else if(is.list(message_content) && length(message_content) &gt; 0){\n        first_elem &lt;- message_content[[1]]\n        \n        if(is.list(first_elem)){\n          # Check for $text field\n          if(!is.null(first_elem$text)){\n            response_text &lt;- first_elem$text\n          } else if(!is.null(first_elem$type) && first_elem$type == \"text\" && !is.null(first_elem$text)){\n            response_text &lt;- first_elem$text\n          }\n        } else if(is.character(first_elem)){\n          response_text &lt;- first_elem\n        }\n      } else if(is.character(message_content)){\n        response_text &lt;- message_content\n      }\n      \n      # If we still don't have text, try to extract it more aggressively\n      if(is.null(response_text) || length(response_text) == 0){\n        # Try to find any 'text' field in the structure\n        if(is.list(message_content)){\n          text_fields &lt;- unlist(lapply(message_content, function(x) {\n            if(is.list(x) && \"text\" %in% names(x)) x$text else NULL\n          }))\n          if(length(text_fields) &gt; 0) response_text &lt;- text_fields[1]\n        }\n      }\n      \n      if(is.null(response_text) || length(response_text) == 0){\n        warning(\"Could not extract text for \", result$custom_id)\n        if(debug){\n          cat(\"=== Full structure for \", result$custom_id, \" ===\\n\")\n          print(str(message_content))\n        }\n        next\n      }\n      \n      # Debug: Print first response text\n      if(debug && i == 1){\n        cat(\"\\n=== First response text (first 500 chars) ===\\n\")\n        cat(substr(response_text, 1, 500), \"\\n\\n\")\n      }\n      \n      # Extract scores using our previous function\n      scores &lt;- extract_scores(response_text, debug = debug && i == 1)\n      \n      # Debug: Check if scores were extracted\n      if(debug && i == 1){\n        cat(\"=== Extracted scores ===\\n\")\n        print(scores)\n      }\n      \n      # Parse custom_id to extract essay_id and repetition\n      # Format: essay_2001_rep_1\n      custom_id_parts &lt;- strsplit(result$custom_id, \"_\")[[1]]\n      essay_id &lt;- custom_id_parts[2]\n      repetition &lt;- as.integer(custom_id_parts[4])\n      \n      scores_df &lt;- rbind(scores_df, data.frame(\n        essay_id = essay_id,\n        repetition = repetition,\n        content = scores[\"content\"],\n        organization = scores[\"organization\"],\n        language = scores[\"language\"],\n        total = scores[\"total\"]\n      ))\n    } else {\n      warning(\"Request \", result$custom_id, \" failed: \", result$result$error$message)\n    }\n  }\n  \n  return(scores_df)\n}\n\n\nessay_batch_df &lt;- extract_batch_scores(essay_batch_results, debug = TRUE)\nsave(essay_batch_df, file = 'data/essay_batch_df.Rdata')\n\n\n\nCodelibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nCodelibrary(DT)\nload('data/essay_batch_df.Rdata')\n\nessay_summary_detailed &lt;- essay_batch_df %&gt;%\n  group_by(essay_id) %&gt;%\n  summarise(\n    n = n(),\n    # Content\n    content_mean = mean(content, na.rm = TRUE),\n    content_sd = sd(content, na.rm = TRUE),\n    content_min = min(content, na.rm = TRUE),\n    content_max = max(content, na.rm = TRUE),\n    # Organization\n    org_mean = mean(organization, na.rm = TRUE),\n    org_sd = sd(organization, na.rm = TRUE),\n    org_min = min(organization, na.rm = TRUE),\n    org_max = max(organization, na.rm = TRUE),\n    # Language\n    lang_mean = mean(language, na.rm = TRUE),\n    lang_sd = sd(language, na.rm = TRUE),\n    lang_min = min(language, na.rm = TRUE),\n    lang_max = max(language, na.rm = TRUE),\n    # Total\n    total_mean = mean(total, na.rm = TRUE),\n    total_sd = sd(total, na.rm = TRUE),\n    total_min = min(total, na.rm = TRUE),\n    total_max = max(total, na.rm = TRUE),\n  ) %&gt;%\n  arrange(total_mean)\n\nessay_summary_detailed &lt;- essay_summary_detailed %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 2)))\n\ndatatable(essay_summary_detailed, \n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"Summary Statistics by Essay (20 repetitions each)\")\n\n\n\n\n\nSo we did see some variation, but I also used the default temperature.\n\n20.3.6 Batch Scoring (Temp = 0)\nLet’s redo the batch call with temperature = 0.\n\nCodecreate_lowtemp_requests &lt;- function(essays_df, n_repetitions = 20){\n\n  # Create a list to store batch requests\n  batch_requests &lt;- list()\n  request_counter &lt;- 1\n  \n  # Create n_repetitions requests for each essay (grouped by essay)\n  for(i in 1:nrow(essays_df)){\n    essay_prompt &lt;- claude_all_rubric_prompt(essays_df$prompt[i], essays_df$essay[i])\n    \n    # Create 20 requests for this essay\n    for(rep in 1:n_repetitions){\n      request &lt;- list(\n        custom_id = paste0(\"essay_\", essays_df$id[i], \"_rep_\", rep),\n        params = list(\n          model = \"claude-sonnet-4-20250514\",\n          max_tokens = 1024,\n          temperature = 0, # added low temp\n          messages = list(\n            list(\n              role = \"user\",\n              content = essay_prompt\n            )\n          )\n        )\n      )\n      \n      batch_requests[[request_counter]] &lt;- request\n      request_counter &lt;- request_counter + 1\n    }\n  }\n  \n  cat(\"Created\", length(batch_requests), \"batch requests for\", nrow(essays_df), \"essays\\n\")\n  \n  return(batch_requests)\n}\n\nlowtemp_batch_requests &lt;- create_lowtemp_requests(student_essays)\nlowtemp_batch_jsonl &lt;- write_batch_file(lowtemp_batch_requests)\nlowtemp_batch_info &lt;- submit_batch(lowtemp_batch_jsonl)\n# Batch submitted successfully!\n# Batch ID: msgbatch_0184nJ7Kui9v6WVVh47939mn \n\nlowtemp_status &lt;- check_batch_status(lowtemp_batch_info$id)\nlowtemp_start_time &lt;- ymd_hms(lowtemp_status$created_at)\nlowtemp_stop_time &lt;- ymd_hms(lowtemp_status$ended_at)\nlowtemp_run_time &lt;- as.numeric(lowtemp_stop_time - lowtemp_start_time)\n# Time difference of 1.473892 mins\n\nlowtemp_batch_results &lt;- get_batch_results(lowtemp_batch_info$id)\nsave(lowtemp_batch_results, file = \"data/lowtemp_batch_results.Rdata\")\n\nlowtemp_batch_df &lt;- extract_batch_scores(lowtemp_batch_results)\nsave(lowtemp_batch_df, file = 'data/lowtemp_batch_df.Rdata')\n\n\n\nCodeload('data/lowtemp_batch_df.Rdata')\n\nlowtemp_summary_detailed &lt;- lowtemp_batch_df %&gt;%\n  group_by(essay_id) %&gt;%\n  summarise(\n    n = n(),\n    # Content\n    content_mean = mean(content, na.rm = TRUE),\n    content_sd = sd(content, na.rm = TRUE),\n    content_min = min(content, na.rm = TRUE),\n    content_max = max(content, na.rm = TRUE),\n    # Organization\n    org_mean = mean(organization, na.rm = TRUE),\n    org_sd = sd(organization, na.rm = TRUE),\n    org_min = min(organization, na.rm = TRUE),\n    org_max = max(organization, na.rm = TRUE),\n    # Language\n    lang_mean = mean(language, na.rm = TRUE),\n    lang_sd = sd(language, na.rm = TRUE),\n    lang_min = min(language, na.rm = TRUE),\n    lang_max = max(language, na.rm = TRUE),\n    # Total\n    total_mean = mean(total, na.rm = TRUE),\n    total_sd = sd(total, na.rm = TRUE),\n    total_min = min(total, na.rm = TRUE),\n    total_max = max(total, na.rm = TRUE),\n  ) %&gt;%\n  arrange(total_mean)\n\nlowtemp_summary_detailed &lt;- lowtemp_summary_detailed %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 2)))\n\ndatatable(lowtemp_summary_detailed, \n          options = list(pageLength = 10, scrollX = TRUE),\n          caption = \"Summary Statistics by Essay (20 repetitions each) - Temperature = 0\")\n\n\n\n\n\nNow we’re seeing more consistent scores, and it took less than 90 seconds to process all 200 requests.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#chief-concern-of-chest-pain",
    "href": "activity-scoring-with-rubrics.html#chief-concern-of-chest-pain",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "",
    "text": "Status: INCLUDED\nJustification: The note clearly states “45-year-old male presents with Chest pain” in the opening sentence, explicitly identifying chest pain as the chief concern.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#episodic-pattern-of-symptoms",
    "href": "activity-scoring-with-rubrics.html#episodic-pattern-of-symptoms",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "\n21.2 2. Episodic pattern of symptoms",
    "text": "21.2 2. Episodic pattern of symptoms\nStatus: INCLUDED\nJustification: The note states “Recalls previous less severe episodes,” which indicates a pattern of recurring symptoms, demonstrating episodic nature.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#poorly-controlled-history-of-hypertension",
    "href": "activity-scoring-with-rubrics.html#poorly-controlled-history-of-hypertension",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "\n21.3 3. Poorly controlled history of hypertension",
    "text": "21.3 3. Poorly controlled history of hypertension\nStatus: INCLUDED\nJustification: The note mentions “H/O high blood pressure” (history of hypertension) and current vitals show “positive for hypertension,” indicating the hypertension is not well controlled despite treatment.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#vitals-indicate-hypertension",
    "href": "activity-scoring-with-rubrics.html#vitals-indicate-hypertension",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "\n21.4 4. Vitals indicate hypertension",
    "text": "21.4 4. Vitals indicate hypertension\nStatus: INCLUDED\nJustification: The note explicitly states “Current vitals positive for hypertension,” directly confirming elevated blood pressure on vital signs.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#pain-radiates-to-the-back",
    "href": "activity-scoring-with-rubrics.html#pain-radiates-to-the-back",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "\n21.5 5. Pain radiates to the back",
    "text": "21.5 5. Pain radiates to the back\nStatus: NOT INCLUDED\nJustification: There is no mention of radiation of pain to the back or any other location. The note only describes “pressure-like chest discomfort” without documenting radiation patterns.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-scoring-with-rubrics.html#likely-diagnosis-of-acute-coronary-syndrome-acs-nstemi-or-stemi",
    "href": "activity-scoring-with-rubrics.html#likely-diagnosis-of-acute-coronary-syndrome-acs-nstemi-or-stemi",
    "title": "20  Activity: Scoring with Rubrics",
    "section": "\n21.6 6. Likely diagnosis of acute coronary syndrome (ACS), NSTEMI, or STEMI",
    "text": "21.6 6. Likely diagnosis of acute coronary syndrome (ACS), NSTEMI, or STEMI\nStatus: NOT INCLUDED\nJustification: While the note mentions “possible angina” as a consideration, it does not specifically identify ACS, NSTEMI, or STEMI as likely diagnoses. The differential includes anxiety and GERD with equal weight, suggesting the student did not commit to an acute coronary syndrome diagnosis.\n\nSummary: 4 of 6 criteria met",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Activity: Scoring with Rubrics</span>"
    ]
  },
  {
    "objectID": "activity-chained-workflows.html",
    "href": "activity-chained-workflows.html",
    "title": "22  Activity: Chained Workflows",
    "section": "",
    "text": "22.1 Task: Generate Feedback Report\nNow let’s work through an example of how you might implement a chained workflow. We’ll start with the work we did in a previous section where we applied analytic rubrics to an OSCE note. What can one do with that note? Many things! Below is an image demonstrating how one could build a chained workflow for a comprehensive approach to evaluating student notes from OSCE assessments.\nWe won’t complete all of these tasks. Just working through a few steps will be sufficient to give you a sense of how a chained workflow might work in practice.\nWe have a report that we obtained by applying an analytic rubric to the student’s OSCE note, but that report is not sufficient to provide the student with helpful feedback on their performance. Yes, it does provide a score and an indication of what information was present or not in their note, but this doesn’t translate to actionable feedback. We can now craft a prompt that uses this generated report as the basis for a prompt to generate feedback that would be more appropriate to provide to the student.\nCodeload(\"data/analytic_response.Rdata\")\n\nbuild_feedback_prompt &lt;- function(completed_rubric){\n  glue::glue(\n    \"Using the following completed rubric as a guide, provide constructive feedback to the student about their clinical case documentation. \n    \n    The feedback should consist of two well-developed paragraphs:\n    The first paragraph should offer a balanced assessment of the student's documentation. \n    Begin by acknowledging elements that were documented well (if any), then identify the most critical missing or inadequately addressed components based on the rubric. \n    Be specific about what was missing and, where applicable, note the quality or completeness of what was documented.\n    \n    The second paragraph should provide educational context by explaining why the missing or inadequate elements are clinically important for evaluating and managing a patient presenting with this specific chief concern. \n    Connect these documentation gaps to potential impacts on patient care, differential diagnosis, treatment planning, or patient safety. \n    Use this as a teaching opportunity to help the student understand the clinical reasoning behind thorough documentation.\n    \n    Here is the complete rubric on the student's performance: {completed_rubric}\n    \")\n}\n\nfeedback_prompt &lt;- build_feedback_prompt(analytic_rubric)\nfeedback_response &lt;- claude_plus(feedback_prompt,\n                                 temperature = 0)",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Activity: Chained Workflows</span>"
    ]
  },
  {
    "objectID": "activity-chained-workflows.html#task-generate-feedback-report",
    "href": "activity-chained-workflows.html#task-generate-feedback-report",
    "title": "22  Activity: Chained Workflows",
    "section": "",
    "text": "View Feedback Response\n\n\n\n\n\n\nCodelibrary(stringr)\nlibrary(knitr)\n\n# Some cleaning for the quarto output; not strictly necessary\nsource('downloads/format_for_qmd.R')\n\nfeedback_response &lt;- format_for_qmd(feedback_response)\n\nknitr::asis_output(feedback_response)\n\nConstructive Feedback on Clinical Case Documentation\nYour documentation demonstrates attention to several critical elements of this cardiovascular presentation. You appropriately identified the chief concern of chest pain and documented the episodic pattern of symptoms, which is essential for characterizing the temporal nature of cardiac events. You also captured the patient’s poorly controlled hypertension history and recorded vital signs indicating current hypertensive values. However, your documentation has significant gaps that could compromise patient care. Most critically, you failed to document whether the pain radiates to the back, which is a key distinguishing feature in chest pain evaluation. Additionally, and most concerning, you did not formulate or document a working diagnosis such as acute coronary syndrome (ACS), NSTEMI, or STEMI. Without documenting your clinical impression, there is no clear indication of your clinical reasoning process or the urgency level you assigned to this presentation.\nThe missing elements in your documentation have serious clinical implications for this patient presenting with chest pain. Radiation of chest pain to the back is a red flag symptom that could indicate aortic dissection—a life-threatening condition that can mimic ACS but requires completely different management (contraindicated thrombolytics, need for emergent imaging and surgical consultation). Failing to assess and document this characteristic could lead to catastrophic treatment errors. Furthermore, not documenting a working diagnosis of ACS/NSTEMI/STEMI represents a critical gap in clinical reasoning documentation. In a patient with chest pain, hypertension, and episodic symptoms, ACS must be at the top of your differential diagnosis, driving immediate interventions such as ECG interpretation, cardiac biomarkers, antiplatelet therapy, and cardiology consultation. Your documentation should reflect the urgency of the situation and guide other healthcare providers who may assume care. Complete documentation protects patient safety, ensures care continuity, and demonstrates your clinical decision-making process—all essential components of competent clinical practice.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Activity: Chained Workflows</span>"
    ]
  },
  {
    "objectID": "activity-chained-workflows.html#task-generate-instructor-report",
    "href": "activity-chained-workflows.html#task-generate-instructor-report",
    "title": "22  Activity: Chained Workflows",
    "section": "\n22.2 Task: Generate Instructor Report",
    "text": "22.2 Task: Generate Instructor Report\nNow that we have student feedback, let’s use this to provide information to the instructor about where they might need to improve their course materials or provide additional instruction / information / attention.\n\nCodebuild_remediation_prompt &lt;- function(feedback_output){\n  glue::glue(\"\n  You will analyze feedback that has been provided to a student about their performance. \n  Your task is to identify any potential gaps in the course materials or instruction that could be improved to further help the learning. \n  Based on the documentation deficiencies and missing clinical elements identified in the student feedback:\n  \n  1. Pattern Analysis: Determine whether these gaps likely reflect:\n  • Individual student oversight or understanding issues\n  • Systematic instructional gaps that may affect multiple students\n  • Unclear expectations in the assignment or rubric\n  \n  2. Curricular Recommendations: If the gaps suggest instructional needs, provide 2-3 specific, actionable recommendations for strengthening the course materials. \n  For each recommendation, specify:\n  • Which course component to enhance (e.g., lecture content, practice cases, rubric clarity, pre-assignment resources)\n  • What specific content or skill should be emphasized\n  • Why this would address the observed documentation gap\n  \n  3. Context Considerations: Note whether the missing elements are:\n  • Foundational knowledge that should have been covered previously\n  • Advanced concepts that may need more instructional time\n  • Clinical reasoning skills requiring additional practice opportunities\n\n  Format your response as a brief analysis followed by concrete action items that an instructor can implement.\n  \n  Complete this task using this feedback: {feedback_output}\n  \")\n}\n\nremediation_prompt &lt;- build_remediation_prompt(feedback_response)\nremediation_response &lt;- claude_plus(remediation_prompt,\n                                    temperature = 0)\n\n\n\n\n\n\n\n\nView Feedback Response\n\n\n\n\n\n\nCoderemediation_response &lt;- format_for_qmd(remediation_response)\n\nknitr::asis_output(remediation_response)\n\nAnalysis of Feedback and Instructional Gaps\nPattern Analysis\nThis feedback reveals systematic instructional gaps rather than simple student oversight. The missing elements fall into two critical categories:\n\n\nStructured clinical reasoning documentation - The absence of a working diagnosis suggests students may not understand that documentation must explicitly demonstrate their clinical thought process\n\nRed flag symptom assessment - Missing the back pain radiation inquiry indicates potential gaps in teaching systematic approaches to high-risk chief complaints\n\nThese are unlikely to be individual oversights because they represent fundamental clinical reasoning steps that should be automatic in chest pain evaluation. The pattern suggests students may be completing tasks (gathering some history, recording vitals) without understanding the clinical decision-making framework that should guide comprehensive assessment.\nCurricular Recommendations\nRecommendation 1: Enhance Pre-Assignment Resources with Clinical Reasoning Templates\nComponent: Pre-assignment preparatory materials **\nSpecific Content: ** - Provide a “Clinical Reasoning Documentation Framework” that explicitly shows: Chief Complaint → Systematic Assessment (including red flags) → Working Diagnosis → Clinical Rationale - Include annotated exemplars showing how experienced clinicians document their differential diagnosis thinking - Create a chest pain-specific checklist highlighting life-threatening differentials (ACS, aortic dissection, PE) with their distinguishing features\nRationale: Students appear to be documenting data collection without understanding they must also document clinical synthesis. This addresses the complete absence of working diagnosis documentation and provides scaffolding for systematic red flag assessment.**\nRecommendation 2: Integrate Deliberate Practice Cases for High-Risk Chief Complaints\nComponent: Practice cases with formative feedback **\nSpecific Content: - Develop 3-4 progressive chest pain scenarios requiring students to: (a) identify must-ask questions for life-threatening conditions, (b) formulate and document differential diagnoses, (c) explain clinical reasoning - Include cases where missing key questions (like back radiation) leads to different diagnoses - Provide immediate feedback showing consequences of incomplete assessment (e.g., “Without asking about back radiation, you cannot rule out aortic dissection”)\nRationale: This directly addresses the missing back pain radiation assessment by making the clinical consequences explicit and memorable. Repeated practice with feedback builds automatic inclusion of critical elements.**\nRecommendation 3: Clarify Rubric Expectations for Clinical Reasoning Documentation\nComponent: Assignment rubric and instructions **\nSpecific Content: - Add explicit rubric criteria: “Documents working diagnosis with supporting clinical rationale” (separate from data collection) - Include rubric item: “Assesses and documents red flag symptoms specific to chief complaint” - Provide rubric language: “Demonstrates systematic approach to ruling out life-threatening conditions” - In assignment instructions, explicitly state: “Your documentation must include your clinical impression/working diagnosis and the reasoning behind it”\nRationale: Students may believe their task is complete after data collection if expectations for clinical reasoning documentation aren’t explicit. This clarifies that documentation of thinking is as important as documentation of findings.**\nContext Considerations\nFoundational Knowledge Gaps: - Understanding that documentation serves as evidence of clinical reasoning (not just data recording) - this is foundational professional competency - Knowledge of red flag symptoms for common chief complaints - should be covered in basic clinical assessment courses\nAdvanced Skills Requiring More Support: - Differential diagnosis formulation in ambiguous presentations - this is an evolving skill requiring substantial practice - Translating clinical reasoning into clear, actionable documentation - bridges cognitive and communication skills\nClinical Reasoning Development: - The gap between knowing facts (aortic dissection exists) and applying systematic approaches (always assess back radiation in chest pain) requires deliberate practice with specific feedback - Students need multiple opportunities to practice the complete cycle: assess → synthesize → document reasoning → receive feedback on clinical logic\nImplementation Priority\nImmediate action: Clarify rubric expectations (Recommendation 3) - can be implemented before next assignment **\nShort-term: Develop clinical reasoning templates (Recommendation 1) - provides immediate scaffolding **\nOngoing development: Build practice case library (Recommendation 2) - most resource-intensive but highest impact for skill development**\nThe pattern suggests students may be approaching this as a documentation exercise rather than a clinical reasoning demonstration. All recommendations aim to make the expectation explicit: document not just what you found, but what you’re thinking and why.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Activity: Chained Workflows</span>"
    ]
  },
  {
    "objectID": "activity-chained-workflows.html#task-all-3-steps-at-once",
    "href": "activity-chained-workflows.html#task-all-3-steps-at-once",
    "title": "22  Activity: Chained Workflows",
    "section": "\n22.3 Task: All 3 Steps at Once!",
    "text": "22.3 Task: All 3 Steps at Once!\nLet’s put this all together in a single workflow. I’ve written a different simulated OSCE note so we can see the variation in what the model produces given a different basis for the workflow.\nI saved the functions that we used earlier so we can easily load them. This allows us to pay more attention to the chained workflow and not get caught up in the details of the lengthy prompt text. Saving the individual prompts as files to be loaded by source() later is a good practice because you can refine pieces of the chained workflow individually, catching errors and optimizing each step in the workflow. I also saved analytic_rubric in the build_osce_analytic_prompt file instead of repeating it here.\n\nCodesource('downloads/build_osce_analytic_prompt.R')\nsource('downloads/build_feedback_prompt.R')\nsource('downloads/build_remediation_prompt.R')\n\nosce_note_2 &lt;- \"\n45yo m presents with shortness of breath due to intermittent chest pain.\nReports that the pain more noticable on exertion this morning (walking up stairs). \nHypertensive, although on medication, suggesting it is poorly controlled.\nDiagnostic testing to rule out ACS should be completed first.\"\n\n\nanalytic_response_2 &lt;- claude_plus(\n  build_osce_analytic_prompt(osce_note_2, analytic_rubric),\n  temperature = 0)\nsave(analytic_response_2, file = 'data/analytic_response_2.Rdata')\n  \nfeedback_response_2 &lt;- claude_plus(\n  build_feedback_prompt(analytic_response_2),\n  temperature = 0)\nsave(feedback_response_2, file = 'data/feedback_response_2.Rdata')\n\nremediation_response_2 &lt;- claude_plus(\n  build_remediation_prompt(feedback_response_2),\n  temperature = 0)\nsave(remediation_response_2, file = 'data/remediation_response_2.Rdata')\n\nfull_report &lt;- paste(analytic_response_2,\n                     feedback_response_2,\n                     remediation_response_2,\n                     collapse = \"\\n\\n  ***  \\n\\n\")\n\nfull_report &lt;- format_for_qmd(full_report)\n\n\n\n\n\n\n\n\nView Workflow Output\n\n\n\n\n\n\nCodeknitr::asis_output(full_report)\n\nOSCE Post-Encounter Note Scoring Rubric\nCriterion 1: Chief concern of chest pain\nStatus: Included\nJustification: The note clearly states “45yo m presents with shortness of breath due to intermittent chest pain.” While shortness of breath is mentioned first, chest pain is explicitly identified as part of the presenting complaint.**\nCriterion 2: Episodic pattern of symptoms\nStatus: Included\nJustification: The note describes “intermittent chest pain” and specifies that “the pain more noticable on exertion this morning (walking up stairs),” which indicates an episodic pattern with exertional triggers.**\nCriterion 3: Poorly controlled history of hypertension\nStatus: Included\nJustification: The note explicitly states “Hypertensive, although on medication, suggesting it is poorly controlled,” which directly addresses both the history of hypertension and its poor control status.**\nCriterion 4: Vitals indicate hypertension\nStatus: Not Included\nJustification: While the note mentions the patient is hypertensive, there is no documentation of actual vital sign measurements (e.g., specific blood pressure readings) to support this finding.**\nCriterion 5: Pain radiates to the back\nStatus: Not Included\nJustification: There is no mention of pain radiation to the back or any other location in the note.**\nCriterion 6: Likely diagnosis of acute coronary syndrome (ACS), NSTEMI, or STEMI\nStatus: Included\nJustification: The note states “Diagnostic testing to rule out ACS should be completed first,” which indicates ACS is being considered as a likely diagnosis requiring immediate evaluation.**\n\nSummary: 4 of 6 criteria met ** Constructive Feedback**\nYour clinical documentation demonstrates several strengths, particularly in capturing the essential presenting complaint and relevant medical history. You clearly identified both the chest pain and shortness of breath, recognized the episodic and exertional nature of the symptoms, and appropriately documented the patient’s poorly controlled hypertension despite medication use. Most importantly, you correctly prioritized acute coronary syndrome (ACS) in your differential diagnosis and appropriately indicated the need for immediate diagnostic testing to rule out this life-threatening condition. However, there are two critical gaps in your documentation that significantly limit the completeness of your clinical assessment. First, you failed to document the actual vital sign measurements, particularly the blood pressure reading that would objectively demonstrate the degree of hypertension present during this acute presentation. Second, you did not document whether the chest pain radiates to the back or any other location—a key characteristic that was presumably elicited during your history-taking.\nThese omissions have important clinical implications for this patient presenting with chest pain. Documenting specific vital signs, especially blood pressure values, is essential because the degree of hypertension can help differentiate between diagnoses (such as hypertensive emergency, aortic dissection, or ACS) and guides immediate treatment decisions, including whether antihypertensive therapy should be initiated or adjusted during the acute presentation. The pattern of pain radiation is equally critical in this scenario—radiation to the back is a red flag symptom for aortic dissection, a catastrophic condition that can mimic ACS but requires completely different management. Administering thrombolytics or anticoagulation for presumed ACS in a patient with unrecognized aortic dissection could be fatal. In emergency presentations with chest pain, these specific details aren’t just documentation formalities; they are essential data points that directly impact your differential diagnosis, determine which diagnostic tests are ordered, and influence potentially life-saving treatment decisions. Always document objective measurements and specific symptom characteristics to ensure safe, appropriate care. * Analysis of Feedback and Instructional Gaps\nPattern Analysis\nThis feedback reveals systematic instructional gaps rather than simple student oversight. The student demonstrated strong clinical reasoning (correctly prioritizing ACS, recognizing the need for immediate testing) but failed to document two fundamental clinical elements:\n\n\nObjective vital sign values (specific BP measurements)\n\nCritical symptom characteristics (pain radiation pattern)\n\nThe disconnect between good clinical thinking and incomplete documentation suggests students may not fully understand: - Which specific data elements are non-negotiable in documentation - Why certain details have life-or-death clinical implications - How documentation directly drives differential diagnosis refinement\nThis pattern indicates students are learning clinical reasoning but not the parallel skill of translating clinical assessment into complete, actionable documentation.\n\nCurricular Recommendations\n*Recommendation 1: Create a “High-Stakes Documentation Checklist” Resource**\nComponent to enhance: Pre-assignment resources and rubric materials**\nSpecific content: Develop a symptom-specific documentation checklist for common chief complaints (chest pain, shortness of breath, abdominal pain, headache). For chest pain specifically, create a visual checklist highlighting:** - Mandatory vital signs with rationale (BP values → differentiates dissection/hypertensive emergency) - Critical symptom descriptors (radiation pattern → rules in/out dissection) - “Cannot proceed without” documentation elements marked distinctly\nWhy this addresses the gap: Students need explicit guidance on which elements are clinically mandatory versus supplementary. The feedback shows the student captured history but missed critical specifics—a checklist bridges the gap between “taking a history” and “documenting decision-critical details.” This makes implicit expectations explicit.**\n\n*Recommendation 2: Integrate “Documentation Consequences” Case Scenarios**\nComponent to enhance: Lecture content and practice cases**\nSpecific content: Add 2-3 brief case vignettes demonstrating adverse outcomes from incomplete documentation:** - Case A: Chest pain documented without radiation pattern → aortic dissection missed, anticoagulation given, patient deteriorates - Case B: “Elevated BP” documented without values → treatment delayed, hypertensive emergency progresses\nFollow each vignette with guided reflection: “What specific documentation element was missing? What was the clinical consequence? How does this change your documentation approach?”\nWhy this addresses the gap: The feedback emphasizes clinical implications, but students may not viscerally understand the connection between documentation omissions and patient harm. Concrete examples of “what goes wrong” when specific elements are missing transforms documentation from a bureaucratic task to a patient safety imperative.**\n\n*Recommendation 3: Add “Documentation Translation” Practice Exercises**\nComponent to enhance: Practice cases with structured feedback**\nSpecific content: Provide 3-4 practice scenarios where students receive a clinical vignette with complete information, then must:** 1. Identify which elements are “mandatory to document” vs. “supplementary” 2. Write the documentation 3. Receive immediate feedback comparing their documentation against an expert exemplar with annotations explaining why each element matters clinically\nInclude specific practice with vital sign documentation (writing actual values, not interpretations) and symptom characterization (specific descriptors, not generalizations).\nWhy this addresses the gap: This student likely heard the information during history-taking but didn’t recognize it as documentation-critical. Deliberate practice in translating clinical data into complete documentation—with immediate feedback—builds the habit of capturing specific, actionable details rather than general impressions.**\n\nContext Considerations\nFoundational knowledge requiring reinforcement: - Documentation of objective measurements (vital signs as numbers, not interpretations) - This is basic clinical documentation but may need explicit re-teaching in the context of decision-making\nClinical reasoning skills requiring additional practice: - Connecting specific documentation elements to differential diagnosis refinement (e.g., pain radiation → changes DDx from ACS alone to ACS vs. dissection) - Understanding documentation as a clinical reasoning tool, not just a record-keeping task\nAdvanced concept needing more emphasis: - The concept of “red flag symptoms” and their documentation priority - How specific details change management pathways (dissection vs. ACS requiring opposite treatments)\n\nImplementation Priority\nImmediate action: Implement Recommendation 1 (checklist) before the next assignment—low effort, high impact.**\nShort-term: Develop Recommendation 3 (practice exercises) for ongoing skill-building.**\nCurriculum revision: Integrate Recommendation 2 (consequence cases) into core lecture",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Activity: Chained Workflows</span>"
    ]
  },
  {
    "objectID": "activity-chained-workflows.html#task-brainstorm-a-chained-workflow",
    "href": "activity-chained-workflows.html#task-brainstorm-a-chained-workflow",
    "title": "22  Activity: Chained Workflows",
    "section": "\n22.4 Task: Brainstorm a Chained Workflow",
    "text": "22.4 Task: Brainstorm a Chained Workflow\nNow it’s time for you to think about how you can incorporate a chained workflow into some of your processes at your organization / institution! What sophisticated tasks / workflows could you use a generative AI chained workflow to automate? In what steps does the model output flow directly into another model call? Are there places where you’d like to have human review before the model completes additional steps?\nFor this activity, identify at least 3 steps where a generative AI model could be useful in your workflow.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Activity: Chained Workflows</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html",
    "href": "activity-prompt-engineering.html",
    "title": "16  Activity: Prompt Engineering",
    "section": "",
    "text": "16.1 Task 1: Prompt Formulas\nIt’s time to practice prompt engineering!\nPick a particular use case and try fitting that use case into the 3 different prompt formulas:\nFor my example, I’m going to use the use case of explaining test reliability to K-12 educators who do not have a psychometrics background and want to better understand why student test scores vary between test administrations.\nWhat did you notice when writing the prompts? What information was more easily apparent that needed to be included in the prompt by using the different formulas? Which one did you like best, and why?",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#task-1-prompt-formulas",
    "href": "activity-prompt-engineering.html#task-1-prompt-formulas",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.2 Understanding Test Reliability",
    "text": "Role + Task + Output\nAction, Context, Tone (ACT) Formula\n\nThe C-A-R-E Framework\n\nContext: [situation]\nAction: [what to do]\nResult: [desired output]\nExplanation: [why or how]\n\n\n\n\n\n\n\n\n\n\nRole + Task + Output\n\n\n\n\n\n\nCodesource('downloads/claude_plus.R')\nsource('downloads/format_for_qmd.R')\n\nrto_prompt &lt;- \n  \"Role: You are en educational measurement specialist.\n   \n   Task: Explain the concept of *test reliability* to K–12 educators who are unfamiliar with psychometrics and want to better understand why student scores sometimes vary between test administrations.\n   \n  Output: Provide a concise explanation (2-3 short paragraphs) followed by one classroom-based example in bullet form.\"\n\nreliability_rto &lt;- claude_plus(rto_prompt)\nreliability_rto &lt;- format_for_qmd(reliability_rto)\n\n\n\nCodeknitr::asis_output(reliability_rto)\n\n\n16.2 Understanding Test Reliability\nTest reliability refers to the consistency of test scores—whether a student would get similar results if they took the same test (or an equivalent version) multiple times under similar conditions. Think of reliability like a bathroom scale: a reliable scale gives you nearly the same weight each time you step on it within a few minutes, while an unreliable scale might show wildly different numbers. A perfectly reliable test would give students identical scores every time, but in reality, all tests have some amount of measurement error that causes scores to fluctuate somewhat.\nSeveral factors affect reliability and explain why student scores vary between administrations. These include student factors (fatigue, anxiety, illness, or guessing), test factors (unclear questions, ambiguous wording, or too few items), and administration factors (distracting testing environments or inconsistent timing). A highly reliable test minimizes these sources of error, though some variation is inevitable. Generally, longer tests, clearly written items, and standardized administration procedures improve reliability.\nIt’s important to note that reliability doesn’t mean the test is measuring the right thing (that’s validity)—it simply means the measurement is consistent. Understanding reliability helps educators interpret test scores more thoughtfully and recognize that a single test score is actually a range or “band” rather than a precise, unchanging number.\n\n16.3 Classroom Example\nScenario: A teacher gives a 10-question Friday spelling quiz and notices one student’s scores vary widely week to week (90%, 60%, 85%, 70%).\n\n\nPossible reliability issues:\n\nToo few items (only 10 questions means each one heavily impacts the score)\nInconsistent testing conditions (noisy classroom some weeks, quiet others)\nStudent-level factors (didn’t sleep well, was anxious, or guessed strategically)\n\n\n\nHow to improve reliability: Increase to 20 questions, ensure consistent quiet environment, give clear instructions, and look at patterns across multiple quizzes rather than one score\n\n\n\n\n\n\n\n\n\n\n\n\n“ACT” (Action - Context - Tone)\n\n\n\n\n\n\nCodeact_prompt &lt;- \"\n  Action: Explain the concept of test reliability.\n  \n  Context: You’re writing to K–12 educators who are unfamiliar with psychometrics and want to better understand why student scores sometimes vary between test administrations.\n\n  Tone: Friendly, conversational, and encouraging — as if you’re helping teachers connect a familiar classroom experience to an underlying measurement idea.\"\n\nreliability_act &lt;- claude_plus(act_prompt)\nreliability_act &lt;- format_for_qmd(reliability_act)\n\n\n\nCodeknitr::asis_output(reliability_act)\n\nUnderstanding Test Reliability: Why Scores Can Vary\nHave you ever had a student score 75% on a quiz one day, then take a very similar version the next week and get 82%? Or maybe you’ve wondered why a student’s reading level seems to jump around from one assessment to another? You’re definitely not imagining things—and there’s actually a scientific concept that explains this: test reliability.\nWhat Is Test Reliability?\nThink of test reliability as consistency. A reliable test is like a dependable bathroom scale—if you step on it multiple times in a row, you should get pretty much the same reading each time (assuming your weight hasn’t actually changed).\nIn testing terms, reliability asks: “If we could test the same student on the same content under similar conditions, how similar would their scores be?”\nWhy Don’t Students Get the Exact Same Score Every Time?\nGreat question! Even when a student’s actual knowledge hasn’t changed much, scores can vary for lots of everyday reasons:\n\n\nThey guessed differently on multiple-choice questions\n\nMood and energy levels varied (tired on Monday, energized on Friday)\n\nTest anxiety hit harder one day than another\n\nSlightly different questions tapped into what they happened to study\n\nEnvironmental factors like noise, temperature, or distractions\n\nAttention and focus naturally fluctuate\n\nThis variation is sometimes called “measurement error”—not because anyone made a mistake, but because no test can be a perfect snapshot of what a student knows.\nWhat Makes a Test More Reliable?\nYou can think of reliability on a spectrum. More reliable tests tend to:\n\n\nHave more questions (a 40-question test is generally more reliable than a 10-question quiz)\n\nHave clear, well-written items that students interpret consistently\n\nBe administered under standardized conditions (same directions, timing, environment)\n\nFocus on stable knowledge rather than rapidly changing skills\n\nWhat This Means for Your Classroom\nUnderstanding reliability can actually be pretty empowering:\n✅ Don’t over-interpret small score differences. If a student’s score moves from 78% to 83%, that might just be normal variation, not necessarily real growth.\n✅ Look for patterns over time. Multiple data points give you a much clearer picture than any single test.\n✅ Use multiple measures. Combining tests with observations, projects, and classwork gives you a more complete (and reliable) understanding of student learning.\n✅ Be thoughtful about high-stakes decisions. When something really matters—like grade promotion or special education placement—one test score shouldn’t be the only factor.\nThe Bottom Line\nTest reliability reminds us that every assessment is a sample of student knowledge taken at a particular moment in time. Just like one snapshot doesn’t tell someone’s whole life story, one test score doesn’t capture everything a student knows.\nThe good news? You already knew this intuitively! You’ve always known that students have good days and bad days, and that the full picture of their learning comes from watching them over time. Test reliability just gives you a framework for understanding why—and reassures you that you’re right to consider multiple sources of evidence when making decisions about your students.\n\n\n\n\n\n\n\n\n\n\n\n“CARE” (Context - Action- Result - Explanation)\n\n\n\n\n\n\nCodecare_prompt &lt;- \"\n  Context: Teachers have noticed that their students’ scores fluctuate across testing sessions and are unsure what that means.\n\n  Action: Explain the concept of test reliability in a way that helps K–12 educators unfamiliar with psychometrics make sense of these score variations.\n\n  Result: They should understand that reliability reflects the consistency of test scores and why it matters for interpreting student performance.\n\n  Explanation: Include one concrete example that links reliability to real classroom assessment practices.\"\n\nreliability_care &lt;- claude_plus(care_prompt)\nreliability_care &lt;- format_for_qmd(reliability_care)\n\n\n\nCodeknitr::asis_output(reliability_care)\n\nUnderstanding Test Reliability: Why Student Scores Fluctuate\nWhat Is Reliability?\nThink of test reliability as consistency. A reliable test works like a dependable ruler—it measures student knowledge the same way every time. Just as a good ruler always shows the same length for your desk, a reliable test should give similar scores for a student whose knowledge hasn’t changed.\nWhy Scores Fluctuate\nStudent scores naturally vary for several reasons:\nMeasurement Error (the “noise” in testing):** - A student was distracted during one test - They guessed correctly on multiple-choice questions - The test happened to include topics they just reviewed - They felt sick or anxious on test day\nTrue Change (what we want to measure):** - The student actually learned more material - Skills genuinely improved through practice\nA reliable test minimizes the first type (random fluctuation) so you can trust that score changes reflect real learning differences.\nConcrete Classroom Example: Weekly Spelling Tests\nImagine you give 10-word spelling tests every Friday:\nLow Reliability Scenario: - Week 1: Words are simple (cat, dog, run) - Week 2: Words are complex (encyclopedia, conscientious) - Week 3: Medium difficulty (kitchen, butterfly)\nEmma scores 90%, then 40%, then 70%. Is she improving or declining? You can’t tell because the test difficulty keeps changing—it’s unreliable.\nHigh Reliability Scenario: - All tests have similar difficulty levels - Same format and length each week - Consistent testing conditions\nNow Emma scores 60%, 65%, 70%. You can be more confident she’s genuinely improving because the test consistently measures the same thing.\nWhat This Means for Your Practice\nQuestions to Ask About Your Assessments:\n\n\nAm I testing the same content/skills the same way?\n\nDifferent question formats can produce different scores even if knowledge is the same\n\n\n\nAre testing conditions consistent?\n\nTime of day, room noise, and time limits all matter\n\n\n\nIs the test long enough?\n\nVery short tests (3-5 questions) are less reliable than longer ones (20+ questions)\nLike flipping a coin: 3 flips might look weird, but 100 flips shows the pattern\n\n\n\nPractical Implications:\nFor high-stakes decisions (promotion, program placement):** - Use multiple assessments over time - Don’t rely on a single test score - Look for patterns, not one-time results\nFor everyday formative assessment: - Some variability is normal and expected - Focus on trends rather than day-to-day changes - A 3-point difference might just be “noise”\nThe Bottom Line\nReliability doesn’t mean every student gets the same score every time—that would mean no one learns anything! Instead, it means:**\n\nIf a student’s knowledge stays the same, their scores should stay similar\nIf scores change significantly, you can trust it reflects real change in understanding\nYou can make confident decisions about instruction and intervention\n\nWhen you notice scores fluctuating, ask: “Is this test reliable enough to trust these differences?” If not, consider making your assessments more consistent before drawing conclusions about student learning.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#understanding-test-reliability",
    "href": "activity-prompt-engineering.html#understanding-test-reliability",
    "title": "16  Activity: Prompt Engineering",
    "section": "",
    "text": "Test reliability refers to the consistency of test scores—whether a student would get similar results if they took the same test (or an equivalent version) multiple times under similar conditions. Think of reliability like a bathroom scale: a reliable scale gives you nearly the same weight each time you step on it within a few minutes, while an unreliable scale might show wildly different numbers. A perfectly reliable test would give students identical scores every time, but in reality, all tests have some amount of measurement error that causes scores to fluctuate somewhat.\nSeveral factors affect reliability and explain why student scores vary between administrations. These include student factors (fatigue, anxiety, illness, or guessing), test factors (unclear questions, ambiguous wording, or too few items), and administration factors (distracting testing environments or inconsistent timing). A highly reliable test minimizes these sources of error, though some variation is inevitable. Generally, longer tests, clearly written items, and standardized administration procedures improve reliability.\nIt’s important to note that reliability doesn’t mean the test is measuring the right thing (that’s validity)—it simply means the measurement is consistent. Understanding reliability helps educators interpret test scores more thoughtfully and recognize that a single test score is actually a range or “band” rather than a precise, unchanging number.",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#classroom-example",
    "href": "activity-prompt-engineering.html#classroom-example",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.3 Classroom Example",
    "text": "16.3 Classroom Example\nScenario: A teacher gives a 10-question Friday spelling quiz and notices one student’s scores vary widely week to week (90%, 60%, 85%, 70%).\n\n\nPossible reliability issues:\n\nToo few items (only 10 questions means each one heavily impacts the score)\nInconsistent testing conditions (noisy classroom some weeks, quiet others)\nStudent-level factors (didn’t sleep well, was anxious, or guessed strategically)\n\n\n\nHow to improve reliability: Increase to 20 questions, ensure consistent quiet environment, give clear instructions, and look at patterns across multiple quizzes rather than one score",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#task-2-progressive-contraints",
    "href": "activity-prompt-engineering.html#task-2-progressive-contraints",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.4 Task 2: Progressive Contraints",
    "text": "16.4 Task 2: Progressive Contraints\nLet’s now do a similar activity - that of adding progressive constraints. This process probably most closely mirrors how I iterate and refine prompts before I start to empirically test them.\nStart with a vague prompt, and then add another bit of detail and see how this changes the output. And then another bit of detail, see how it changes the output, and so on. I’ll do this so there are a total of 4 interactions with the model.\nI’ll also add a system prompt of “You are a measurement educator who explains things concisely in a single paragraph” to decrease the amount of output you’ll have to read through. 😅\n\n\n\n\n\n\nProgressive 1\n\n\n\n\n\n\nCodeprogressive_1 &lt;- \"Explain test validity.\"\n\nvalidity_p1 &lt;- claude_plus(progressive_1,\n                           system = \"You are a measurement educator who explains things concisely in a single paragraph\")\n\nvalidity_p1 &lt;- format_for_qmd(validity_p1)\n\n\n\nCodeknitr::asis_output(validity_p1)\n\nTest validity refers to the extent to which a test actually measures what it claims or intends to measure, and whether the interpretations and uses of test scores are appropriate and meaningful. A valid test produces results that accurately reflect the construct, skill, or knowledge being assessed. There are several types of validity evidence: content validity (whether test items adequately represent the content domain), criterion validity (how well test scores relate to external criteria, either concurrently or predictively), and construct validity (whether the test measures the theoretical concept it’s supposed to measure, supported by convergent and discriminant evidence). For example, a math achievement test has high validity if it truly measures mathematical ability rather than reading comprehension, and if scores can meaningfully predict future math performance or correlate with other math assessments. Validity is not a fixed property but rather an ongoing evaluation of evidence supporting specific interpretations and uses of test scores in particular contexts.\n\n\n\n\n\n\n\n\n\n\n\nProgressive 2\n\n\n\n\n\n\nCodeprogressive_2 &lt;- \"Explain test validity to K–12 educators who are not familiar with psychometrics.\"\n\nvalidity_p2 &lt;- claude_plus(progressive_2,\n                           system = \"You are a measurement educator who explains things concisely in a single paragraph\")\n\nvalidity_p2 &lt;- format_for_qmd(validity_p2)\n\n\n\nCodeknitr::asis_output(validity_p2)\n\nTest validity is the extent to which a test actually measures what it’s supposed to measure and supports the inferences or decisions you want to make from the scores. Think of it this way: if you give students a math word problem test that requires advanced reading skills, low scores might reflect reading difficulties rather than math ability—that’s a validity problem. Validity isn’t just about the test itself, but about how you interpret and use the results. For example, a valid reading comprehension test might not be valid for identifying students who need special education services without additional evidence. There are different types of validity evidence educators should consider: content validity (does the test cover the important material you taught?), criterion validity (do test scores relate to other measures of the same skill, like grades or state assessments?), and construct validity (does the test measure the underlying skill or concept you care about?). When selecting or creating tests, ask yourself: “Will this test give me accurate information about what my students actually know or can do, and will it help me make good decisions about their learning?” If the answer is yes, you’re likely dealing with a valid assessment for your purpose.\n\n\n\n\n\n\n\n\n\n\n\nProgressive 3\n\n\n\n\n\n\nCodeprogressive_3 &lt;- \"Explain test validity to K–12 educators unfamiliar with psychometrics, focusing on helping them understand why some test results may not reflect true student ability.\"\n\nvalidity_p3 &lt;- claude_plus(progressive_3,\n                           system = \"You are a measurement educator who explains things concisely in a single paragraph\")\n\nvalidity_p3 &lt;- format_for_qmd(validity_p3)\n\n\n\nCodeknitr::asis_output(validity_p3)\n\nTest validity is about whether a test actually measures what it’s supposed to measure and whether we can trust the conclusions we draw from scores. Imagine giving students a math word problem test that requires advanced reading skills—low scores might reflect reading difficulties rather than math ability, making it an invalid measure of math knowledge. Similarly, a science test given on a student’s first week in a new country might not validly measure their science understanding if language barriers prevent them from showing what they know. Test results can also be misleading when students experience severe test anxiety, lack motivation, have a bad day, or when the test content doesn’t align with what was actually taught. Cultural bias in test items, unclear directions, or time limits that disadvantage certain learners can all threaten validity. This is why it’s crucial to look at multiple sources of evidence—classwork, projects, observations, and different types of assessments—rather than relying on a single test score to understand what students truly know and can do. When a test score seems inconsistent with what you observe in the classroom, it’s often a validity issue: the test simply isn’t capturing the student’s actual abilities due to factors unrelated to the skill being measured.\n\n\n\n\n\n\n\n\n\n\n\nProgressive 4\n\n\n\n\n\n\nCodeprogressive_4 &lt;- \"Explain test validity to K–12 educators unfamiliar with psychometrics, focusing on helping them understand why some test results may not reflect true student ability. Write in plain language suitable for a short teacher newsletter and avoid using the words “psychometrics” or “construct.”\"\n\nvalidity_p4 &lt;- claude_plus(progressive_4,\n                           system = \"You are a measurement educator who explains things concisely in a single paragraph\")\n\nvalidity_p4 &lt;- format_for_qmd(validity_p4)\n\n\n\nCodeknitr::asis_output(validity_p4)\n\nUnderstanding Test Validity: Does the Test Really Measure What We Think It Does?\nTest validity is about whether a test actually measures what it’s supposed to measure. Think of it like using a thermometer to check if someone has a fever—if the thermometer is broken or you’re holding it wrong, the reading won’t tell you the truth about their health. The same thing happens with student tests. A math test might give us misleading results if it requires so much reading that struggling readers can’t show their actual math skills. A science test taken on a student’s first difficult day back after illness might not reflect what they truly know. Tests can also fall short when they’re designed for one group of students but used with others who have different backgrounds or experiences. Even the testing environment matters—noise, time pressure, or anxiety can prevent students from demonstrating their real abilities. This is why we should never rely on a single test score to make important decisions about a student. Valid assessment means looking at multiple sources of evidence over time, considering each student’s unique circumstances, and always asking ourselves: “Is this test result giving us an accurate picture of what this student actually knows and can do?”",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#task-3-including-examples-zero-shot-v-few-shot",
    "href": "activity-prompt-engineering.html#task-3-including-examples-zero-shot-v-few-shot",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.5 Task 3: Including Examples (Zero-shot v Few-shot)",
    "text": "16.5 Task 3: Including Examples (Zero-shot v Few-shot)\nThis activity demonstrates how large language models can learn implicitly from examples provided within the prompt, even when the task involves completely unfamiliar or made-up categories (e.g., information not well represented in their training data).\nIn a zero-shot setting, the model receives only instructions, so it must guess without any contextual grounding.\nIn a few-shot setting, however, the model can observe patterns in the examples — inferring a latent rule or decision boundary and then applying that inferred rule to new inputs.\n\n\n\n\n\n\nZero-Shot\n\n\n\n\n\n\nCodemake_zero &lt;- \"Classify the following sentences as wamples or doglets:\n\n1. The concert was amazing and everyone was smiling.\n\n2. The student felt frustrated after failing the exam.\n\n3. The sunset filled the sky with brilliant colors.\n\n4. The meeting dragged on and everyone was bored.\n\nRespond _only_ with the sentence and either (wample) or (doglet) after the sentence.\"\n\nzeroshot &lt;- claude_plus(make_zero)\nzeroshot &lt;- format_for_qmd(zeroshot)\n\n\n\nCodeknitr::asis_output(zeroshot)\n\n\nThe concert was amazing and everyone was smiling. (wample)\nThe student felt frustrated after failing the exam. (doglet)\nThe sunset filled the sky with brilliant colors. (wample)\nThe meeting dragged on and everyone was bored. (doglet)\n\n\n\n\n\n\n\n\n\n\n\n\nFew-Shot\n\n\n\n\n\n\nCodemake_few &lt;- \"\nA doglet sentence describes a pleasant or positive experience.\nA wample sentence describes an unpleasant or negative experience.\n\nExamples:\n– The people were having an enjoyable day. (doglet)\n– It was raining and the woman was sad. (wample)\n– The person was happy to be eating their favorite food. (doglet)\n– The person had a stomach ache after eating too fast. (wample)\n\nClassify the following sentences as wamples or doglets:\n\n1. The concert was amazing and everyone was smiling.\n\n2. The student felt frustrated after failing the exam.\n\n3. The sunset filled the sky with brilliant colors.\n\n4. The meeting dragged on and everyone was bored.\n\nRespond _only_ with the sentence and either (wample) or (doglet) after the sentence.\"\n\nfewshot &lt;- claude_plus(make_few)\nfewshot &lt;- format_for_qmd(fewshot)\n\n\n\nCodeknitr::asis_output(fewshot)\n\n\nThe concert was amazing and everyone was smiling. (doglet)\nThe student felt frustrated after failing the exam. (wample)\nThe sunset filled the sky with brilliant colors. (doglet)\nThe meeting dragged on and everyone was bored. (wample)",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#task-4-same-prompt-different-system-prompts",
    "href": "activity-prompt-engineering.html#task-4-same-prompt-different-system-prompts",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.6 Task 4: Same Prompt, Different System Prompts",
    "text": "16.6 Task 4: Same Prompt, Different System Prompts\nteaching: first-year undergrad, grad student, expert (use 3-paragraphs on the relationship of statistics and generative AI models)\nintro student, graduate student\nnovice item writer, expert item writer",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#task-5-output-format",
    "href": "activity-prompt-engineering.html#task-5-output-format",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.7 Task 5: Output Format",
    "text": "16.7 Task 5: Output Format\n🧮 4. Schema or Output Control\nAsk the model to produce outputs in a structured format (e.g., JSON, Markdown table, CSV-ready). Then have students parse the response in R and see which prompts produce parseable output. Goal: practice output constraints for R integration.\n\nMost important for non-conversational interactions with models.\nIn conversational interactions you can provide feedback. ###################",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#task-x-prompt-improvement",
    "href": "activity-prompt-engineering.html#task-x-prompt-improvement",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.8 Task X : Prompt Improvement",
    "text": "16.8 Task X : Prompt Improvement\n\nStep 1: Write your own prompt - whatever topic you’d like, or re-use one that we’ve already used today. Don’t submit the prompt to the model yet.\nStep 2: Using the prompt below to ask the model for help with your prompt.\n\nI’d like your help improving the prompt below. Please review it and suggest ways to make it clearer and more effective. Specifically: • Identify any missing context or details that would help generate better results • Point out areas where the prompt might be vague or ambiguous • Suggest how to better structure the request • Recommend any information I should add about my goals, audience, or desired format Here is my prompt: {paste your prompt here}\n\nCodechat &lt;- chat_anthropic()\n\nchat$chat(\"I'd like your help improving the prompt below. Please review it and suggest ways to make it clearer and more effective.\n\nSpecifically:\n•   Identify any missing context or details that would help generate better results\n•   Point out areas where the prompt might be vague or ambiguous\n•   Suggest how to better structure the request\n•   Recommend any information I should add about my goals, audience, or desired format\n\nHere is my prompt:\n\nWhat are the pros and cons of Bayesian vs Frequentist statistics? For each point, include a brief explanation.\")\n\n\n\n\n\n\n\n\nBayesian v Frequentist Statistics Prompt Revision\n\n\n\n\n\nYour prompt is a good start, but here are several ways to make it clearer and more effective:\n\n16.9 Current Issues:\nVague/Ambiguous Areas: - “Brief explanation” is subjective - what length do you consider brief? - No specification of technical depth or mathematical complexity - Unclear what aspects of the comparison matter most to you - No guidance on format or structure preferences\nMissing Context: - Your background/expertise level in statistics - Intended audience for this information - Specific use case or application context - Whether you want practical examples or theoretical focus\n\n16.10 Improved Prompt Structure:\n“I need a comparison of Bayesian vs Frequentist statistical approaches for [YOUR CONTEXT - e.g., “a graduate-level biostatistics course” or “business decision-making presentation”].\nBackground: [YOUR EXPERIENCE LEVEL - e.g., “I have basic statistical knowledge but am new to these philosophical differences”]\nFormat requested: - 4-6 key pros and cons for each approach - Each point explained in 2-3 sentences - [Include/exclude mathematical notation] - Include 1-2 practical examples where each approach excels\nFocus areas I’m most interested in: - [e.g., Practical implementation, philosophical differences, computational requirements, interpretation of results]\nAudience: [e.g., “Non-statisticians who need to understand methodology choices” or “Statistics students preparing for exams”]”\n\n16.11 Recommended Additions:\n\n\nSpecify your goal - Are you choosing between methods, teaching others, or writing a paper?\n\nDefine scope - Do you want to cover computational aspects, philosophical foundations, or practical applications?\n\nSet constraints - Word count, technical level, specific statistical contexts (hypothesis testing, parameter estimation, etc.)\n\nWould you like to revise your prompt with some of these suggestions?\n\n\n\n\nWhat improvements did the model suggest? Did it identify anything you hadn’t considered? Was the feedback helpful or too generic? How would you revise your prompt based on the suggestions? If you used the prompt before, did you get a better result using the revised prompt suggested by AI?",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#current-issues",
    "href": "activity-prompt-engineering.html#current-issues",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.9 Current Issues:",
    "text": "16.9 Current Issues:\nVague/Ambiguous Areas: - “Brief explanation” is subjective - what length do you consider brief? - No specification of technical depth or mathematical complexity - Unclear what aspects of the comparison matter most to you - No guidance on format or structure preferences\nMissing Context: - Your background/expertise level in statistics - Intended audience for this information - Specific use case or application context - Whether you want practical examples or theoretical focus",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#improved-prompt-structure",
    "href": "activity-prompt-engineering.html#improved-prompt-structure",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.10 Improved Prompt Structure:",
    "text": "16.10 Improved Prompt Structure:\n“I need a comparison of Bayesian vs Frequentist statistical approaches for [YOUR CONTEXT - e.g., “a graduate-level biostatistics course” or “business decision-making presentation”].\nBackground: [YOUR EXPERIENCE LEVEL - e.g., “I have basic statistical knowledge but am new to these philosophical differences”]\nFormat requested: - 4-6 key pros and cons for each approach - Each point explained in 2-3 sentences - [Include/exclude mathematical notation] - Include 1-2 practical examples where each approach excels\nFocus areas I’m most interested in: - [e.g., Practical implementation, philosophical differences, computational requirements, interpretation of results]\nAudience: [e.g., “Non-statisticians who need to understand methodology choices” or “Statistics students preparing for exams”]”",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "activity-prompt-engineering.html#recommended-additions",
    "href": "activity-prompt-engineering.html#recommended-additions",
    "title": "16  Activity: Prompt Engineering",
    "section": "\n16.11 Recommended Additions:",
    "text": "16.11 Recommended Additions:\n\n\nSpecify your goal - Are you choosing between methods, teaching others, or writing a paper?\n\nDefine scope - Do you want to cover computational aspects, philosophical foundations, or practical applications?\n\nSet constraints - Word count, technical level, specific statistical contexts (hypothesis testing, parameter estimation, etc.)\n\nWould you like to revise your prompt with some of these suggestions?",
    "crumbs": [
      "Transactional Interactions",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Activity: Prompt Engineering</span>"
    ]
  }
]