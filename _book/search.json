[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Integrating Generative AI into R Workflows: From APIs to Shiny Apps",
    "section": "",
    "text": "Welcome\nThis online book was created to be a user-friendly way to present the materials in support of the “Integrating Generative AI into R Workflows: From APIs to Shiny Apps” workshop, first given at AIMECON in October, 2025. Although created primarily for this purpose, I intend to contiually update these materials as I learn.\nChanges to the book last made on: 2025-10-12 21:06:27\n\n\nLearning Objectives\n\nBy the end of this workshop, participants will be able to:\n\nExplain key LLM architectural features that inform effective integration practices\nApply prompt engineering principles to achieve consistent, reliable outputs in R workflows\nImplement both single-use and conversational API interactions with LLMs from R\nDesign and deploy simple multi-agentic systems for complex tasks\nBuild interactive Shiny applications that leverage LLM capabilities for end-user functionality\n\n\n\n⚠️ Privacy Reminder\n\nPlease don’t enter any personal, confidential, or sensitive information into generative AI tools. This includes private details about yourself or others, as well as proprietary data. Assume that anything you input could be used to train future models or be visible to others. It is important for you to consult both your IT departments and legal counsel before using a generative AI model in any operational setting.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#workshop-goals",
    "href": "index.html#workshop-goals",
    "title": "AIMECON R Workshop",
    "section": "1.1 Workshop Goals",
    "text": "1.1 Workshop Goals\n\nLearn R fundamentals\nPractice data analysis\nBuild reproducible reports",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>AIMECON R Workshop</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction to R",
    "section": "",
    "text": "3 Getting Started with R\nThis chapter covers the basics of R programming.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "intro.html#installing-r",
    "href": "intro.html#installing-r",
    "title": "2  Introduction to R",
    "section": "3.1 Installing R",
    "text": "3.1 Installing R\nInstructions for installing R…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to R</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Workshop Summary",
    "section": "",
    "text": "4 Summary\nKey takeaways from the workshop…",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Workshop Summary</span>"
    ]
  },
  {
    "objectID": "06-reference-materials.html",
    "href": "06-reference-materials.html",
    "title": "Reference Materials",
    "section": "",
    "text": "Background Information on GPTs\nThese materials / links were last checked on October 2, 2025. All apologies for links that no longer work. Please email me at CRunyon@nbme.org if you notice something no longer works so I can change / remove the link.\nThe 3Blue1Brown YouTube Channel provides several good videos on the some of the technical aspects of large language models.\nAnthropic’s paper On the Biology of a Large Language Model is particularly interesting.\nThis post on lesswrong provides a nice high-level summary for understanding LLMs.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#background-information-on-gpts",
    "href": "06-reference-materials.html#background-information-on-gpts",
    "title": "Reference Materials",
    "section": "",
    "text": "The Neural Networks section is particularly informative.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#reference-guides-and-prompt-engineering",
    "href": "06-reference-materials.html#reference-guides-and-prompt-engineering",
    "title": "Reference Materials",
    "section": "Reference Guides (and Prompt Engineering)",
    "text": "Reference Guides (and Prompt Engineering)\nOpenAI Cookbook",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#shiny-related-resources",
    "href": "06-reference-materials.html#shiny-related-resources",
    "title": "Reference Materials",
    "section": "Shiny-related Resources",
    "text": "Shiny-related Resources\nPosit has many useful things.\n\nMaterials for the R language start here.\nMaterials for the Python language start here.\n\nnanxstats has nicely organized many Shiny extension packages.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "06-reference-materials.html#legal-considerations",
    "href": "06-reference-materials.html#legal-considerations",
    "title": "Reference Materials",
    "section": "Legal Considerations",
    "text": "Legal Considerations\nThe information provided here is for general informational purposes only and does not constitute legal advice. You should not act upon any information presented without first seeking qualified legal counsel regarding your specific situation. The authors disclaim any liability for actions taken based on the content provided here.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "00a-R-setup.html",
    "href": "00a-R-setup.html",
    "title": "2  R Setup",
    "section": "",
    "text": "2.1 Installing R\nThis workshop is based in the R programming language. As such, it is necessary to have a recent version installed on your computer. I would suggest R Version \\(\\geq\\) 4.4.\nThese workshop materials were built and tested using R version 4.5.1 (Great Square Root, released 2025-06-13), with all packages updated on or after October 5, 2025.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "00a-R-setup.html#installing-r-studio-desktop",
    "href": "00a-R-setup.html#installing-r-studio-desktop",
    "title": "2  R Setup",
    "section": "\n2.2 Installing R Studio Desktop",
    "text": "2.2 Installing R Studio Desktop\nPosit makes a wonderful integrated development environment (IDE) called R Studio Desktop. This is my preferred IDE for R, although you may choose others if you like. I will be using R Studio throughout the workshop.\nI am using R Studio Version 2025.09.1+401, which was installed on October 5, 2025.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "00a-R-setup.html#installing-r-tools",
    "href": "00a-R-setup.html#installing-r-tools",
    "title": "2  R Setup",
    "section": "\n2.3 Installing R Tools",
    "text": "2.3 Installing R Tools\nRTools is necessary to install or build R packages that require compilation of C, C++, or Fortran code, as it provides the necessary compiler toolchain and build tools. The packages that are available on CRAN do not require having RTools installed because the packages hosted there are precompiled binary packages.\nI’m unsure of which packages I’ve downloaded from Github or other sources that require compilation, so I recommend also installing at RTools 4.4. (And it’s just good to have in case you need it later.)",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>R Setup</span>"
    ]
  },
  {
    "objectID": "00b-R-packages.html",
    "href": "00b-R-packages.html",
    "title": "3  R Packages",
    "section": "",
    "text": "3.1 Workshop package\nR’s strength lies not just in its statistical capabilities, but in its open-source nature. R’s code is freely available for anyone to inspect, modify, and improve. This openness has cultivated a vibrant global community of statisticians, data scientists, and developers who actively contribute to the language’s evolution.\nThe R package ecosystem exemplifies this collaborative spirit. Community members can develop packages that address specific needs that span many academic domains (see Task Views). These packages allow you to utilize and build on classic and new analytical methods, advancing science and, thereby, public good.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Packages</span>"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html",
    "href": "01a-gen-ai-basics.html",
    "title": "5  Foundational Principles",
    "section": "",
    "text": "5.1 Tokens",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-learning.html",
    "href": "01b-gen-ai-learning.html",
    "title": "4  Learning More about Gen AI",
    "section": "",
    "text": "4.1 YouTube / Video Resources\nBrownBlue",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Learning More about Gen AI</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-learning.html#articles-and-blogs",
    "href": "01b-gen-ai-learning.html#articles-and-blogs",
    "title": "4  Learning More about Gen AI",
    "section": "4.2 Articles and Blogs",
    "text": "4.2 Articles and Blogs\nCool recent jawn",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Learning More about Gen AI</span>"
    ]
  },
  {
    "objectID": "02a-general-prompt-information.html",
    "href": "02a-general-prompt-information.html",
    "title": "6  General Prompt Engineering",
    "section": "",
    "text": "6.1 Including Context\nIntegrate AI Fundamentals information in this section.\nLink to promptingguide.ai here?",
    "crumbs": [
      "Prompt Engineering",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>General Prompt Engineering</span>"
    ]
  },
  {
    "objectID": "02b-prompt-formulas.html",
    "href": "02b-prompt-formulas.html",
    "title": "7  Example Prompt Formulas",
    "section": "",
    "text": "#Prompt Formulas”\nLink to examples here",
    "crumbs": [
      "Prompt Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Example Prompt Formulas</span>"
    ]
  },
  {
    "objectID": "02c-chained-workflows.html",
    "href": "02c-chained-workflows.html",
    "title": "8  Chained Workflows",
    "section": "",
    "text": "8.1 Prompt Chaining\nHistory in chain-of-thought models; evolved to workflows and reasoning models.",
    "crumbs": [
      "Prompt Engineering",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Chained Workflows</span>"
    ]
  },
  {
    "objectID": "03a-api-keys.html",
    "href": "03a-api-keys.html",
    "title": "10  API Keys",
    "section": "",
    "text": "10.1 What are these?",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>API Keys</span>"
    ]
  },
  {
    "objectID": "03b-prompting-models.html",
    "href": "03b-prompting-models.html",
    "title": "10  Prompting LLMs via API",
    "section": "",
    "text": "10.1 Single interactions with Models\nWhen are these appropriate?",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Prompting LLMs via API</span>"
    ]
  },
  {
    "objectID": "04a-conversations-via-api.html",
    "href": "04a-conversations-via-api.html",
    "title": "10  Conversational Models",
    "section": "",
    "text": "10.1 Continued interactions with models\nWhen are these appropriate?",
    "crumbs": [
      "The ellmer() package",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conversational Models</span>"
    ]
  },
  {
    "objectID": "04b-rag-models.html",
    "href": "04b-rag-models.html",
    "title": "11  Retrieval Augmented Generation",
    "section": "",
    "text": "11.1 What are RAG models?\nWhen are these appropriate?",
    "crumbs": [
      "The ellmer() package",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "05a-shiny.html",
    "href": "05a-shiny.html",
    "title": "12  Shiny",
    "section": "",
    "text": "12.1 Online Web Application\nCan be hosted locally\nWritten in either R or Python",
    "crumbs": [
      "Shiny Integration",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Shiny</span>"
    ]
  },
  {
    "objectID": "05b-vibe-coding.html",
    "href": "05b-vibe-coding.html",
    "title": "15  “Vibe Coding”",
    "section": "",
    "text": "15.1 Feel the Vibe!\nVibe Coding is the recent term of the art for using an LLM to help you write syntax, which includes online applications. The term was coined by Andrej Karpathy in a tweet on February 2, 2025 (ars technica article)\nReduces tedious syntax writing Focus on functionality",
    "crumbs": [
      "Shiny Integration",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>\"Vibe Coding\"</span>"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html",
    "href": "01-gen-ai-fundamentals.html",
    "title": "Generative AI Fundamentals",
    "section": "",
    "text": "What is Generative AI?\nIt is useful to learn some fundamental aspects about GenAI models prior to their use. I have often found that there are many guides online that tell you how to use these LLMs, but don’t tell you why you should use these LLMs. Although the guidance given by these guides is often useful, when your prompt isn’t resulting in the output you want it’s unclear how to make improvements because you don’t have the foundational knowledge of how the model might be processing your prompt.\nEven though these generative AI models continue to improve without extensive prompt engineering, this foundational knowledge will serve you well in building your own generative AI workflows.\nSo what is generative artificial intelligence? We often hear the words GPT, generative AI, AI, LLM, and machine learning models used somewhat interchangeably and that isn’t always necessarily the case. Sometimes the speaker may not understand that they’re referring to something they don’t intend to refer to, or sometimes the technical aspects of what exactly they’re referring to doesn’t quite matter.\nIn this diagram, I’ve attempted to show you the relationship between Artificial Intelligence, Natural Language Processing, and related fields, with Generative Artificial Intelligence in the middle. The scale of the circles in this image is not accurate – I’ve only attempted to show the relationships between some of these larger fields.\nWorking from the left-hand side inward, our outermost circle is Artificial Intelligence, or AI. Artificial intelligence is the broad field of computer science focused on creating systems that can perform tasks typically requiring human intelligence, such as problem-solving, reasoning, and understanding language. I’ve heard some people argue that statistical models like regressions can be included in this large circle because you’re learning about relationships in data that are not directly observable, but I’m not sure that I’m convinced of this argument, but it does give you an idea of how broad the field of AI can be construed.\nAs we start to go inward a little more, we have Machine Learning. Machine learning is a subset of AI that involves algorithms that learn patterns from data and improve their performance on tasks over time without being explicitly programmed for every possible scenario. This is different from a regression where you identify all the key predictor variables of interest based on your knowledge of the domain. With machine learning you just need to identify enough of the potentially relevant variables to maximize your predictions. Here is where we start to encounter so-called “black box” models because sometimes interpreting the decision-making processes of the algorithm becomes difficult due to the complex nature of the models.\nGoing further inward, we have Deep Learning, which is a subset of Machine Learning models. These models eliminate the need for identifying variables due to the large amount and variety of data that are supplied to the models. Deep Learning models use very complex algorithms to identify complex patterns in the data, and their results are often more uninterpretable. This is good for things like speech and natural language, where relationships between variables are not easily identified.\nI’ll now jump to the right-hand side and the large field of Natural Language Processing. Broadly speaking, natural language processing is focused on enabling computers to understand and interpret human language, and you can see it’s a very broad field of study in and of itself. There are areas of NLP that don’t necessarily overlap with machine learning or deep learning, such as sentiment analysis or topic modeling which may not use Machine Learning or Deep Learning Models.\nIf we look at the overlap between deep learning models and natural language processing models, we see that Large Language Models – LLMS – live here. LLMs represent a deep learning approach within NLP that leverages vast amounts of text data to generate and understand human-like language. These models go beyond traditional NLP techniques by using statistical learning to predict and produce coherent, context-aware text at scale. Not all LLMs are generative AI models, such as language translation app.\nNow, finally, in the middle we have Generative AI Models (Gen AI). These models that are a specific type of LLM that are designed to generate output that model resemble their training data. The most common model is a generative text model, although models are now being developed that have the capability to generate images, audio, and video.",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#what-is-a-gpt",
    "href": "01-gen-ai-fundamentals.html#what-is-a-gpt",
    "title": "Generative AI Fundamentals",
    "section": "What is a GPT?",
    "text": "What is a GPT?",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#co-occurrence",
    "href": "01-gen-ai-fundamentals.html#co-occurrence",
    "title": "Generative AI Fundamentals",
    "section": "Co-occurrence",
    "text": "Co-occurrence",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#attention",
    "href": "01-gen-ai-fundamentals.html#attention",
    "title": "Generative AI Fundamentals",
    "section": "Attention",
    "text": "Attention",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#model-parameters",
    "href": "01-gen-ai-fundamentals.html#model-parameters",
    "title": "Generative AI Fundamentals",
    "section": "Model Parameters",
    "text": "Model Parameters\nOrganize into classes of parameters?\n\nTemperature\n\nGeneration based?\nChecking if 4 hashtags work\n\n\n\ntop_p\n\n\ntop_k\n\n\nmax tokens",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html",
    "href": "02-prompt-engineering.html",
    "title": "Prompt Engineering",
    "section": "",
    "text": "Prompt Engineering Importance\nWe’ll only cover a sampling of these methods in the workshop today because we’ll be focusing more on their application than extensively testing different prompting strategies, but I encourage you to further explore these techniques based on your specific use case.\nTie in to chapter 1",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#madlibs",
    "href": "02-prompt-engineering.html#madlibs",
    "title": "Prompt Engineering",
    "section": "Madlibs",
    "text": "Madlibs",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#prompt-formulas",
    "href": "02-prompt-engineering.html#prompt-formulas",
    "title": "Prompt Engineering",
    "section": "Prompt Formulas",
    "text": "Prompt Formulas",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#prompt-chaining",
    "href": "02-prompt-engineering.html#prompt-chaining",
    "title": "Prompt Engineering",
    "section": "Prompt Chaining",
    "text": "Prompt Chaining\n\nChain-of-thought Prompting\n\nInvisible Instructions\n\n\n\nManual Prompt Chaining",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "03-api-implementation.html",
    "href": "03-api-implementation.html",
    "title": "API Implementation",
    "section": "",
    "text": "Choosing a Generative AI Model\nArguably the most popular method for interacting with a generative AI model is a simple chatbot-based interface. These interfaces support a conversational interactions with LLM much like a text conversation or Teams chat (or AOL Instant Messaging). There is a text box, you enter a prompt, the model processes that prompt, and provides a response. And continue.\nThis method of interacting with a generative AI model has many advantages, the most salient of which is ease of interaction. It’s easy have a back-and-forth conversation, and many of the model providers have made it so you can continue old conversations or search through previous conversations.\nHowever, for certain tasks (e.g., repetitive tasks to be completed at scale), the chatbot interface can be inefficient. It can be time consuming to copy-cut-paste-submit-copy-cut-paste - and repeat - for the n number of times you need to complete a task. In these instances it may be better to interact with a model via an application programming interface (API).\nAn API is a structured way for one piece of software to communicate with another. When you use a generative AI model through its API, you’re programmatically sending requests with specific instructions and receiving structured responses—allowing you to integrate AI capabilities directly into your R scripts, automate repetitive tasks, and process data at scale. Unlike a chatbot interface where you manually type and read each exchange, the API allows your code to handle hundreds or thousands of interactions automatically, making it the foundation for building LLM-powered tools and workflows.",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03-api-implementation.html#r-packages-for-api-implementation",
    "href": "03-api-implementation.html#r-packages-for-api-implementation",
    "title": "API Implementation",
    "section": "R Packages for API Implementation",
    "text": "R Packages for API Implementation\nMany available. Covering a handful - saving special time for ellmer, which was created by Hadley Wickham and other amazing developers at Posit.",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03-api-implementation.html#system-prompt",
    "href": "03-api-implementation.html#system-prompt",
    "title": "API Implementation",
    "section": "System Prompt",
    "text": "System Prompt",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03-api-implementation.html#user-prompt",
    "href": "03-api-implementation.html#user-prompt",
    "title": "API Implementation",
    "section": "User Prompt",
    "text": "User Prompt\nWhat’s the difference?",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "04-ellmer.html",
    "href": "04-ellmer.html",
    "title": "The ellmer() package",
    "section": "",
    "text": "ellmer() is the GOAT\nImplements conversational AI easily, much like a chatbot.\nWhen good?\nWhen bad?",
    "crumbs": [
      "The ellmer() package"
    ]
  },
  {
    "objectID": "04-ellmer.html#first-thing",
    "href": "04-ellmer.html#first-thing",
    "title": "The ellmer() package",
    "section": "First Thing",
    "text": "First Thing",
    "crumbs": [
      "The ellmer() package"
    ]
  },
  {
    "objectID": "04-ellmer.html#second-thing",
    "href": "04-ellmer.html#second-thing",
    "title": "The ellmer() package",
    "section": "Second Thing",
    "text": "Second Thing",
    "crumbs": [
      "The ellmer() package"
    ]
  },
  {
    "objectID": "05-shiny-integration.html",
    "href": "05-shiny-integration.html",
    "title": "Shiny Integration",
    "section": "",
    "text": "What is Shiny?\nThis chapter covers the basics of R programming.",
    "crumbs": [
      "Shiny Integration"
    ]
  },
  {
    "objectID": "05-shiny-integration.html#vibe-coding",
    "href": "05-shiny-integration.html#vibe-coding",
    "title": "Shiny Integration",
    "section": "Vibe Coding",
    "text": "Vibe Coding\nV I B E C O D I N G\n\n\n\nThis is the image’s alt text.",
    "crumbs": [
      "Shiny Integration"
    ]
  },
  {
    "objectID": "00-preparation.html",
    "href": "00-preparation.html",
    "title": "Set Up",
    "section": "",
    "text": "This section will help you ensure that you are appropriately set up to complete the rest of the workshop.",
    "crumbs": [
      "Set Up"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html",
    "href": "00c-LLM-R-packages.html",
    "title": "3  LLM-specific R Packages",
    "section": "",
    "text": "3.1 ellmer\nA number of packages have been developed to more easily facilitate interacting with LLMs via R. Many of these packages are useful (we’ll cover some of those in the workshop), whereas other packages include some developer design decisions that don’t work particularly well for my usual workflows.\nBelow is a non-exhaustive list of packages that I’ve found to interact with LLMs. This is not meant to be exhaustive or a curated list; it’s only to provide you with information about the packages you’ll be using in the workshop (and others) in the case you find them helpful for your workflow. All package summaries were initially generated with AI. Some summaries have been edited, some have not.\nellmer Overview CRAN Documentation\nellmer is an R package that provides a unified interface for interacting with large language models from over 17 providers including OpenAI, Anthropic, Google Gemini, and AWS Bedrock. It supports advanced features like streaming outputs, tool/function calling, structured data extraction, and multimodal inputs. Chat objects are stateful and maintain conversation context, enabling both interactive console-based conversations and programmatic use in R scripts and applications.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#tidyprompt",
    "href": "00c-LLM-R-packages.html#tidyprompt",
    "title": "3  LLM-specific R Packages",
    "section": "3.2 tidyprompt",
    "text": "3.2 tidyprompt\ntidyprompt Overview CRAN Documentation\ntidyprompt is an R package that provides a compositional framework (“prompt wraps”) for building prompts enriched with logic, validation, and extraction functions when interacting with LLMs. It supports structured output, retry/feedback loops, reasoning strategies (e.g. ReAct or chain-of-thought), and even autonomous R code or function calling as part of an LLM dialogue. The package is provider-agnostic, meaning its features can layer on top of any chat completion API (e.g. via ellmer) to produce more robust, predictable interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#tidyllm",
    "href": "00c-LLM-R-packages.html#tidyllm",
    "title": "3  LLM-specific R Packages",
    "section": "3.3 tidyllm",
    "text": "3.3 tidyllm\ntidyllm Overview CRAN Documentation\ntidyllm provides a tidy, pipeline-friendly interface for interacting with multiple LLM APIs (e.g. Claude, OpenAI, Gemini, Mistral) and local models via Ollama. It supports multimodal inputs (text, images, PDFs), maintains conversational history, handles batching and rate limits, and allows structured schema-based extraction of responses. The design emphasizes composability and integration into typical R data workflows.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#chattr",
    "href": "00c-LLM-R-packages.html#chattr",
    "title": "3  LLM-specific R Packages",
    "section": "3.4 chattr",
    "text": "3.4 chattr\nchattr Overview CRAN Documentation\nchattr is an R package that enables interactive communication with large language models directly within RStudio using a Shiny gadget or from the console. It enriches prompts with contextual information (e.g. loaded data frames) and integrates with various back-ends (e.g. OpenAI, Copilot, local LlamaGPT) via the ellmer interface. The package is geared toward exploratory workflows and rapid prototyping of LLM-assisted analysis.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#llmagentr",
    "href": "00c-LLM-R-packages.html#llmagentr",
    "title": "3  LLM-specific R Packages",
    "section": "3.5 LLMAgentR",
    "text": "3.5 LLMAgentR\nLLMAgentR Overview CRAN Documentation\nLLMAgentR is an R package for constructing language model “agents” using a modular, graph-based execution framework inspired by LangChain/LangGraph architectures. It offers a suite of agent types (e.g. code generation, data wrangling, SQL agents, document summarization) that iteratively reason, generate R code, execute, debug, and explain results. The package aims to support reproducible AI workflows for analysis, research, and automation by integrating LLM reasoning and domain logic.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00c-LLM-R-packages.html#packetllm",
    "href": "00c-LLM-R-packages.html#packetllm",
    "title": "3  LLM-specific R Packages",
    "section": "3.6 PacketLLM",
    "text": "3.6 PacketLLM\nPacketLLM Overview CRAN Documentation\nPacketLLM offers an interactive RStudio gadget interface for chatting with OpenAI LLMs (e.g. GPT-5 and variants) directly within the R environment. It supports multiple simultaneous conversation tabs, file upload (e.g. .R, PDF, DOCX) as contextual input, and per-conversation system message configuration. API calls are handled asynchronously (via promises + future) to avoid blocking the R console during model interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#what-is-a-generative-pre-trained-transformer",
    "href": "01-gen-ai-fundamentals.html#what-is-a-generative-pre-trained-transformer",
    "title": "Generative AI Fundamentals",
    "section": "What is a Generative Pre-trained Transformer?",
    "text": "What is a Generative Pre-trained Transformer?\nGenerative Pre-trained Transformers (GPTs) are a type of generative AI model. While not all generative text models have “GPT” in their name, understanding their underlying structure can be helpful for learning how these models work. Let’s look at each letter in the acronym:\n\nGenerative\nThe G in GPT stands for generative, which refers to the model’s ability to generate new content based on its training data. :Make analogy to regression:\n\n\nPre-trained\nGenerative AI models undergo a vast amount of model pre-training (the P). In a generative text model, this includes a vast corpus of text data. To give you an idea of how large this training data is, when I was first learning about these models, I often heard presenters say some of these models were trained on all the publicly-available text on the internet. Having such a vast amount of data equips the model with a broad “understanding” of language and its nuances. This understanding is a recognition of the patterns of word usage across many contexts.\nIt is important to note that the models are made to generate content and do not have the ability to reflect on the factual accuracy of the information that is provided. If the training data contains extensive information that is related to the input, there’s a higher probability that it will be correct, but there is no guarantee. When using text models, these models are merely predicting the most likely next token in the output. There is no self-reflective step where the model evaluates whether the information being provided is factually accurate. When the model produces information that is not correct in some aspect, this is what is known as a “hallucination”.\n\n\nTransformer\nAnd, finally, T – transformer. This is a technical aspect of the model architecture. : More details and links for this audience. :\nThis blog post on Financial Times is a great visual explanation of transformers (and related concepts) transformers without being too technical.",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html#attention",
    "href": "01a-gen-ai-basics.html#attention",
    "title": "5  Foundational Principles",
    "section": "5.3 Attention",
    "text": "5.3 Attention\nThe transformer architecture1 revolutionized NLP. As of October 7, this paper had 197,775 citations on Google Scholar.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html#embeddings",
    "href": "01a-gen-ai-basics.html#embeddings",
    "title": "5  Foundational Principles",
    "section": "5.4 Embeddings",
    "text": "5.4 Embeddings\n\n\n\n\n\n\n1. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Advances in neural information processing systems. 2017;30.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html",
    "href": "01b-gen-ai-parameters.html",
    "title": "\n6  Generation Parameters\n",
    "section": "",
    "text": "6.1 Sampling Controls\nWhen large language models produce text, they do so through a probabilistic process—choosing one word (or token) at a time based on learned likelihoods from vast amounts of training data. Generation parameters govern how that probabilistic process unfolds. Rather than altering what the model “knows,” these parameters control how it expresses that knowledge: how much variability is allowed, how long a response can be, and how the model manages uncertainty while generating language.\nFrom an educational measurement perspective, generation parameters serve a role analogous to setting conditions for test administration or scoring protocols. They define the boundaries within which the model operates, affecting reliability, reproducibility, and interpretability. Understanding these controls allows researchers and educators to align model behavior with the goals of a particular task—whether that task emphasizes consistency and fairness in scoring or diversity and creativity in content generation.\nThese parameters affect the randomness and diversity of the model output.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#temperature",
    "href": "01b-gen-ai-parameters.html#temperature",
    "title": "5  Model Parameters",
    "section": "",
    "text": "5.1.1 Generation based?\nChecking if 3 hashtags work",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#top_p",
    "href": "01b-gen-ai-parameters.html#top_p",
    "title": "5  Model Parameters",
    "section": "5.2 top_p",
    "text": "5.2 top_p",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#top_k",
    "href": "01b-gen-ai-parameters.html#top_k",
    "title": "5  Model Parameters",
    "section": "5.3 top_k",
    "text": "5.3 top_k",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#max-tokens",
    "href": "01b-gen-ai-parameters.html#max-tokens",
    "title": "5  Model Parameters",
    "section": "5.4 max tokens",
    "text": "5.4 max tokens",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#frequency-penalty",
    "href": "01b-gen-ai-parameters.html#frequency-penalty",
    "title": "5  Model Parameters",
    "section": "5.5 Frequency Penalty",
    "text": "5.5 Frequency Penalty\nWhat about other model arguments, such as: ## System Prompt",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#user-prompt",
    "href": "01b-gen-ai-parameters.html#user-prompt",
    "title": "5  Model Parameters",
    "section": "5.6 User Prompt",
    "text": "5.6 User Prompt\nWhat’s the difference?",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Model Parameters</span>"
    ]
  },
  {
    "objectID": "03-api-implementation.html#choosing-a-generative-ai-model",
    "href": "03-api-implementation.html#choosing-a-generative-ai-model",
    "title": "API Implementation",
    "section": "",
    "text": "Anthropic\n\n\nOpenAI\n\n\nOther Model Options",
    "crumbs": [
      "API Implementation"
    ]
  },
  {
    "objectID": "03a-api-keys.html#workshop-api-key",
    "href": "03a-api-keys.html#workshop-api-key",
    "title": "9  API Keys",
    "section": "9.2 Workshop API Key",
    "text": "9.2 Workshop API Key\nI created an API key that you will be able to use for the purposes of the workshop. This API key will only be active during the workshop hours. If you attempt to use the API key outside of these hours, you will see that it has been disabled and your calls to the model will not be completed.\nBecause API keys are cost-per-use, I ask that you please only do the workshop activities and other experimentation. Use costs are relatively low for this type of use and I’m happy to cover the cost and provide an API key for educational purposes.\nI used Anthropic’s Claude model for the majority of",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>API Keys</span>"
    ]
  },
  {
    "objectID": "03b-prompting-models.html#batch-processing",
    "href": "03b-prompting-models.html#batch-processing",
    "title": "10  Prompting LLMs via API",
    "section": "10.2 Batch Processing",
    "text": "10.2 Batch Processing",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Prompting LLMs via API</span>"
    ]
  },
  {
    "objectID": "04-integration.html",
    "href": "04-integration.html",
    "title": "Integrating LLMs into R Workflows",
    "section": "",
    "text": "First Thing",
    "crumbs": [
      "Integrating LLMs into R Workflows"
    ]
  },
  {
    "objectID": "04-integration.html#second-thing",
    "href": "04-integration.html#second-thing",
    "title": "Integrating LLMs into R Workflows",
    "section": "Second Thing",
    "text": "Second Thing",
    "crumbs": [
      "Integrating LLMs into R Workflows"
    ]
  },
  {
    "objectID": "04a-single-interactions-via-api.html",
    "href": "04a-single-interactions-via-api.html",
    "title": "11  Single Interactions via API",
    "section": "",
    "text": "11.1 Continued interactions with models\nWhen are these appropriate?",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Single Interactions via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html",
    "href": "04b-conversations-via-api.html",
    "title": "\n15  Chat Conversations via API\n",
    "section": "",
    "text": "15.1 Quick Start\nSo far I’ve focused on just interacting with generative models in one-off interactions, where the chat history isn’t preserved. This functionality significantly differs from the normal chatbot interface user experience. This may not be helpful, depending on what you want out of your interaction with the generative model.\nThe ellmer package, developed by Posit, offers an easy way to have a conversational interaction with the different generative AI models. In this section we’ll go over some of the basics for using this functionality. We’ll continue to use our Anthropic API key, although ellmer supports interactions with a variety of model providers: OpenAI, Google Gemini, Mistal, Hugging Face, perplexity.ai, etc. What follows can mostly be generalized to working with other models, with some slight differences (which I’ll point out below).\nThe easiest way to start a conversation is just to use the default settings for a model. You’ll see (as of October 10) that the model is using Claude Sonnet 4 by default. This may change in the future. You can see what Anthropic models are available with the following:\nCodelibrary(ellmer)\n\nmodels_anthropic()\n\n                           id              name created_at cached_input input\nNA claude-sonnet-4-5-20250929 Claude Sonnet 4.5 2025-09-29           NA    NA\n14   claude-opus-4-1-20250805   Claude Opus 4.1 2025-08-05         1.50 15.00\n15     claude-opus-4-20250514     Claude Opus 4 2025-05-22         1.50 15.00\n16   claude-sonnet-4-20250514   Claude Sonnet 4 2025-05-22         0.30  3.00\n6  claude-3-7-sonnet-20250219 Claude Sonnet 3.7 2025-02-24         0.30  3.00\n1   claude-3-5-haiku-20241022  Claude Haiku 3.5 2024-10-22         0.08  0.80\n8     claude-3-haiku-20240307    Claude Haiku 3 2024-03-07         0.03  0.25\n   output\nNA     NA\n14  75.00\n15  75.00\n16  15.00\n6   15.00\n1    4.00\n8    1.25\nThe “input” column is the cost per million tokens of model input (the prompts). The “output” column is the cost per million tokens of the model response. For some context, Shakespeare’s Romeo and Juliet is about 25,000 words, which roughly translates to 40,000 tokens (depends on the tokenization method of the model).\nFor the purposes of demonstration in our workshop, there’s no need to change it, although I’ll show you how you can do this below.\nCode# Gets the API from the .Renviron file\napi_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n\n# You'll see a Claude Sonnet 4 is being used by default.\nchat &lt;- chat_anthropic()\n\nUsing model = \"claude-sonnet-4-20250514\".\nNow let’s look at the conversational functionality. Below I’ve prompted the model via chat$chat(\"prompt\"), and then immediately used the same syntax again (with a different prompt). I’ve hidden the output because it’s so long; you’ll need to click to see the prompt and model response.\nAs you can see, the second response from the model takes into context the first prompt - it’s still talking about the moon! This conversational functionality is useful when you’re doing iterative development or planning, and the previous calls to the model are important for providing content and building upon previous prompts and model responses.",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04c-rag-models.html",
    "href": "04c-rag-models.html",
    "title": "16  Retrieval Augmented Generation",
    "section": "",
    "text": "16.1 ragnar\nRetrieval-Augmented Generation (RAG) is an approach that enhances large language models by connecting them to external knowledge sources. Instead of relying solely on the information encoded in the model during training, RAG systems first search through a database or document collection to find relevant information, then use that retrieved content to generate more accurate and grounded responses. Think of it like an open-book exam versus a closed-book exam: the model can “look up” information from trusted sources rather than depending entirely on memorized knowledge.\nWhen you submit a query to a RAG system, it first converts your question into a mathematical representation called an embedding (see ?sec-embeddings). The documents in the RAG database have been pre-processed the same way - each document or chunk of text has been converted into its own embedding. The system then performs a similarity search to find which document embeddings are most similar to your query embedding, retrieving the most relevant passages.\nThose retrieved passages are then inserted directly into the prompt that gets sent to the LLM. So the model receives something like: “Here are some relevant documents: [retrieved passage 1], [retrieved passage 2], [retrieved passage 3]. Now answer this question: [your original query].” The LLM reads both the retrieved context and your question together, then generates a response based on that combined information. The database itself doesn’t generate anything—it just stores and retrieves text. The LLM does all the language understanding and generation, but it’s working with an enriched prompt that includes relevant background information it didn’t have in its training data. This is why RAG is sometimes described as giving the model a “working memory” or “external knowledge base”—you’re dynamically providing it with relevant information to reference while generating its response.\nFor educational measurement professionals, RAG has promising applications in areas like automated item generation, where the system could retrieve examples from existing item banks before generating new assessment items, or in providing feedback to students by pulling from curriculum materials and scoring rubrics. The key advantage is that RAG systems can work with your organization’s specific content—test specifications, standards documents, or assessment frameworks—without requiring expensive retraining of the underlying model. This makes the technology more practical and trustworthy for high-stakes applications, since you can update the knowledge base as standards evolve and trace the model’s responses back to specific source documents.\nUnsuprisingly (again), Posit has created an app called ragnar that is part of their tidyverse suite of packages.",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "07-acknowledgements.html",
    "href": "07-acknowledgements.html",
    "title": "16  Acknowledgements",
    "section": "",
    "text": "Many thanks to those that helped me in creating this guide (whether they knew it or not 😅).\n\nVictoria Yaneva, who really helped me upskill in all things related to NLP and AI.\nHadley Wickham. Aside from being generally inspirational in the quality of their work and open-science attitude, I largely modeled this Quarto book off his immensely helpful “R for Data Science” book. Being able to review the code used to make that book saved me a significant amount of time in learning Quarto and making this book.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Acknowledgements</span>"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "\n1  Test\n",
    "section": "",
    "text": "Inline test: 4\n\nCode2 + 2\n\n[1] 4",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Test</span>"
    ]
  },
  {
    "objectID": "08-ref-start.html",
    "href": "08-ref-start.html",
    "title": "References",
    "section": "",
    "text": "1. Vaswani A, Shazeer N, Parmar N, et al.\nAttention is all you need. Advances in neural information processing\nsystems. 2017;30.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "00b-R-packages.html#package-details",
    "href": "00b-R-packages.html#package-details",
    "title": "3  R Packages",
    "section": "\n3.2 Package details",
    "text": "3.2 Package details\n\nCodeinstall.packages(c(\"tidyverse\", \"curl\", \"ragnar\", \"duckdb\", \"shiny\", \"shinyjs\"))",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>R Packages</span>"
    ]
  },
  {
    "objectID": "08-acknowledgements.html",
    "href": "08-acknowledgements.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "Many thanks to those that helped me in creating this guide (whether they knew it or not 😅).\n\nVictoria Yaneva, who really helped me upskill in all things related to NLP and AI.\nThe many kind, patient members of the NBME AI and Data Science team, who were always kind enough to help me learn, correct my mistakes, and point me in the direction of new things happening in AI.\n\nYiyun Zhou\nSaed Rezayi\nTazin Afrin\nAndrew Emerson\nKeelan Evanini\n\nHadley Wickham. Aside from being generally inspirational in the quality of their work and open-science attitude, I largely modeled this Quarto book off his immensely helpful “R for Data Science” book. Being able to review the code used to make that book saved me a significant amount of time in learning Quarto and making this book.",
    "crumbs": [
      "Acknowledgements"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#sampling-controls",
    "href": "01b-gen-ai-parameters.html#sampling-controls",
    "title": "\n6  Generation Parameters\n",
    "section": "",
    "text": "6.1.1 Temperature\nTemperature controls how much randomness is introduced during text generation. A value near 0 produces deterministic, highly focused responses; higher values (e.g., 0.8–1.0) make the output more varied and creative. Statistically, it scales the logits before sampling, flattening or sharpening the probability distribution over possible next tokens. For reproducible outputs or grading tasks, low temperature is preferred; for brainstorming or ideation, higher values work better.\n\n6.1.2 top_p (Nucleus Sampling)\ntop_p defines how much of the total probability mass is considered when sampling the next token. The model first sorts possible next tokens by probability and keeps only the smallest set whose cumulative probability exceeds p.  For example, top_p = 0.9 means sampling only from the top 90% of the probability mass. This is another way to control diversity — lower values produce more predictable text.\n\n6.1.3 top_k\n\ntop_k restricts the number of candidate tokens the model can choose from at each step. If k = 50, only the 50 most likely next tokens are considered. This parameter is conceptually similar to top_p but framed in terms of count rather than probability. Many APIs use either top_p or top_k, but not both — using one usually provides enough control over randomness.\n\n6.1.4 Seed\nThe seed parameter fixes the random number generator used during sampling, ensuring that the same prompt and parameters produce identical outputs every time. It’s especially valuable for research or assessment contexts where reproducibility matters. Setting a seed makes model behavior more deterministic, which supports fair comparisons across conditions or versions of a prompt.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#length-and-structure-controls",
    "href": "01b-gen-ai-parameters.html#length-and-structure-controls",
    "title": "\n6  Generation Parameters\n",
    "section": "\n6.2 Length and Structure Controls",
    "text": "6.2 Length and Structure Controls\nThese parameters constrain how much or what kind of text the model can produce.\n\n6.2.1 Max Tokens\nmax_tokens sets the upper limit for how long the model’s output can be, measured in tokens (roughly pieces of words). If the model reaches this limit, it stops generating even if the thought or sentence isn’t complete. This parameter is useful for keeping outputs concise or fitting within budget constraints, since longer outputs consume more tokens (and thus cost more).\n\n6.2.2 Stop Sequences\nStop sequences define one or more strings that tell the model when to stop generating text. When the model outputs any of these sequences, generation ends immediately. This helps control response boundaries—useful for cutting off unwanted explanations or ensuring that responses end cleanly at a specific marker, such as “END SCORE” or “###”.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#bias-and-repetition-controls",
    "href": "01b-gen-ai-parameters.html#bias-and-repetition-controls",
    "title": "\n6  Generation Parameters\n",
    "section": "\n6.3 Bias and Repetition Controls",
    "text": "6.3 Bias and Repetition Controls\nThese parameters discourage certain token patterns.\n\n6.3.1 Frequency Penalty\nfrequency_penalty discourages the model from repeating the same words or phrases. It adjusts token probabilities based on how often they’ve already appeared in the current response. Higher values push the model to use more varied vocabulary, while lower or zero values allow freer repetition. It’s especially useful for generating longer outputs that shouldn’t sound redundant.\n\n6.3.2 Presence Penalty\nThe presence_penalty discourages the model from reusing tokens that have already appeared in the text. Unlike the frequency_penalty, which scales with repetition, the presence penalty applies whenever a token has occurred before, even once. Increasing this value nudges the model to introduce new concepts or vocabulary, which can make generated text more diverse and exploratory.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#prompt-components",
    "href": "01b-gen-ai-parameters.html#prompt-components",
    "title": "\n6  Generation Parameters\n",
    "section": "\n6.4 Prompt Components",
    "text": "6.4 Prompt Components\n\n6.4.1 System Prompt\nThe system prompt sets the model’s overall role, tone, or behavior—essentially, the “meta” instruction that defines how the model should interpret everything that follows. For example, it might specify “You are an R assistant who explains concepts clearly and uses examples.” This prompt influences style and scope across the entire conversation. Most often the default system prompt is set to “Assistant” or “User”.\n\n6.4.2 User Prompt\nThe user prompt is the immediate question or task you’re asking the model to perform. It represents the actual input or query, such as “Write an R function that calculates bootstrapped confidence intervals.” Together, the system and user prompts define both who the model should be and what it should do—analogous to a function’s global defaults and its current arguments.\n\n6.4.3 Response Schema\nA response schema specifies the structure or format the model should follow when producing its output. For example, you might require responses in JSON with fields like “score” and “rationale”. Defining a schema encourages consistency across runs, simplifies parsing in R workflows, and reduces the need for post-processing or cleanup.",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "01b-gen-ai-parameters.html#deterministic-replicable-and-non-reproducible-sampling",
    "href": "01b-gen-ai-parameters.html#deterministic-replicable-and-non-reproducible-sampling",
    "title": "\n6  Generation Parameters\n",
    "section": "\n6.5 Deterministic, Replicable, and Non-Reproducible Sampling",
    "text": "6.5 Deterministic, Replicable, and Non-Reproducible Sampling\n\nCode# --- Helper Functions ------------------------------------------------------\n\nsoftmax &lt;- function(x) exp(x - max(x)) / sum(exp(x - max(x)))\n\nsample_next_token &lt;- function(logits, temperature = 1, top_p = 1) {\n  # Deterministic path\n  if (isTRUE(all.equal(temperature, 0))) {\n    return(which.max(logits))\n  }\n\n  # Apply temperature and convert to probabilities\n  p &lt;- softmax(logits / temperature)\n\n  # Nucleus (top_p) filtering: keep smallest prefix whose cumulative prob &gt;= top_p\n  ord &lt;- order(p, decreasing = TRUE)\n  p_sorted &lt;- p[ord]\n  cutoff_idx &lt;- which(cumsum(p_sorted) &gt;= top_p)[1]\n  if (is.na(cutoff_idx)) cutoff_idx &lt;- length(p_sorted)\n  keep &lt;- ord[seq_len(cutoff_idx)]\n  p_keep &lt;- p[keep] / sum(p[keep])\n\n  # Sample from truncated distribution\n  sample(keep, size = 1, prob = p_keep)\n}\n\ndraw_tokens &lt;- function(logits, n = 12, temperature = 0.7, top_p = 0.9) {\n  ids &lt;- integer(n)\n  for (i in seq_len(n)) {\n    ids[i] &lt;- sample_next_token(logits, temperature = temperature, top_p = top_p)\n  }\n  ids\n}\n\nid2token &lt;- function(id, vocab) vocab[id]\n\n# --- A larger \"vocabulary\" and fixed base logits (no RNG here) -------------\nvocab &lt;- c(\n  \"the\",\"patient\",\"reports\",\"fever\",\"no\",\"cough\",\"today\",\"denies\",\"nausea\",\"vomiting\",\n  \"chills\",\"sore\",\"throat\",\"pain\",\"abdominal\",\"headache\",\"shortness\",\"breath\",\"since\",\"yesterday\"\n)\n\n# Hand-tuned logits (arbitrary but stable). Higher = more likely.\nbase_logits &lt;- c(\n  1.20, 0.85, 1.05, 0.15, -0.20, 0.40, -0.10, 0.70, 0.55, 0.25,\n  0.35, -0.05, 0.30, 0.95, 0.10, 0.65, 0.50, 0.45, -0.15, 0.05\n)\n\n# A tiny \"model update\" nudging logits (deterministic noise)\nset.seed(1234)\nupdated_logits &lt;- base_logits + rnorm(length(base_logits), mean = 0, sd = 0.03)\n\n# --- 1) Deterministic (temperature = 0) ------------------------------------\ndet_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0, top_p = 1)\n\n# --- 2) Replicable (same seed + same params) --------------------------------\nset.seed(42)\nrepA_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\nset.seed(42)\nrepB_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\nset.seed(7)\nrepC_ids &lt;- draw_tokens(base_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\n# --- 3) Non-Reproducible (same seed/params, different logits) ---------------\nset.seed(42)\nupd_ids &lt;- draw_tokens(updated_logits, n = 12, temperature = 0.7, top_p = 0.9)\n\n# --- Pretty printing --------------------------------------------------------\ncat(\"**Deterministic (temp = 0):**\\n\", paste(id2token(det_ids, vocab), collapse = \" \"), \"\\n\\n\")\n\n**Deterministic (temp = 0):**\n the the the the the the the the the the the the \n\nCodecat(\"**Replicable A (seed = 42):**\\n\", paste(id2token(repA_ids, vocab), collapse = \" \"), \"\\n\")\n\n**Replicable A (seed = 42):**\n fever fever pain chills nausea denies breath the shortness breath denies breath \n\nCodecat(\"**Replicable B (seed = 42):**\\n\", paste(id2token(repB_ids, vocab), collapse = \" \"), \"\\n\")\n\n**Replicable B (seed = 42):**\n fever fever pain chills nausea denies breath the shortness breath denies breath \n\nCodecat(\"**Different seed (seed = 7):**\\n\", paste(id2token(repC_ids, vocab), collapse = \" \"), \"\\n\\n\")\n\n**Different seed (seed = 7):**\n yesterday patient the the reports chills pain abdominal reports denies reports reports \n\nCodecat(\"**Updated model (seed = 42, new logits):**\\n\", paste(id2token(upd_ids, vocab), collapse = \" \"), \"\\n\\n\")\n\n**Updated model (seed = 42, new logits):**\n abdominal abdominal pain chills nausea denies breath the shortness breath denies breath \n\nCode# --- Compact comparison table ----------------------------------------------\ndf &lt;- data.frame(\n  pos = rep(1:12, 5),\n  run = rep(c(\"Deterministic\",\"Replicable_A_s42\",\"Replicable_B_s42\",\"DiffSeed_s7\",\"UpdatedModel_s42\"), each = 12),\n  token = c(id2token(det_ids, vocab),\n            id2token(repA_ids, vocab),\n            id2token(repB_ids, vocab),\n            id2token(repC_ids, vocab),\n            id2token(upd_ids, vocab))\n)\n\n# Quick checks: A vs B identical; A vs Updated often differs at multiple positions.\nidentical_A_B &lt;- identical(repA_ids, repB_ids)\nmatches_A_upd &lt;- sum(repA_ids == upd_ids)\n\ncat(sprintf(\"Check — A vs B identical: %s\\n\", ifelse(identical_A_B, \"YES\", \"NO\")))\n\nCheck — A vs B identical: YES\n\nCodecat(sprintf(\"Positions matching A vs Updated: %d of %d\\n\\n\", matches_A_upd, length(repA_ids)))\n\nPositions matching A vs Updated: 10 of 12\n\nCodeknitr::kable(df, caption = \"Deterministic, Replicable, and Non-Reproducible sequences (12 tokens)\")\n\n\nDeterministic, Replicable, and Non-Reproducible sequences (12 tokens)\n\npos\nrun\ntoken\n\n\n\n1\nDeterministic\nthe\n\n\n2\nDeterministic\nthe\n\n\n3\nDeterministic\nthe\n\n\n4\nDeterministic\nthe\n\n\n5\nDeterministic\nthe\n\n\n6\nDeterministic\nthe\n\n\n7\nDeterministic\nthe\n\n\n8\nDeterministic\nthe\n\n\n9\nDeterministic\nthe\n\n\n10\nDeterministic\nthe\n\n\n11\nDeterministic\nthe\n\n\n12\nDeterministic\nthe\n\n\n1\nReplicable_A_s42\nfever\n\n\n2\nReplicable_A_s42\nfever\n\n\n3\nReplicable_A_s42\npain\n\n\n4\nReplicable_A_s42\nchills\n\n\n5\nReplicable_A_s42\nnausea\n\n\n6\nReplicable_A_s42\ndenies\n\n\n7\nReplicable_A_s42\nbreath\n\n\n8\nReplicable_A_s42\nthe\n\n\n9\nReplicable_A_s42\nshortness\n\n\n10\nReplicable_A_s42\nbreath\n\n\n11\nReplicable_A_s42\ndenies\n\n\n12\nReplicable_A_s42\nbreath\n\n\n1\nReplicable_B_s42\nfever\n\n\n2\nReplicable_B_s42\nfever\n\n\n3\nReplicable_B_s42\npain\n\n\n4\nReplicable_B_s42\nchills\n\n\n5\nReplicable_B_s42\nnausea\n\n\n6\nReplicable_B_s42\ndenies\n\n\n7\nReplicable_B_s42\nbreath\n\n\n8\nReplicable_B_s42\nthe\n\n\n9\nReplicable_B_s42\nshortness\n\n\n10\nReplicable_B_s42\nbreath\n\n\n11\nReplicable_B_s42\ndenies\n\n\n12\nReplicable_B_s42\nbreath\n\n\n1\nDiffSeed_s7\nyesterday\n\n\n2\nDiffSeed_s7\npatient\n\n\n3\nDiffSeed_s7\nthe\n\n\n4\nDiffSeed_s7\nthe\n\n\n5\nDiffSeed_s7\nreports\n\n\n6\nDiffSeed_s7\nchills\n\n\n7\nDiffSeed_s7\npain\n\n\n8\nDiffSeed_s7\nabdominal\n\n\n9\nDiffSeed_s7\nreports\n\n\n10\nDiffSeed_s7\ndenies\n\n\n11\nDiffSeed_s7\nreports\n\n\n12\nDiffSeed_s7\nreports\n\n\n1\nUpdatedModel_s42\nabdominal\n\n\n2\nUpdatedModel_s42\nabdominal\n\n\n3\nUpdatedModel_s42\npain\n\n\n4\nUpdatedModel_s42\nchills\n\n\n5\nUpdatedModel_s42\nnausea\n\n\n6\nUpdatedModel_s42\ndenies\n\n\n7\nUpdatedModel_s42\nbreath\n\n\n8\nUpdatedModel_s42\nthe\n\n\n9\nUpdatedModel_s42\nshortness\n\n\n10\nUpdatedModel_s42\nbreath\n\n\n11\nUpdatedModel_s42\ndenies\n\n\n12\nUpdatedModel_s42\nbreath",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Generation Parameters</span>"
    ]
  },
  {
    "objectID": "00c-test-connect.html",
    "href": "00c-test-connect.html",
    "title": "3  Testing API Connection",
    "section": "",
    "text": "Many of the interactive portions of this workshop depend on successfully being able to connect to a generative AI model via API (we will discuss API keys in a little more depth later). For the purposes of this workshop I’ve set up an API key through Anthropic for you all to use (see Section 10.2).\nThe following is a very basic function for you to test that you can connect to Anthropic’s Claude Sonnet 4.5 model. As we progress through the workshop we’ll use a slightly different function that gives you more control over some of the model generation parameters.\nTo use this function you must have a .Renviron file in your working directory, as the function pulls the API key from that environment file. I will securely distribute this file to workshop participants (contact me if you have not yet received this file). If you’ve just added the .Renviron file to your directory, you’ll need to refresh your R session in order for it to be registered in the environment. You can do this via the tabs at the top of the R Studio interface: Session –&gt; Restart R.\nYou can download the following function by clicking the copy button (looks like a clipboard) in the code chunk displayed below. Alternatively, you can click the icon below:\n 📥 Download call_claude \n\nCode# Required packages\nlibrary(httr)\nlibrary(jsonlite)\n\ncall_claude &lt;- function(prompt,\n                        model = \"claude-sonnet-4-5-20250929\") {\n  \n  # Get API key from environment\n  # You will need to change this to your own API key after workshop\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n\n  # Convert text prompt to required message format\n  messages &lt;- list(list(role = \"user\", content = prompt))\n  \n  # Build request body\n  request_body &lt;- list(\n    model = model,\n    messages = messages,\n    max_tokens = 1024 # Required; will be an argument in other functions\n  )\n  \n  # Set up headers\n  headers &lt;- add_headers(\n    \"x-api-key\" = api_key,\n    \"anthropic-version\" = \"2023-06-01\",\n    \"content-type\" = \"application/json\"\n  )\n  \n  # Make the API request\n  response &lt;- POST(\n    url = \"https://api.anthropic.com/v1/messages\",\n    headers,\n    body = toJSON(request_body, auto_unbox = TRUE)\n  )\n  \n  # Check if request was successful\n  if (http_status(response)$category != \"Success\") {\n    stop(paste(\"API request failed:\", http_status(response)$message, \n               \"\\nDetails:\", content(response, \"text\", encoding = \"UTF-8\")))\n  }\n  \n  # Parse response and extract text content\n  result &lt;- fromJSON(content(response, \"text\", encoding = \"UTF-8\"))\n  return(as.character(result$content)[2])\n}\n\n\nThe only argument that needs to be supplied to the function is the prompt that you want to send to the model. This should be a text string that is enclosed by parentheses: “Tell me a joke about educational measurement.”\n\nCodetest_joke &lt;- call_claude(\"Tell me a joke about educational measurement.\")\n\ntest_joke\n\n[1] \"Why did the test item go to therapy?\\n\\nIt had too many validity issues and couldn't tell if it was really measuring what it was supposed to measure! \\n\\n(And its reliability was so low, it gave a different answer every time someone asked.)\"\n\n\nA few things to note:\n\nGenerally speaking, the process used by a generative AI model involves predicting the next token to be selected and sampling from a distribution of possible options. (We will discuss ways to control this later.) A (sometimes, mostly) beautiful implication of this is that responses from generative AI models will often be different, even if the exact same prompt is used. This is especially the case in the simple function above, as we haven’t made an effort to tune the model generation parameters to achieve a deterministic or replicable result. Thus, when you call Anthropic and have it generate a joke for you using the same prompt, you will likely get different responses each time. In fact, each time that I compile this book on GitHub the result changes!\nYou may notice that escape sequences (\\n, \\t, etc.) may be present because the information is returned in the JSON format. One way to handle this is to wrap your response in cat(). We’ll revisit this again later in other sections.\n\n\nCodecat(test_joke)\n\nWhy did the test item go to therapy?\n\nIt had too many validity issues and couldn't tell if it was really measuring what it was supposed to measure! \n\n(And its reliability was so low, it gave a different answer every time someone asked.)",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Testing API Connection</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html",
    "href": "00d-LLM-R-packages.html",
    "title": "4  LLM-specific R Packages",
    "section": "",
    "text": "4.1 ellmer\nA number of packages have been developed to more easily facilitate interacting with LLMs via R. Many of these packages are useful (we’ll cover some of those in the workshop), whereas other packages include some developer design decisions that don’t work particularly well for my usual workflows.\nBelow is a non-exhaustive list of packages that I’ve found to interact with LLMs. This is not meant to be exhaustive or a curated list; it’s only to provide you with information about the packages you’ll be using in the workshop (and others) in the case you find them helpful for your workflow. All package summaries were initially generated with AI. Some summaries have been edited, some have not.\nellmer Overview CRAN Documentation\nellmer is an R package that provides a unified interface for interacting with large language models from over 17 providers including OpenAI, Anthropic, Google Gemini, and AWS Bedrock. It supports advanced features like streaming outputs, tool/function calling, structured data extraction, and multimodal inputs. Chat objects are stateful and maintain conversation context, enabling both interactive console-based conversations and programmatic use in R scripts and applications.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#tidyprompt",
    "href": "00d-LLM-R-packages.html#tidyprompt",
    "title": "4  LLM-specific R Packages",
    "section": "4.2 tidyprompt",
    "text": "4.2 tidyprompt\ntidyprompt Overview CRAN Documentation\ntidyprompt is an R package that provides a compositional framework (“prompt wraps”) for building prompts enriched with logic, validation, and extraction functions when interacting with LLMs. It supports structured output, retry/feedback loops, reasoning strategies (e.g. ReAct or chain-of-thought), and even autonomous R code or function calling as part of an LLM dialogue. The package is provider-agnostic, meaning its features can layer on top of any chat completion API (e.g. via ellmer) to produce more robust, predictable interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#tidyllm",
    "href": "00d-LLM-R-packages.html#tidyllm",
    "title": "4  LLM-specific R Packages",
    "section": "4.3 tidyllm",
    "text": "4.3 tidyllm\ntidyllm Overview CRAN Documentation\ntidyllm provides a tidy, pipeline-friendly interface for interacting with multiple LLM APIs (e.g. Claude, OpenAI, Gemini, Mistral) and local models via Ollama. It supports multimodal inputs (text, images, PDFs), maintains conversational history, handles batching and rate limits, and allows structured schema-based extraction of responses. The design emphasizes composability and integration into typical R data workflows.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#chattr",
    "href": "00d-LLM-R-packages.html#chattr",
    "title": "4  LLM-specific R Packages",
    "section": "4.4 chattr",
    "text": "4.4 chattr\nchattr Overview CRAN Documentation\nchattr is an R package that enables interactive communication with large language models directly within RStudio using a Shiny gadget or from the console. It enriches prompts with contextual information (e.g. loaded data frames) and integrates with various back-ends (e.g. OpenAI, Copilot, local LlamaGPT) via the ellmer interface. The package is geared toward exploratory workflows and rapid prototyping of LLM-assisted analysis.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#llmagentr",
    "href": "00d-LLM-R-packages.html#llmagentr",
    "title": "4  LLM-specific R Packages",
    "section": "4.5 LLMAgentR",
    "text": "4.5 LLMAgentR\nLLMAgentR Overview CRAN Documentation\nLLMAgentR is an R package for constructing language model “agents” using a modular, graph-based execution framework inspired by LangChain/LangGraph architectures. It offers a suite of agent types (e.g. code generation, data wrangling, SQL agents, document summarization) that iteratively reason, generate R code, execute, debug, and explain results. The package aims to support reproducible AI workflows for analysis, research, and automation by integrating LLM reasoning and domain logic.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "00d-LLM-R-packages.html#packetllm",
    "href": "00d-LLM-R-packages.html#packetllm",
    "title": "4  LLM-specific R Packages",
    "section": "4.6 PacketLLM",
    "text": "4.6 PacketLLM\nPacketLLM Overview CRAN Documentation\nPacketLLM offers an interactive RStudio gadget interface for chatting with OpenAI LLMs (e.g. GPT-5 and variants) directly within the R environment. It supports multiple simultaneous conversation tabs, file upload (e.g. .R, PDF, DOCX) as contextual input, and per-conversation system message configuration. API calls are handled asynchronously (via promises + future) to avoid blocking the R console during model interactions.",
    "crumbs": [
      "Set Up",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>LLM-specific R Packages</span>"
    ]
  },
  {
    "objectID": "01-gen-ai-fundamentals.html#what-is-generative-ai",
    "href": "01-gen-ai-fundamentals.html#what-is-generative-ai",
    "title": "Generative AI Fundamentals",
    "section": "",
    "text": "Comparison of AI and NLP",
    "crumbs": [
      "Generative AI Fundamentals"
    ]
  },
  {
    "objectID": "01a-gen-ai-basics.html#co-occurrence",
    "href": "01a-gen-ai-basics.html#co-occurrence",
    "title": "5  Foundational Principles",
    "section": "5.2 Co-occurrence",
    "text": "5.2 Co-occurrence",
    "crumbs": [
      "Generative AI Fundamentals",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Foundational Principles</span>"
    ]
  },
  {
    "objectID": "03a-api-keys.html#sec-workshop-key",
    "href": "03a-api-keys.html#sec-workshop-key",
    "title": "10  API Keys",
    "section": "10.2 Workshop API Key",
    "text": "10.2 Workshop API Key\nI created an API key that you will be able to use for the purposes of the workshop. This API key will only be active during the workshop hours. If you attempt to use the API key outside of these hours, you will see that it has been disabled and your calls to the model will not be completed.\nBecause API keys are cost-per-use, I ask that you please only do the workshop activities and other experimentation. Use costs are relatively low for this type of use and I’m happy to cover the cost and provide an API key for educational purposes.\nI used Anthropic’s Claude model for the majority of",
    "crumbs": [
      "API Implementation",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>API Keys</span>"
    ]
  },
  {
    "objectID": "06-references.html",
    "href": "06-references.html",
    "title": "References",
    "section": "",
    "text": "1. Vaswani A, Shazeer N, Parmar N, et al.\nAttention is all you need. Advances in neural information processing\nsystems. 2017;30.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "07-reference-materials.html",
    "href": "07-reference-materials.html",
    "title": "Reference Materials",
    "section": "",
    "text": "Background Information on GPTs\nThese materials / links were last checked on October 2, 2025. All apologies for links that no longer work. Please email me at CRunyon@nbme.org if you notice something no longer works so I can change / remove the link.\nThe 3Blue1Brown YouTube Channel provides several good videos on the some of the technical aspects of large language models.\nAnthropic’s paper On the Biology of a Large Language Model is particularly interesting.\nThis post on lesswrong provides a nice high-level summary for understanding LLMs.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#background-information-on-gpts",
    "href": "07-reference-materials.html#background-information-on-gpts",
    "title": "Reference Materials",
    "section": "",
    "text": "The Neural Networks section is particularly informative.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#reference-guides-and-prompt-engineering",
    "href": "07-reference-materials.html#reference-guides-and-prompt-engineering",
    "title": "Reference Materials",
    "section": "Reference Guides (and Prompt Engineering)",
    "text": "Reference Guides (and Prompt Engineering)\nOpenAI Cookbook",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#shiny-related-resources",
    "href": "07-reference-materials.html#shiny-related-resources",
    "title": "Reference Materials",
    "section": "Shiny-related Resources",
    "text": "Shiny-related Resources\nPosit has many useful things.\n\nMaterials for the R language start here.\nMaterials for the Python language start here.\n\nnanxstats has nicely organized many Shiny extension packages.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#resources-for-continuing-development",
    "href": "07-reference-materials.html#resources-for-continuing-development",
    "title": "Reference Materials",
    "section": "Resources for Continuing Development",
    "text": "Resources for Continuing Development\nThe following list of books / blogs / newsletters / training is a compilation of resources that have either I found useful or have been recommended by colleagues.\n\nNewsletters\n\n\nBooks\n\n\nBlogs\n\n\nTraining",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "07-reference-materials.html#legal-considerations",
    "href": "07-reference-materials.html#legal-considerations",
    "title": "Reference Materials",
    "section": "Legal Considerations",
    "text": "Legal Considerations\nThe information provided here is for general informational purposes only and does not constitute legal advice. You should not act upon any information presented without first seeking qualified legal counsel regarding your specific situation. The authors disclaim any liability for actions taken based on the content provided here.",
    "crumbs": [
      "Reference Materials"
    ]
  },
  {
    "objectID": "02-prompt-engineering.html#resources",
    "href": "02-prompt-engineering.html#resources",
    "title": "Prompt Engineering",
    "section": "Resources",
    "text": "Resources\nAnthropic’s Prompt Engineering Guide\n\nOpenAI’s Prompt Engineering Guide\n\nGoogle Gemini’s Prompt Engineering Guide\n\nIncludes directions on uploading a file via API as part of a prompt.\n\n\npromptingguide.ai is a great resources for learning more about prompt engineering techniques.",
    "crumbs": [
      "Prompt Engineering"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html",
    "href": "activity-parameter-testing.html",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "",
    "text": "9.1 claude_plus function\nThis activity is designed to allow you to get hands-on experience playing around with model generation parameters. To do so, we need to modify the initial call_claude function to incorporate specifying the generation parameters.\nThe parameters that we will be testing are temperature,top_p, and top_k. (max_tokens is also included, but we won’t be using that in our testing.) Note that for our particular model (Claude Sonnet 4.5), you cannot specify temperature and top_p at the same time.\nYou can download the following function by clicking the copy button (looks like a clipboard) in the code chunk displayed below. Alternatively, you can click the icon below:\n📥 Download claude_plus\nCodelibrary(httr)\nlibrary(jsonlite)\n\nclaude_plus &lt;- function(prompt,\n                        model = \"claude-sonnet-4-5-20250929\",\n                        temperature = NULL,\n                        top_p = NULL,\n                        max_tokens = 1024) {\n  \n  # Check if both temperature and top_p are supplied\n  if (!is.null(temperature) && !is.null(top_p)) {\n    warning(\"Both temperature and top_p arguments are supplied. The Anthropic API does not support using both simultaneously. Only temperature will be used.\")\n  }\n  \n  # Get API key from environment\n  # You will need to change this to your own API key after workshop\n  api_key &lt;- Sys.getenv(\"ANTHROPIC_API_KEY\")\n  # Convert text prompt to required message format\n  messages &lt;- list(list(role = \"user\", content = prompt))\n  \n  # Build request body\n  request_body &lt;- list(\n    model = model,\n    messages = messages,\n    max_tokens = max_tokens # Required; will be an argument in other functions\n  )\n  \n  # Add temperature if provided (takes precedence over top_p)\n  if (!is.null(temperature)) {\n    request_body$temperature &lt;- temperature\n  } else if (!is.null(top_p)) {\n    # Only add top_p if temperature is not provided\n    request_body$top_p &lt;- top_p\n  }\n  \n  # Set up headers\n  headers &lt;- add_headers(\n    \"x-api-key\" = api_key,\n    \"anthropic-version\" = \"2023-06-01\",\n    \"content-type\" = \"application/json\"\n  )\n  \n  # Make the API request\n  response &lt;- POST(\n    url = \"https://api.anthropic.com/v1/messages\",\n    headers,\n    body = toJSON(request_body, auto_unbox = TRUE)\n  )\n  \n  # Check if request was successful\n  if (http_status(response)$category != \"Success\") {\n    stop(paste(\"API request failed:\", http_status(response)$message, \n               \"\\nDetails:\", content(response, \"text\", encoding = \"UTF-8\")))\n  }\n  \n  # Parse response and extract text content\n  result &lt;- fromJSON(content(response, \"text\", encoding = \"UTF-8\"))\n  return(as.character(result$content)[2])\n}\nUnderstanding how these generation parameters change model output is more obvious when you can see the different outputs from the model using the same parameters. I’ve made the claude_param_test function to help with this - it includes a n_reps argument (default = 5, max of 20) that replicates the call several times.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#sec-claude-param-test",
    "href": "activity-parameter-testing.html#sec-claude-param-test",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.2 claude_param_test function",
    "text": "9.2 claude_param_test function\nYou can download the following function by clicking the copy button (looks like a clipboard) in the code chunk displayed below. Alternatively, you can click the icon below:\n 📥 Download claude_param_test \n\nCodeclaude_param_test &lt;- function(prompt,\n                              temperature = NULL,\n                              top_p = NULL,\n                              top_k = NULL,\n                              max_tokens = 1024,\n                              n_reps = 5) {\n  \n  # Check if n_reps is within allowed range\n  if (n_reps &gt; 20) {\n    warning(\"n_reps exceeds maximum allowed value of 20. Setting n_reps to 20.\")\n    n_reps &lt;- 20\n  }\n  \n  if (n_reps &lt; 5) {\n    warning(\"n_reps is below minimum value of 5. Setting n_reps to 5.\")\n    n_reps &lt;- 5\n  }\n  \n  # Initialize results dataframe\n  results &lt;- data.frame(\n    rep_n = integer(),\n    temp = numeric(),\n    top_p = numeric(),\n    top_k = integer(),\n    output = character(),\n    stringsAsFactors = FALSE\n  )\n  \n  # Loop through n_reps\n  for (i in 1:n_reps) {\n    # Call claude_plus\n    output &lt;- claude_plus(\n      prompt = prompt,\n      temperature = temperature,\n      top_p = top_p,\n      top_k = top_k,\n      max_tokens = max_tokens\n    )\n    \n    # Add to results\n    results &lt;- rbind(results, data.frame(\n      rep_n = i,\n      temp = ifelse(is.null(temperature), NA, temperature),\n      top_p = ifelse(is.null(top_p), NA, top_p),\n      top_k = ifelse(is.null(top_k), NA, top_k),\n      output = output,\n      stringsAsFactors = FALSE\n    ))\n    \n    # Sleep between calls (except after last call)\n    if (i &lt; n_reps) {\n      Sys.sleep(0.2)\n    }\n  }\n  \n  return(results)\n}",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#quick-reference-generation-parameter-table",
    "href": "activity-parameter-testing.html#quick-reference-generation-parameter-table",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.3 Quick Reference Generation Parameter Table",
    "text": "9.3 Quick Reference Generation Parameter Table\n\n\n\n\n\n\n\nGeneration Parameter\nDescription\nRange\n\n\n\ntemperature\nAffects creativity by changing the probability distribution when choosing the next token\n\\(0 \\leq \\texttt{temperature} \\leq 1\\)\n\n\ntop_p\nUses nucleus sampling to limit token choices by selecting options that make up the top_p proportion of options\n\\(0 &lt; \\texttt{top\\_p} \\leq 1\\)\n\n\ntop_k\nSamples from only the top k most probable tokens\n\\(\\texttt{top\\_k} \\geq 1\\)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-1-high-temperature",
    "href": "activity-parameter-testing.html#task-1-high-temperature",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.4 Task 1: High temperature\n",
    "text": "9.4 Task 1: High temperature\n\nFor each of the following tasks, you’re welcome to use my prompt or update the syntax with your own.\n\nCodehigh_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                temperature = 1,\n                                n_reps = 10)\nsave(high_pizza, file = \"./data/high_pizza.R\")\n\n\n\nCodelibrary(knitr)\nload(\"data/high_pizza.R\")\n  kable(high_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\n1\nNA\nNA\nPerfect combo of savory cheese, tangy sauce, crispy crust, and endless delicious toppings.\n\n\n2\n1\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory, salty, satisfying flavors.\n\n\n3\n1\nNA\nNA\nPerfect blend of cheese, sauce, and toppings on crispy-chewy crust hits all taste centers.\n\n\n4\n1\nNA\nNA\nPerfect combo of melted cheese, savory sauce, crispy crust, and endless topping possibilities.\n\n\n5\n1\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible flavors and textures.\n\n\n6\n1\nNA\nNA\nPerfect balance of carbs, cheese, and toppings plus crispy-chewy texture creates ultimate comfort food.\n\n\n7\n1\nNA\nNA\nPerfect blend of savory cheese, tangy sauce, and crispy crust hits all taste receptors.\n\n\n8\n1\nNA\nNA\nPerfect combination of cheese, sauce, bread, and toppings creates satisfying flavors and textures.\n\n\n9\n1\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates an irresistible savory flavor explosion.\n\n\n10\n1\nNA\nNA\nPerfect combo of crispy crust, tangy sauce, melted cheese, and endless topping possibilities.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-2-low-temperature",
    "href": "activity-parameter-testing.html#task-2-low-temperature",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.5 Task 2: Low temperature\n",
    "text": "9.5 Task 2: Low temperature\n\n\nCodelow_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                temperature = 0,\n                                n_reps = 10)\nsave(low_pizza, file = \"./data/low_pizza.R\")\n\n\n\nCodeload(\"data/low_pizza.R\")\nkable(low_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n2\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n3\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n4\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n5\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n6\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n7\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n8\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n9\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n10\n0\nNA\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n\n\n\nNote although the results from this experiment show that the output was identical across the 10 calls, this was by chance; setting the temperature to 0 does not guarantee the exact same outputs will be obtained across different calls.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-3-low-temperature-longer-output",
    "href": "activity-parameter-testing.html#task-3-low-temperature-longer-output",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n9.6 Task 3: Low temperature, longer output",
    "text": "9.6 Task 3: Low temperature, longer output\nI repeated the above task, but increased max_token = 2048 and use the prompt of “Write a short essay about the importance of educational measurement.” I was shocked to see that all the essays were still identical!\n\nCodeedmeasure_low &lt;- claude_param_test(\"Write a short essay about the importance of educational measurement.\",\n                                   temperature = 0,\n                                   n_reps = 20,\n                                   max_tokens = 2048)\nsave(edmeasure_low, file = \"./data/edmeasure_low.R\")\n\n\n\nCodeload(\"data/edmeasure_low.R\")\n\n\n\n\nEssay 1\nEssay 2\nEssay 3\nEssay 4\nEssay 5\n\n\n\n\n10 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n11 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n12 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n13 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.\n\n\n\n\n14 The Importance of Educational Measurement\nEducational measurement serves as the compass guiding modern education systems, providing essential information that shapes teaching, learning, and policy decisions. Far from being merely about assigning grades, effective measurement practices are fundamental to educational quality and equity.\nInforming Instruction\nAt its core, educational measurement helps teachers understand what students know and can do. Through formative assessments, educators identify learning gaps in real-time, allowing them to adjust instruction to meet diverse student needs. This diagnostic function transforms teaching from a one-size-fits-all approach into a responsive, personalized practice that maximizes learning opportunities for every student.\nEnsuring Accountability\nMeasurement provides transparency and accountability within education systems. Standardized assessments offer comparable data across schools and districts, helping stakeholders evaluate whether educational resources are being used effectively and whether students are meeting established learning standards. This accountability, when implemented thoughtfully, drives continuous improvement and ensures that all students receive quality education regardless of their background.\nSupporting Student Growth\nValid and reliable assessments give students clear feedback about their progress, helping them understand their strengths and areas for improvement. This information empowers learners to take ownership of their education and set meaningful goals. Moreover, measurement data helps identify students who need additional support, enabling early intervention before small difficulties become major obstacles.\nGuiding Policy and Resource Allocation\nEducational measurement informs critical decisions about curriculum development, resource distribution, and policy reform. Data-driven insights help administrators and policymakers identify successful programs worth expanding and ineffective practices requiring revision.\nIn conclusion, educational measurement is indispensable to effective education. When conducted ethically and interpreted wisely, it illuminates the path toward educational excellence and equity for all learners.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-4-high-top_p",
    "href": "activity-parameter-testing.html#task-4-high-top_p",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n14.1 Task 4: High top_p\n",
    "text": "14.1 Task 4: High top_p\n\n\nCodehighp_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                 top_p = .90,\n                                 n_reps = 10)\nsave(highp_pizza, file = \"./data/highp_pizza.R\")\n\n\n\nCodeload(\"data/highp_pizza.R\")\nkable(highp_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\nNA\n0.9\nNA\nPerfect blend of cheese, sauce, bread, and toppings creates irresistible savory comfort food.\n\n\n2\nNA\n0.9\nNA\nPerfect combo of crispy crust, tangy sauce, melted cheese, and endless topping possibilities.\n\n\n3\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n4\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n5\nNA\n0.9\nNA\nPerfect blend of carbs, cheese, and toppings creates an irresistible savory, satisfying comfort food.\n\n\n6\nNA\n0.9\nNA\nPerfect combo of melty cheese, tangy sauce, crispy crust, and endless topping possibilities.\n\n\n7\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and satisfying textures.\n\n\n8\nNA\n0.9\nNA\nPerfect combo of crispy crust, tangy sauce, melted cheese, and endless topping possibilities.\n\n\n9\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory, satisfying flavors.\n\n\n10\nNA\n0.9\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "activity-parameter-testing.html#task-5-low-top_p",
    "href": "activity-parameter-testing.html#task-5-low-top_p",
    "title": "9  Activity: Generation Parameter Testing",
    "section": "\n14.2 Task 5: Low top_p\n",
    "text": "14.2 Task 5: Low top_p\n\n\nCodelowp_pizza &lt;- claude_param_test(\"In 15 words or fewer, tell me why pizza is so good.\",\n                                top_p = .10,\n                                n_reps = 10)\nsave(lowp_pizza, file = \"./data/lowp_pizza.R\")\n\n\n\nCodeload(\"data/lowp_pizza.R\")\nkable(lowp_pizza)\n\n\n\n\n\n\n\n\n\n\nrep_n\ntemp\ntop_p\ntop_k\noutput\n\n\n\n1\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n2\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n3\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n4\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n5\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n6\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n7\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n8\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n9\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.\n\n\n10\nNA\n0.1\nNA\nPerfect combo of cheese, sauce, bread, and toppings creates irresistible savory flavors and textures.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Activity: Generation Parameter Testing</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#quick-start",
    "href": "04b-conversations-via-api.html#quick-start",
    "title": "\n15  Chat Conversations via API\n",
    "section": "",
    "text": "Click to see first prompt and response\n\n\n\n\n\n\nCodechat$chat(\"Tell me about the history of the exploration of the moon\")\n\n# The History of Lunar Exploration\n\nThe exploration of the Moon represents one of humanity's greatest achievements,\nspanning from ancient observations to modern robotic missions.\n\n## Ancient and Early Modern Observations\n- **Ancient civilizations** tracked lunar phases and eclipses, creating some of\nthe first astronomical records\n- **1609**: Galileo Galilei used his telescope to make the first detailed \nobservations of the Moon's surface, discovering mountains and craters\n- **17th-19th centuries**: Astronomers mapped the Moon and named its features\n\n## Early Space Age (1950s-1960s)\n### Soviet Achievements\n- **Luna 1 (1959)**: First spacecraft to reach the vicinity of the Moon\n- **Luna 2 (1959)**: First human-made object to reach the Moon's surface\n- **Luna 3 (1959)**: First images of the Moon's far side\n- **Luna 9 (1966)**: First successful soft landing\n\n### U.S. Programs\n- **Ranger program**: Crash-landed probes that sent back images during descent\n- **Surveyor program**: Soft-landing missions to test lunar surface conditions\n- **Lunar Orbiter program**: Mapped the Moon to select Apollo landing sites\n\n## The Apollo Era (1961-1972)\n- **Apollo 8 (1968)**: First crewed mission to orbit the Moon\n- **Apollo 11 (July 20, 1969)**: Neil Armstrong and Buzz Aldrin became the \nfirst humans to walk on the Moon\n- **Six successful landings**: Apollo 11, 12, 14, 15, 16, and 17\n- **Scientific achievements**: Collected 842 pounds of lunar samples, conducted\nexperiments, and deployed scientific instruments\n\n## Post-Apollo Period (1970s-1990s)\n- **Soviet Luna program continued**: Luna 16, 20, and 24 returned samples \nrobotically\n- **Lunokhod rovers**: First successful robotic rovers on another celestial \nbody\n- **Long hiatus**: Limited lunar activity during the 1980s and early 1990s\n\n## Modern Lunar Renaissance (1990s-Present)\n### Major Missions\n- **Clementine (1994)**: First evidence of water ice at lunar poles\n- **Lunar Prospector (1998)**: Confirmed water ice evidence\n- **SMART-1 (2003)**: European Space Agency's first lunar mission\n- **Chang'e program**: China's successful lunar exploration series, including \nsample returns\n- **Chandrayaan missions**: India's lunar exploration program\n- **Artemis program**: NASA's current plan to return humans to the Moon\n\n### Recent Achievements\n- **2019**: China's Chang'e 4 achieved the first soft landing on the far side\n- **2020**: China's Chang'e 5 returned fresh lunar samples\n- **Ongoing**: Multiple nations and private companies planning lunar missions\n\n## Scientific Impact\nLunar exploration has revealed that the Moon likely formed from a giant impact,\nhelped us understand planetary formation, and provided insights into early \nEarth history. The Moon continues to be a target for scientific research and a \nstepping stone for deeper space exploration.\n\nThe story of lunar exploration reflects humanity's drive to explore and \nunderstand our cosmic neighborhood, involving international collaboration and \ncompetition that has advanced our technological capabilities immensely.\n\n\n\n\n\n\n\n\n\n\n\nClick to see follow-up prompt and response\n\n\n\n\n\n\nCodechat$chat(\"What are the most important non-USA exploration missions?\")\n\n# Most Important Non-USA Lunar Exploration Missions\n\n## Soviet Union/Russia - The Pioneers\n\n### Luna Program Breakthroughs\n- **Luna 2 (1959)**: First human-made object to reach the Moon\n- **Luna 3 (1959)**: Provided first images of the Moon's far side, \nrevolutionizing our understanding\n- **Luna 9 (1966)**: First successful soft landing, proving the surface could \nsupport spacecraft\n- **Luna 16 (1970)**: First robotic sample return mission, bringing back 101 \ngrams of lunar soil\n- **Luna 17/Lunokhod 1 (1970)**: First successful planetary rover, operated for\n11 months\n- **Lunokhod 2 (1973)**: More advanced rover that traveled 39 km and returned \nover 80,000 images\n\n**Impact**: The Soviet missions achieved most of the major \"firsts\" in lunar \nexploration and proved that robotic missions could accomplish complex \nscientific tasks.\n\n## China - The Modern Leader\n\n### Chang'e Program\n- **Chang'e 1 & 2**: Successful orbital missions that mapped the Moon\n- **Chang'e 3/Yutu (2013)**: First soft landing since 1976, deployed the Yutu \nrover\n- **Chang'e 4/Yutu-2 (2019)**: **Historic first soft landing on the far side** \nof the Moon, using a relay satellite for communication\n- **Chang'e 5 (2020)**: First sample return mission in 44 years, brought back \n1.7 kg of fresh samples from a previously unexplored region\n\n**Impact**: Revitalized global lunar exploration and demonstrated China's \nadvanced space capabilities.\n\n## European Space Agency\n\n### SMART-1 (2003-2006)\n- First European lunar mission\n- Used revolutionary ion propulsion\n- Discovered new details about lunar composition and the theory of Moon \nformation\n- **Impact**: Proved Europe's deep space capabilities and advanced propulsion \ntechnology\n\n## India\n\n### Chandrayaan Program\n- **Chandrayaan-1 (2008)**: Made crucial discovery of water molecules across \nthe lunar surface using NASA's Moon Mineralogy Mapper\n- **Chandrayaan-2 (2019)**: Though the lander crashed, the orbiter continues \nsuccessful operations\n- **Chandrayaan-3 (2023)**: Successfully soft-landed near the lunar south pole\n\n**Impact**: Major scientific discoveries about lunar water and demonstrated \nIndia's cost-effective space exploration approach.\n\n## Japan\n\n### SELENE/Kaguya (2007-2009)\n- Most comprehensive lunar observation mission since Apollo\n- Created detailed gravitational and topographical maps\n- Provided stunning high-definition video of Earth-rise from lunar orbit\n- **Impact**: Advanced our understanding of lunar internal structure and \nevolution\n\n## Israel (Private/International)\n\n### Beresheet (2019)\n- First privately funded lunar mission\n- Though it crashed during landing, it demonstrated that smaller nations and \nprivate entities could attempt lunar exploration\n- **Impact**: Opened new models for lunar exploration beyond major space \nagencies\n\n## Most Scientifically Significant Contributions\n\n1. **Luna 3's far side images** - Revealed the Moon's asymmetry\n2. **Chang'e 4's far side landing** - Enabled study of the Moon's most ancient \nterrain\n3. **Chandrayaan-1's water discovery** - Fundamentally changed our \nunderstanding of the Moon\n4. **Chang'e 5's sample return** - Provided new insights into lunar geological \ntimeline\n5. **Soviet robotic sample returns** - Proved automated exploration \ncapabilities\n\n## Current Global Lunar Landscape\n\nToday, lunar exploration is truly international, with active programs from:\n- **China**: Most active current program\n- **India**: Cost-effective missions with major discoveries\n- **Russia**: Planning Luna-25 and future missions\n- **Japan, South Korea, UAE**: All have current lunar programs\n- **Private companies**: SpaceX, Intuitive Machines, and others\n\nThese non-USA missions have been crucial in maintaining momentum in lunar \nscience, making fundamental discoveries, and ensuring that lunar exploration \nremains a global endeavor rather than the achievement of a single nation.",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#chat_anthropic-details",
    "href": "04b-conversations-via-api.html#chat_anthropic-details",
    "title": "\n15  Chat Conversations via API\n",
    "section": "\n15.2 chat_anthropic Details",
    "text": "15.2 chat_anthropic Details\nLet’s look at the details of chat_anthropic (from this page of the ellmer package reference.)\n\nCodechat_anthropic(\n  system_prompt = NULL,\n  params = NULL,\n  max_tokens = deprecated(),\n  model = NULL,\n  api_args = list(),\n  base_url = \"https://api.anthropic.com/v1\",\n  beta_headers = character(),\n  api_key = anthropic_key(),\n  api_headers = character(),\n  echo = NULL\n)\n\n\nIt’s important to note the params argument, which allows you to set a variety of model parameters when chatting with the model. This is a general argument in the ellmer package. You’ll need to ensure that your model input allows a specific generation parameter before including it in your call.\nThis is one place where having a conversation with a model via API instead of via a chatbot interface is different - it’s not always easy (and sometimes impossible) to change these parameters in the normal chat interface.\n\nCodeparams(\n  temperature = NULL,\n  top_p = NULL,\n  top_k = NULL,\n  frequency_penalty = NULL,\n  presence_penalty = NULL,\n  seed = NULL,\n  max_tokens = NULL,\n  log_probs = NULL,\n  stop_sequences = NULL,\n  ...\n)",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#options-for-clearing-the-chat",
    "href": "04b-conversations-via-api.html#options-for-clearing-the-chat",
    "title": "\n15  Chat Conversations via API\n",
    "section": "\n15.3 Options for Clearing the Chat",
    "text": "15.3 Options for Clearing the Chat\nThere are two methods to reset the chat history. This is useful when you want to start a conversation about another topic.\n\n15.3.1 Clearing While Maintaining Chat Configuration\nThe following syntax simply clears the turns but maintains the other aspects of the chat configuration (which I’ll discuss momentarily in ?sec-chat-config). In the background the ellmer package is saving a history of your prompts and model responses, and it sending this history as part of the prompt when you send a new prompt. This is also happens when having a conversation with a chatbot, but it’s even less obvious.\n\nCodechat$set_turns(list())\n\n\n\n15.3.2 Clearing All Chat Settings\nThis starts an entirely new chat with the Anthropic model, and removes any settings you’ve made (system prompt, parameters). You can also include this in your argument - the important part is that using chat_anthropic() again resets any previously-specified chat configuration.\n\nCodechat &lt;- chat_anthoropic",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "04b-conversations-via-api.html#system-prompt",
    "href": "04b-conversations-via-api.html#system-prompt",
    "title": "\n15  Chat Conversations via API\n",
    "section": "\n15.4 System Prompt",
    "text": "15.4 System Prompt\nWe briefly discussed system prompts in the section about generative parameters (see ?sec-system-prompt). The system prompt is an instruction to the model that is maintained throughout all of your interactions with the model. I don’t generally use system prompts when calling models via API, but I probably should. 😅 Nonetheless, let’s see how changing the system prompt can change the model output.\nFirst, with no setting of the system prompt:\n\nCodechat &lt;- chat_anthropic()\n\nUsing model = \"claude-sonnet-4-20250514\".\n\nCodechat$chat(\"Briefly tell me the point of using a Rasch model.\")\n\nThe main point of using a Rasch model is to create **linear, interval-level \nmeasurements** from ordinal data (like test scores or survey responses).\n\nKey benefits:\n- **Person-item separation**: Measures both person ability and item difficulty \non the same scale\n- **Invariant measurement**: Person ability estimates don't depend on which \nspecific items were used\n- **Missing data handling**: Can estimate scores even with incomplete responses\n- **Quality control**: Identifies misfitting items or response patterns that \ndon't follow the expected pattern\n\nIt's particularly valuable in educational testing, psychological assessments, \nand surveys where you want to convert raw scores into meaningful, comparable \nmeasurements that behave like true interval scales (equal differences between \nscale points).\n\n\nNow using a playful system prompt:\n\nCodechat &lt;- chat_anthropic(\n  system_prompt = \"You are an assistant that likes to respond in rhymes.\"\n)\n\nUsing model = \"claude-sonnet-4-20250514\".\n\nCodechat$chat(\"Briefly tell me the point of using a Rasch model.\")\n\nThe Rasch model's aim is quite neat,\nTo make measurement fair and complete!\nIt separates trait from task difficulty,\nEnsuring responses show true ability.\n\nWith invariant measures it does provide,\nPerson and item parameters reside\nOn the same scale with equal intervals true,\nMaking comparisons meaningful too!\n\nIn short, it's designed to be:\nA tool for measurement consistency!\n\n\nSome more helpful examples of good system prompts are:\n\n\nspecifying output structure\n\n“Always respond in JSON format with keys: ‘answer’, ‘confidence’, ‘sources’. Never include any text outside the JSON object.”\n\n\n\nensuring constraints\n\n“You are a medical information assistant. Always:\n\n\nEmphasize you’re not a doctor\nRecommend consulting healthcare professionals\nCite medical sources when possible\nNever diagnose conditions”",
    "crumbs": [
      "Integrating LLMs into R Workflows",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Chat Conversations via API</span>"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html",
    "href": "activity-content-development-revision.html",
    "title": "Activity: Content Development and Revision",
    "section": "",
    "text": "Task 1: Pros and Cons\nChoose a topic and ask the model to generate a list of pros and cons for each side. You can use one of the suggestions below or pick your own:\nSuggested prompt: “What are the pros and cons of {topic}? For each point, include a brief explanation.”\nReview the suggested pros and cons. Which points seem most accurate or well-supported? Are any based on weaker reasoning or questionable information?\nCodelibrary(ellmer)\n\nchat &lt;- chat_anthropic()\n\nchat$chat(\"What are the pros and cons of Bayesian vs Frequentist statistics? For each point, include a brief explanation.\")",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#pros-and-cons",
    "href": "activity-content-development-revision.html#pros-and-cons",
    "title": "Activity: Content Development and Revision",
    "section": "Bayesian Statistics",
    "text": "Bayesian vs Frequentist Statistics\nClassical Test Theory vs Item Response Theory\nUnidimensional / Rasch Modeling vs Multidimensional IRT Models\nArgument-Based Validity (Kane) vs Traditional Validity Frameworks (Messick)\nPerformance Assessments vs Selected-Response Items\nStatistical Significance vs Practical Significance in DIF Detection\n\n\n\n\n\n\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\nBayesian Statistics\nPros:\n1. Incorporates Prior Knowledge - Can integrate existing information, expert opinion, or historical data into analysis - Particularly valuable when data is limited or expensive to collect\n2. Intuitive Probability Interpretation - Treats parameters as random variables with probability distributions - Results directly answer questions like “What’s the probability this hypothesis is true?”\n3. Natural Uncertainty Quantification - Credible intervals represent actual probability ranges for parameters - Provides full posterior distributions, not just point estimates\n4. Flexible Model Building - Hierarchical models handle complex data structures naturally - Can easily incorporate missing data and measurement error\n5. Sequential Learning - Today’s posterior becomes tomorrow’s prior - Naturally updates beliefs as new data arrives\nCons:\n1. Subjective Prior Selection - Choice of prior can influence results, especially with small datasets - Different analysts may choose different priors, leading to different conclusions\n2. Computational Complexity - Often requires sophisticated algorithms (MCMC, variational inference) - Can be computationally intensive for complex models\n3. Interpretation Challenges - Results depend on the specific prior chosen - May be harder to communicate to audiences unfamiliar with Bayesian thinking\nFrequentist Statistics\nPros:\n1. Objective and Standardized - Doesn’t require subjective prior assumptions - Provides consistent, reproducible results across analysts\n2. Well-Established Theory - Extensive theoretical foundation with known properties - Widely taught and understood in scientific communities\n3. Computational Efficiency - Many procedures have closed-form solutions or simple algorithms - Generally faster to compute, especially for standard tests\n4. Regulatory Acceptance - Standard approach in clinical trials, quality control, and regulatory science - Well-established guidelines and practices\n5. Long-Run Frequency Guarantees - Confidence intervals and p-values have clear frequentist interpretations - Error rates are controlled over repeated sampling\nCons:\n1. Limited Use of Available Information - Cannot formally incorporate prior knowledge or expert opinion - May ignore relevant historical data\n2. Counter-Intuitive Interpretations - P-values and confidence intervals are often misinterpreted - Doesn’t directly answer “What’s the probability my hypothesis is true?”\n3. Multiple Testing Issues - Requires corrections for multiple comparisons - Can lead to conservative results when many tests are performed\n4. Fixed Sample Size Limitations - Hypothesis tests assume predetermined sample sizes - Optional stopping can invalidate frequentist procedures\n5. Model Inflexibility - Less natural handling of hierarchical structures and missing data - Difficulty incorporating complex uncertainty structures\nBottom Line\nThe choice between Bayesian and Frequentist approaches often depends on the specific problem, available computational resources, domain expertise, and the intended audience. Many modern statisticians advocate for a pragmatic approach, using the method best suited to the particular research question and context.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#bayesian-statistics",
    "href": "activity-content-development-revision.html#bayesian-statistics",
    "title": "Activity: Content Development and Revision",
    "section": "",
    "text": "Pros:\n1. Incorporates Prior Knowledge - Can integrate existing information, expert opinion, or historical data into analysis - Particularly valuable when data is limited or expensive to collect\n2. Intuitive Probability Interpretation - Treats parameters as random variables with probability distributions - Results directly answer questions like “What’s the probability this hypothesis is true?”\n3. Natural Uncertainty Quantification - Credible intervals represent actual probability ranges for parameters - Provides full posterior distributions, not just point estimates\n4. Flexible Model Building - Hierarchical models handle complex data structures naturally - Can easily incorporate missing data and measurement error\n5. Sequential Learning - Today’s posterior becomes tomorrow’s prior - Naturally updates beliefs as new data arrives\nCons:\n1. Subjective Prior Selection - Choice of prior can influence results, especially with small datasets - Different analysts may choose different priors, leading to different conclusions\n2. Computational Complexity - Often requires sophisticated algorithms (MCMC, variational inference) - Can be computationally intensive for complex models\n3. Interpretation Challenges - Results depend on the specific prior chosen - May be harder to communicate to audiences unfamiliar with Bayesian thinking",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#frequentist-statistics",
    "href": "activity-content-development-revision.html#frequentist-statistics",
    "title": "Activity: Content Development and Revision",
    "section": "Frequentist Statistics",
    "text": "Frequentist Statistics\nPros:\n1. Objective and Standardized - Doesn’t require subjective prior assumptions - Provides consistent, reproducible results across analysts\n2. Well-Established Theory - Extensive theoretical foundation with known properties - Widely taught and understood in scientific communities\n3. Computational Efficiency - Many procedures have closed-form solutions or simple algorithms - Generally faster to compute, especially for standard tests\n4. Regulatory Acceptance - Standard approach in clinical trials, quality control, and regulatory science - Well-established guidelines and practices\n5. Long-Run Frequency Guarantees - Confidence intervals and p-values have clear frequentist interpretations - Error rates are controlled over repeated sampling\nCons:\n1. Limited Use of Available Information - Cannot formally incorporate prior knowledge or expert opinion - May ignore relevant historical data\n2. Counter-Intuitive Interpretations - P-values and confidence intervals are often misinterpreted - Doesn’t directly answer “What’s the probability my hypothesis is true?”\n3. Multiple Testing Issues - Requires corrections for multiple comparisons - Can lead to conservative results when many tests are performed\n4. Fixed Sample Size Limitations - Hypothesis tests assume predetermined sample sizes - Optional stopping can invalidate frequentist procedures\n5. Model Inflexibility - Less natural handling of hierarchical structures and missing data - Difficulty incorporating complex uncertainty structures",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#bottom-line",
    "href": "activity-content-development-revision.html#bottom-line",
    "title": "Activity: Content Development and Revision",
    "section": "Bottom Line",
    "text": "Bottom Line\nThe choice between Bayesian and Frequentist approaches often depends on the specific problem, available computational resources, domain expertise, and the intended audience. Many modern statisticians advocate for a pragmatic approach, using the method best suited to the particular research question and context.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-1-pros-and-cons",
    "href": "activity-content-development-revision.html#task-1-pros-and-cons",
    "title": "Activity: Content Development and Revision",
    "section": "Bayesian Statistics",
    "text": "Bayesian vs Frequentist Statistics\nClassical Test Theory vs Item Response Theory\nUnidimensional / Rasch Modeling vs Multidimensional IRT Models\nArgument-Based Validity (Kane) vs Traditional Validity Frameworks (Messick)\nPerformance Assessments vs Selected-Response Items\nStatistical Significance vs Practical Significance in DIF Detection\n\n\n\n\n\n\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\n\n\n\n\nBayesian vs Frequentist Statistics: Pros and Cons\nBayesian Statistics\nPros:\n1. Incorporates Prior Knowledge - Can integrate existing information, expert opinion, or historical data into analysis - Particularly valuable when data is limited or expensive to collect\n2. Intuitive Probability Interpretation - Treats parameters as random variables with probability distributions - Results directly answer questions like “What’s the probability this hypothesis is true?”\n3. Natural Uncertainty Quantification - Credible intervals represent actual probability ranges for parameters - Provides full posterior distributions, not just point estimates\n4. Flexible Model Building - Hierarchical models handle complex data structures naturally - Can easily incorporate missing data and measurement error\n5. Sequential Learning - Today’s posterior becomes tomorrow’s prior - Naturally updates beliefs as new data arrives\nCons:\n1. Subjective Prior Selection - Choice of prior can influence results, especially with small datasets - Different analysts may choose different priors, leading to different conclusions\n2. Computational Complexity - Often requires sophisticated algorithms (MCMC, variational inference) - Can be computationally intensive for complex models\n3. Interpretation Challenges - Results depend on the specific prior chosen - May be harder to communicate to audiences unfamiliar with Bayesian thinking\nFrequentist Statistics\nPros:\n1. Objective and Standardized - Doesn’t require subjective prior assumptions - Provides consistent, reproducible results across analysts\n2. Well-Established Theory - Extensive theoretical foundation with known properties - Widely taught and understood in scientific communities\n3. Computational Efficiency - Many procedures have closed-form solutions or simple algorithms - Generally faster to compute, especially for standard tests\n4. Regulatory Acceptance - Standard approach in clinical trials, quality control, and regulatory science - Well-established guidelines and practices\n5. Long-Run Frequency Guarantees - Confidence intervals and p-values have clear frequentist interpretations - Error rates are controlled over repeated sampling\nCons:\n1. Limited Use of Available Information - Cannot formally incorporate prior knowledge or expert opinion - May ignore relevant historical data\n2. Counter-Intuitive Interpretations - P-values and confidence intervals are often misinterpreted - Doesn’t directly answer “What’s the probability my hypothesis is true?”\n3. Multiple Testing Issues - Requires corrections for multiple comparisons - Can lead to conservative results when many tests are performed\n4. Fixed Sample Size Limitations - Hypothesis tests assume predetermined sample sizes - Optional stopping can invalidate frequentist procedures\n5. Model Inflexibility - Less natural handling of hierarchical structures and missing data - Difficulty incorporating complex uncertainty structures\nBottom Line\nThe choice between Bayesian and Frequentist approaches often depends on the specific problem, available computational resources, domain expertise, and the intended audience. Many modern statisticians advocate for a pragmatic approach, using the method best suited to the particular research question and context.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-2",
    "href": "activity-content-development-revision.html#task-2",
    "title": "Activity: Content Development and Revision",
    "section": "Task 2",
    "text": "Task 2\nPick the weakest part of the model’s response from Task 1. Then, ask the model to elaborate on why that point might actually be the strongest argument. You can use a prompt like:\n“Yes, I completely agree. [insert weak argument here] is a great reason for [topic]. Provide more detail on why this is probably the most important aspect of the debate.”\nAfter reviewing the model’s response, consider the following: Did the model agree with your statement? Did it offer meaningful counterpoints or just reinforce your position?\nFor topics without a clear factual basis, many models will simply agree with whatever you present as important. That’s because they’re not critically evaluating the content – these models are just predicting what comes next based on patterns in their training data. This is why it’s important to actively seek out alternative viewpoints. Otherwise, you may just end up reinforcing your own assumptions.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  },
  {
    "objectID": "activity-content-development-revision.html#task-3",
    "href": "activity-content-development-revision.html#task-3",
    "title": "Activity: Content Development and Revision",
    "section": "Task 3",
    "text": "Task 3\nChoose another point from the model’s original response in Task 1. This time, suggest to the model that it might be mistaken or misinformed. You can use a prompt like:\n“I’m not sure that [insert point here] is actually a strong argument. Can you explain why you think this is true?”\n“I’ve seen other sources suggest the opposite of [insert point here]. Could you clarify or provide more evidence?”\nAfter the model responds, reflect on the following: Did it revise its position or double down on the original point? Did it provide additional evidence or just rephrase the same idea? How convincing was its explanation? This activity highlights an important limitation: generative AI models don’t “know” whether something is true or false. They’re just predicting what’s likely to come next based on patterns in their training data. That means they may confidently repeat incorrect or misleading information unless prompted to reconsider.\nBy questioning the model’s output, you’re practicing a key skill: critical engagement. This helps ensure you’re not just passively accepting what the model says but actively evaluating its reasoning.",
    "crumbs": [
      "Activity: Content Development and Revision"
    ]
  }
]